{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAEs\n",
    "\n",
    "Resources:\n",
    "- [Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n",
    "- [Sparse Feature Circuits](https://arxiv.org/abs/2403.19647)\n",
    "- [Attention SAEs](https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZsparse-autoencoders-work-on-attention-layer-outputs)\n",
    "- [Gated SAEs](https://arxiv.org/abs/2404.16014)\n",
    "- Anthropic Circuit Updates ([Feb](https://transformer-circuits.pub/2024/feb-update/index.html) - [Mar](https://transformer-circuits.pub/2024/march-update/index.html) - [Apr](https://transformer-circuits.pub/2024/april-update/index.html))\n",
    "- GDM [Update](https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrgfull-post-progress-update-1-from-the-gdm-mech-interp-team)\n",
    "\n",
    "\n",
    "https://www.lesswrong.com/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
