{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2abc9a0adb24afa83a4090e7f39043a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "#model_name = 'google/gemma-2b'\n",
    "\n",
    "model_label = model_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec35aaf0c2e46f59fdcfeafbdadd40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name, torch_dtype=torch.float32, n_devices=2)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt):\n",
    "    return model.tokenizer.apply_chat_template([\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ], tokenize=False)\n",
    "\n",
    "def generate_until_stop(prompt, stop_tokens, max_tokens=64, verbose=False, prepend_bos=True):\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = [model.to_single_token(tok) for tok in stop_tokens]\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    gen = True\n",
    "    while gen:\n",
    "        with torch.no_grad():\n",
    "            new_tok = model(tokens).argmax(-1)[:, -1]\n",
    "        \n",
    "        if verbose: print(model.to_string(new_tok), end='')\n",
    "        tokens = torch.cat([tokens, new_tok[None].to(tokens.device)], dim=-1)\n",
    "        if new_tok.item() in stop_tokens or max_tokens == 0:\n",
    "            gen = False\n",
    "        max_tokens -= 1\n",
    "\n",
    "    return model.to_string(tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Let's think step-by-step:\n",
      "\n",
      "(1) Wren is a dumpus.\n",
      "(2) Dumpuses are bright.\n",
      "(3) Wren is bright.\n",
      "\n",
      "Answer: True<|eot_id|>"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer True or False to the following question. Answer as in the examples.\n",
    "\n",
    "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
    "Question: Is Polly muffled?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Polly is an impus.\n",
    "(2) Every impus is muffled.\n",
    "(3) Polly is muffled.\n",
    "Answer: True\n",
    "\n",
    "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
    "Question: Is Alex rainy?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Alex is a rompus.\n",
    "(2) Rompuses are rainy.\n",
    "(3) Alex is rainy.\n",
    "Answer: True\n",
    "\n",
    "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
    "Question: Is Fae slow?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Fae is a vumpus.\n",
    "(2) Each vumpus is not slow.\n",
    "(3) Fae is not slow.\n",
    "Answer: False\n",
    "\n",
    "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
    "Question: Is Wren bright?\n",
    "Think step-by-step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out = generate_until_stop(apply_template(prompt), stop_tokens=['<|eot_id|>'], verbose=True)\n",
    "#out = generate_until_stop(prompt, stop_tokens=[' True', ' False'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cot(x):\n",
    "    try:\n",
    "        check = all([x == y for x, y in zip(x['cot_gold'], x['cot_pred'])])\n",
    "    except: check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = \"3shots\"\n",
    "model_label = model_name.split('/')[-1]\n",
    "\n",
    "result = pd.read_csv(f'results/{model_label}/{n_shots}_cot.csv')\n",
    "result['correct_pred'] = result['label'] == result['pred']\n",
    "result['correct_cot'] = result.apply(check_cot, axis=1)\n",
    "\n",
    "correct_preds = result[result['correct_pred'] & result['correct_cot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
      "Question: Is Polly muffled?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is an impus.\n",
      "(2) Every impus is muffled.\n",
      "(3) Polly is muffled.\n",
      "Answer: True\n",
      "\n",
      "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
      "Question: Is Alex rainy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a rompus.\n",
      "(2) Rompuses are rainy.\n",
      "(3) Alex is rainy.\n",
      "Answer: True\n",
      "\n",
      "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
      "Question: Is Fae slow?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Fae is a vumpus.\n",
      "(2) Each vumpus is not slow.\n",
      "(3) Fae is not slow.\n",
      "Answer: False\n",
      "\n",
      "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
      "Question: Is Wren cold?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dla(prompt, component, a_clean, a_corr=None, prepend_bos=True):\n",
    "    \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    dlas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, component)\n",
    "\n",
    "    cache = ActivationCache(cache, model).to('cpu')\n",
    "    act = cache.stack_activation(component)[:, :, -1]\n",
    "    if len(act.shape) == 4:\n",
    "        act = cache.stack_head_results(-1)[:, :, -1]\n",
    "\n",
    "    dla = model.unembed(act.to(model.W_U.device)).cpu()\n",
    "    del cache\n",
    "    \n",
    "    return dla[..., a_clean].mean(-1) - dla[..., a_corr].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def plot_dla(resid_dla, mlp_dla, attn_dla, max_val=50):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Residual Stream\", \"MLP\", \"Attention Heads\"))\n",
    "\n",
    "    # Add images to the subplots\n",
    "    fig.add_trace(px.imshow(resid_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=1)\n",
    "    fig.add_trace(px.imshow(mlp_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=2)\n",
    "    fig.add_trace(px.imshow(attn_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=3)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='RdBu',\n",
    "            cmin=-max_val,\n",
    "            cmax=max_val\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title_text=\"Direct Logit Attribution\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_patterns(prompt, patterns, n_cols, query_offset, key_offset):\n",
    "    \n",
    "    n_rows = len(patterns) // n_cols + int(len(patterns) % n_cols != 0)\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=patterns)\n",
    "\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    labels = [f\"{tok} ({i})\" for i, tok in enumerate(str_tokens)]\n",
    "    query_labels = labels[query_offset:]\n",
    "    key_labels = labels[key_offset:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, 'attn')\n",
    "\n",
    "    for i, pid in enumerate(patterns):\n",
    "        layer, head = pid.split('H')\n",
    "        layer = int(layer[1:])\n",
    "        head = int(head[:-1])\n",
    "\n",
    "        pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head, query_offset:, key_offset:].cpu()\n",
    "\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "    \n",
    "        fig.add_trace(px.imshow(\n",
    "            pattern,\n",
    "            labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "            x=key_labels,\n",
    "            y=query_labels\n",
    "        ).data[0], row=row, col=col)\n",
    "\n",
    "        fig.update_xaxes(tickangle=35)\n",
    "        fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='Blues',\n",
    "            cmin=1,\n",
    "            cmax=0\n",
    "        ),\n",
    "        height=700 * n_rows,\n",
    "        width=800 * n_cols,\n",
    "        title_text=\"Attention Patterns\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG\n",
    "import torch\n",
    "import einops\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def attribution_patching(x_clean, x_corr, a_clean, a_corr, component, prepend_bos=True, method='standard', num_alphas=5, n_last_tokens=128):\n",
    "    if isinstance(x_clean, str):\n",
    "        clean_tokens = model.to_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        clean_tokens = x_clean\n",
    "\n",
    "    if isinstance(x_corr, str):\n",
    "        corr_tokens = model.to_tokens(x_corr, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        corr_tokens = x_corr\n",
    "\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "\n",
    "    if isinstance(a_corr, str):\n",
    "        a_corr = model.to_single_token(a_corr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corr_logits, corr_cache = get_cache_fw(corr_tokens, component)\n",
    "\n",
    "    if method == 'standard':\n",
    "        logits_diff_, clean_cache, clean_grad_cache = get_cache_fw_and_bw(clean_tokens, a_clean, a_corr, corr_logits, component=component)\n",
    "    elif method == 'ig':\n",
    "        with torch.no_grad():\n",
    "            clean_logits, clean_cache = get_cache_fw(clean_tokens, component)\n",
    "\n",
    "    clean_cache = ActivationCache(clean_cache, model).to('cpu')\n",
    "    corr_cache = ActivationCache(corr_cache, model).to('cpu')\n",
    "\n",
    "    corr_act = clean_cache.stack_activation(component)[:, 0, -n_last_tokens:]\n",
    "    clean_act = corr_cache.stack_activation(component)[:, 0, -n_last_tokens:] # comp, pos dm\n",
    "    del clean_cache, corr_cache\n",
    "    \n",
    "    if clean_act.ndim > 3:\n",
    "        clean_act = clean_act.transpose(1, 2)\n",
    "        corr_act = corr_act.transpose(1, 2)\n",
    "        clean_act = clean_act.reshape(-1, clean_act.size(2), clean_act.size(3))\n",
    "        corr_act = corr_act.reshape(-1, corr_act.size(2), corr_act.size(3))\n",
    "        \n",
    "    if method == 'standard':\n",
    "        clean_grad_cache = ActivationCache(clean_grad_cache, model).to('cpu')\n",
    "        clean_grad_act = clean_grad_cache.stack_activation(component).squeeze()\n",
    "        if clean_grad_act.ndim > 3:\n",
    "            clean_grad_act = clean_grad_act.transpose(1, 2)\n",
    "            clean_grad_act = clean_grad_act.reshape(-1, clean_grad_act.size(2), clean_grad_act.size(3))\n",
    "        clean_grad_act = clean_grad_act[:, -n_last_tokens:].cpu()\n",
    "    elif method == 'ig':\n",
    "        clean_grad_act = []\n",
    "        alphas = torch.linspace(0, 1, num_alphas)\n",
    "        k = clean_act.shape[0] // model.cfg.n_layers\n",
    "        for l in tqdm(range(model.cfg.n_layers)):\n",
    "            ig_patch = torch.zeros_like(clean_act[k*l:k*(l+1)], device=clean_act.device)\n",
    "            for alpha in alphas:\n",
    "                a_alpha = alpha * clean_act[k*l:k*(l+1)] + (1 - alpha) * corr_act[k*l:k*(l+1)]\n",
    "                logits_alpha, grad_alpha = get_cache_fw_with_modified_activations(clean_tokens, a_alpha, a_clean, a_corr, l, component)\n",
    "                if grad_alpha.ndim > 3:\n",
    "                    grad_alpha = grad_alpha.reshape(-1, grad_alpha.size(1), grad_alpha.size(3))\n",
    "                grad_alpha = grad_alpha[:, -n_last_tokens:].cpu()\n",
    "                ig_patch += grad_alpha * (clean_act[k*l:k*(l+1)] - corr_act[k*l:k*(l+1)])\n",
    "                del a_alpha, logits_alpha, grad_alpha\n",
    "            clean_grad_act.append(ig_patch / num_alphas)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        clean_grad_act = torch.cat(clean_grad_act, dim=0)\n",
    "\n",
    "    print(\"Gradients collected! Computing the patch...\")\n",
    "    patch = einops.reduce(\n",
    "        clean_grad_act * (corr_act - clean_act),\n",
    "        \"component pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    del clean_act, corr_act, clean_grad_act\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return patch\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    filter = lambda name: utils.get_act_name(component) in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def get_cache_fw_and_bw(tokens, a_clean, a_corr, corr_logits, component='all'):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "        \n",
    "    model.reset_hooks()\n",
    "    \n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(filter, bw_cache_hook, \"bwd\")\n",
    "\n",
    "    clean_logits = model(tokens).cpu()\n",
    "    value = logits_diff(clean_logits, a_clean, a_corr) #- logits_diff(corr_logits.cpu(), a_clean, a_corr)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n",
    "\n",
    "def logits_diff(logits, a_clean, a_corr=None):\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "    if a_corr:\n",
    "        if isinstance(a_corr, str):\n",
    "            a_corr = [model.to_single_token(a_corr)]\n",
    "        \n",
    "        return logits[0, -1, a_clean] - logits[0, -1, a_corr].mean(-1)\n",
    "    else:\n",
    "        return logits[0, -1, a_clean]\n",
    "\n",
    "def get_cache_fw_with_modified_activations(tokens, x_int, a_clean, a_corr, layer, component):\n",
    "    hook_point = utils.get_act_name(component, layer)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    def fw_hook(act, mod_act, hook):\n",
    "        act = mod_act\n",
    "\n",
    "    fw_hook_fn = partial(fw_hook, mod_act=x_int.squeeze())\n",
    "    model.add_hook(hook_point, fw_hook_fn, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(hook_point, bw_cache_hook, \"bwd\")\n",
    "    logits = model(tokens)\n",
    "    value = logits_diff(logits, a_corr, a_clean)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return value.item(), grad_cache[hook_point]\n",
    "\n",
    "def plot_atp(atp, x_clean, component, n_last_tokens=128, val=1, prepend_bos=True):\n",
    "\n",
    "    str_tokens = model.to_str_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    xs = [f\"{tok} | {i}\" for i, tok in enumerate(str_tokens[-n_last_tokens:])]\n",
    "    \n",
    "    if component in ['z', 'q', 'result']:\n",
    "        ys = [f'L{i}H{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "    elif component in ['k', 'v']:\n",
    "        ys = [f'L{i}{component.upper()}{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_key_value_heads)]\n",
    "    else:\n",
    "        ys = [f\"L{l} {component.upper()}\" for l in range(model.cfg.n_layers)]\n",
    "        \n",
    "    fig = px.imshow(\n",
    "        atp[:, -n_last_tokens:].cpu().numpy(), \n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        color_continuous_scale='RdBu', zmin=-val, zmax=val, aspect='auto'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtasks\n",
    "\n",
    "We then explore each subtask mechanistically to understand which are the components responsible for each choice made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = ['grimpus', 'lorpus', 'wumpus', 'zumpus', 'sterpus', 'numpus', 'jompus', 'brimpus', 'yumpus', 'tumpus', 'dumpus', 'vumpus', 'rompus', 'lempus', 'gorpus', 'shumpus', 'impus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Each yumpus is not transparent. Each shumpus is loud. Every impus is a brimpus. Every shumpus is a tumpus. Each impus is a yumpus. Every impus is not shy. Alex is an impus. Alex is a shumpus.\n",
      "Question: Is Alex shy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is an impus.\n",
      "(2) Every impus is not shy.\n",
      "(3) Alex is not shy.\n",
      "Answer: False\n",
      "\n",
      "Each rompus is an impus. Wumpuses are not feisty. Each wumpus is a lorpus. Rompuses are aggressive. Rompuses are gorpuses. Impuses are not small. Sam is a rompus. Sam is a wumpus.\n",
      "Question: Is Sam aggressive?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is a rompus.\n",
      "(2) Rompuses are aggressive.\n",
      "(3) Sam is aggressive.\n",
      "Answer: True\n",
      "\n",
      "Each impus is floral. Every gorpus is a lempus. Every jompus is wooden. Brimpuses are lorpuses. Every gorpus is an impus. Each lorpus is a grimpus. Every lempus is hot. Each brimpus is a zumpus. Gorpuses are not dull. Every dumpus is a wumpus. Every lorpus is a numpus. Every numpus is overcast. Every dumpus is not fast. Every zumpus is melodic. Each impus is a jompus. Every lorpus is aggressive. Impuses are brimpuses. Each brimpus is large. Sam is a dumpus. Sam is an impus.\n",
      "Question: Is Sam floral?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is an impus.\n",
      "(2) Each impus is floral.\n",
      "(3) Sam is floral.\n",
      "Answer: True\n",
      "\n",
      "Wumpuses are bitter. Each grimpus is a zumpus. Tumpuses are small. Every wumpus is an impus. Grimpuses are not transparent. Wumpuses are tumpuses. Max is a grimpus. Max is a wumpus.\n",
      "Question: Is Max bitter?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "data = result # or correct_preds\n",
    "print(data['prompt'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 - Choosing the right species\n",
    "The first step is choosing the right species to focus on. This is a key step since it preceeds the attribute check. It is also the most difficult one since the choice doesn't depend only on the species and the entity, but has to be made already considering the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = data['prompt'].iloc[idx]\n",
    "cot_gold = data['cot_gold'].iloc[idx]\n",
    "label = data['label'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "\n",
    "for id_, s in enumerate(species):\n",
    "    if s in cot_gold[0]:\n",
    "        break\n",
    "\n",
    "a_clean = species_token[id_].cpu()\n",
    "a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_clean = apply_template(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Max is a"
     ]
    }
   ],
   "source": [
    "stop_tokens = [' a', ' an']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla = compute_dla(clean_out, 'resid_pre', a_clean, a_corr, prepend_bos=False)\n",
    "mlp_dla = compute_dla(clean_out, 'mlp_out', a_clean, a_corr, prepend_bos=False)\n",
    "attn_dla = compute_dla(clean_out, 'result', a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla.reshape(model.cfg.n_layers, -1), max_val=5)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.write_html(f'fig/{model_label}/{idx}_s1_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "assert len(species) == 2, \"More than two species detected!\"\n",
    "assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_corr = cot_gold.copy()\n",
    "\n",
    "cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "context = x_clean.split('\\n\\n')\n",
    "\n",
    "context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "x_corr = '\\n\\n'.join(context)\n",
    "\n",
    "#x_corr = apply_template(x_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Max is a"
     ]
    }
   ],
   "source": [
    "corr_out = generate_until_stop(x_corr, stop_tokens, prepend_bos=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bos before\n",
    "ans = clean_out.split('<|end_header_id|>\\n\\n')[-1]\n",
    "corr_out = '<|end_header_id|>\\n\\n'.join(corr_out.split('<|end_header_id|>\\n\\n')[:-1]) + '<|end_header_id|>\\n\\n' + ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|begin_of_text|>', '<|begin_of_text|>'),\n",
       " ('Answer', 'Answer'),\n",
       " (' True', ' True'),\n",
       " (' or', ' or'),\n",
       " (' False', ' False'),\n",
       " (' to', ' to'),\n",
       " (' the', ' the'),\n",
       " (' following', ' following'),\n",
       " (' question', ' question'),\n",
       " ('.', '.'),\n",
       " (' Answer', ' Answer'),\n",
       " (' as', ' as'),\n",
       " (' in', ' in'),\n",
       " (' the', ' the'),\n",
       " (' examples', ' examples'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' loud', ' loud'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Alex', ' Alex'),\n",
       " (' shy', ' shy'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' False', ' False'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' fe', ' fe'),\n",
       " ('isty', 'isty'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' g', ' g'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' le', ' le'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' j', ' j'),\n",
       " ('omp', 'omp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' wooden', ' wooden'),\n",
       " ('.', '.'),\n",
       " (' Br', ' Br'),\n",
       " ('imp', 'imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' l', ' l'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' le', ' le'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' hot', ' hot'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' G', ' G'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' dull', ' dull'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' over', ' over'),\n",
       " ('cast', 'cast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' mel', ' mel'),\n",
       " ('odic', 'odic'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' j', ' j'),\n",
       " ('omp', 'omp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' floral', ' floral'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('W', 'G'),\n",
       " ('ump', 'rimp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' bitter', ' bitter'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' T', ' T'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Gr', ' Gr'),\n",
       " ('imp', 'imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Max', ' Max'),\n",
       " (' bitter', ' bitter'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model.to_str_tokens(clean_out, prepend_bos=False), model.to_str_tokens(corr_out, prepend_bos=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "for component in ['resid_pre', 'mlp_out', 'attn_out', 'result']: \n",
    "    atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "    torch.save(atp, f'patches/{model_label}/{idx}_s1_AtP_{component}.bin')\n",
    "    fig = plot_atp(atp, clean_out, component=component, prepend_bos=False, n_last_tokens=256)\n",
    "    fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_{component}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "\n",
    "torch.save(atp_q, f'patches/{model_label}/{idx}_s1_AtP_q.bin')\n",
    "torch.save(atp_k, f'patches/{model_label}/{idx}_s1_AtP_k.bin')\n",
    "torch.save(atp_v, f'patches/{model_label}/{idx}_s1_AtP_v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256, **kwargs):\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Queries\", \"Keys\", \"Values\"), shared_xaxes=True, vertical_spacing=0.05)\n",
    "    \n",
    "    for i, (atp, hook) in enumerate(zip([atp_q, atp_k, atp_v], ['q', 'k', 'v'])):\n",
    "        plot = plot_atp(atp, clean_out, component=hook, prepend_bos=False, n_last_tokens=n_last_tokens)\n",
    "        for trace in plot.data:\n",
    "            fig.add_trace(trace, row=1+i, col=1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        coloraxis1=dict(colorscale='RdBu', cmin=-0.5, cmax=0.5),\n",
    "        showlegend=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [02:42<00:00,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [02:42<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [02:41<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q_ig = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_k_ig = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_v_ig = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q_ig, atp_k_ig, atp_v_ig)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv_ig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to_tokens(clean_out, prepend_bos=False)[0], f'patches/{model_label}/{idx}_clean_tokens.bin')\n",
    "torch.save(model.to_tokens(corr_out, prepend_bos=False)[0], f'patches/{model_label}/{idx}_corr_tokens.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, clean_patterns = get_cache_fw(model.to_tokens(clean_out, prepend_bos=False)[0], 'pattern')\n",
    "    _, corr_patterns = get_cache_fw(model.to_tokens(corr_out, prepend_bos=False)[0], 'pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_v1 = {\n",
    "    2: ['L0H6-', 'L0H26-', 'L1H26-', 'L3H18+', 'L9H30-', 'L10H2+', 'L13H0-', 'L13H1+', 'L13H17-', 'L13H18+', 'L14H20+', 'L15H8+', 'L15H10-', 'L15H11+', 'L16H19-', 'L16H28+', 'L16H30-', 'L17H24+', 'L18H20+', 'L18H22-', 'L19H23-', 'L20H13+', 'L20H14-', 'L22H19+', 'L24H27+', 'L26H15-', 'L27H7+', 'L30H11+', 'L30H25-'],\n",
    "    6: ['L1H25+', 'L2H25-', 'L2H27-', 'L3H25-', 'L4H27+', 'L8H27+', 'L10H27-', 'L11H31-', 'L12H31+', 'L13H30+', 'L13H31-', 'L14H30+', 'L14H31-', 'L15H29+', 'L15H31+', 'L16H31+', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L21H31-', 'L24H31+', 'L26H31-', 'L30H31+', 'L31H31-']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inspect_patterns = []\n",
    "corr_inspect_patterns = []\n",
    "\n",
    "for lab in heads_v1[idx]:\n",
    "    l, h = lab.split('H')\n",
    "    l = int(l[1:])\n",
    "    h = int(h[:-1])\n",
    "    clean_inspect_patterns.append(clean_patterns[f'blocks.{l}.attn.hook_pattern'][:, h].cpu())\n",
    "    corr_inspect_patterns.append(corr_patterns[f'blocks.{l}.attn.hook_pattern'][:, h].cpu())\n",
    "    \n",
    "clean_inspect_patterns = torch.cat(clean_inspect_patterns).cpu()\n",
    "corr_inspect_patterns = torch.cat(corr_inspect_patterns).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clean_inspect_patterns, f'patches/{idx}_patterns_clean_v1.bin')\n",
    "torch.save(corr_inspect_patterns, f'patches/{idx}_patterns_corr_v1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_last_tokens = 512\n",
    "\n",
    "for idx in tqdm(range(10)):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    cot_corr = cot_gold.copy()\n",
    "\n",
    "    cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "    context = x_clean.split('\\n\\n')\n",
    "    \n",
    "    context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "    x_corr = '\\n\\n'.join(context)\n",
    "    corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "    try:\n",
    "        assert len(species) == 2, \"More than two species detected!\"\n",
    "        assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv.html\")\n",
    "\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "stop_tokens = [' a',]\n",
    "n_last_tokens = 512\n",
    "\n",
    "\n",
    "idx = 2\n",
    "x_clean = data['prompt'].iloc[idx]\n",
    "cot_gold = data['cot_gold'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "subject  = cot_gold[0].split()[0]\n",
    "a_clean = cot_gold[0].split()[-1][:-1]\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for c in clues:\n",
    "    if subject in c and a_clean not in c:\n",
    "        a_corr = c.split()[-1]\n",
    "        a_corr = a_corr[:-1] if a_corr[-1] == '.' else a_corr\n",
    "\n",
    "clean_attr = clean_question.split()[2][:-1]\n",
    "\n",
    "for c in clues:\n",
    "    if a_corr in c.lower():\n",
    "        corr_attr = c.split()[-1]\n",
    "        if not_a_species(corr_attr):\n",
    "            break\n",
    "\n",
    "corr_question = clean_question.replace(clean_attr, corr_attr)\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + corr_question\n",
    "\n",
    "a_clean = model.to_tokens(' ' + a_clean, prepend_bos=False)[:, 0].cpu()\n",
    "a_corr = model.to_tokens(' ' + a_corr, prepend_bos=False)[:, 0].cpu()\n",
    "\n",
    "assert len(model.to_tokens(' ' + clean_attr)[0]) == len(model.to_tokens(' ' + corr_attr)[0]), \"Attributes with different shapes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' w', ' gr')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(a_clean), model.to_string(a_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 1.282\n",
      "Clean logit difference: -3.247\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "for component in ['resid_pre', 'mlp_out', 'attn_out', 'result']: \n",
    "    atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "    torch.save(atp, f'patches/{model_label}/{idx}_s1_AtP_{component}_v2.bin')\n",
    "    fig = plot_atp(atp, clean_out_new, component=component, prepend_bos=False, n_last_tokens=256)\n",
    "    fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_{component}_v2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "atp_k = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "atp_v = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "torch.save(atp_q, f'patches/{model_label}/{idx}_s1_AtP_q_v2.bin')\n",
    "torch.save(atp_k, f'patches/{model_label}/{idx}_s1_AtP_k_v2.bin')\n",
    "torch.save(atp_v, f'patches/{model_label}/{idx}_s1_AtP_v_v2.bin')\n",
    "\n",
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv_v2.html\")\n",
    "\n",
    "#atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "#atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "#atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "\n",
    "#fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "#fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to_tokens(clean_out_new, prepend_bos=False)[0], f'patches/{idx}_clean_tokens_v2.bin')\n",
    "torch.save(model.to_tokens(corr_out_new, prepend_bos=False)[0], f'patches/{idx}_corr_tokens_v2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, clean_patterns_v2 = get_cache_fw(model.to_tokens(clean_out_new, prepend_bos=False)[0], 'pattern')\n",
    "    _, corr_patterns_v2 = get_cache_fw(model.to_tokens(corr_out_new, prepend_bos=False)[0], 'pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_v2 = {\n",
    "    2: ['L2H25-','L8H11+','L12H2+','L12H15+','L12H27-','L13H0-','L13H1+','L13H16-','L13H17-','L13H18+','L14H20+','L14H22-','L15H8+','L15H10-','L15H11+','L16H28+','L16H30-','L17H24+','L18H20+','L18H22-','L19H23-','L20H13+','L20H14-','L24H27+','L26H15-','L30H24+'],\n",
    "    6: ['L9H30+', 'L10H30-', 'L11H30+', 'L11H31-', 'L12H30+', 'L12H31+', 'L13H30+', 'L13H31-', 'L14H31-', 'L15H31+', 'L16H31-', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L21H31+', 'L22H31+', 'L24H31+', 'L26H31-', 'L30H31+', 'L31H31+']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inspect_patterns = []\n",
    "corr_inspect_patterns = []\n",
    "\n",
    "for lab in heads_v2[idx]:\n",
    "    l, h = lab.split('H')\n",
    "    l = int(l[1:])\n",
    "    h = int(h[:-1])\n",
    "    clean_inspect_patterns.append(clean_patterns_v2[f'blocks.{l}.attn.hook_pattern'][:, h].cpu())\n",
    "    corr_inspect_patterns.append(corr_patterns_v2[f'blocks.{l}.attn.hook_pattern'][:, h].cpu())\n",
    "    \n",
    "clean_inspect_patterns = torch.cat(clean_inspect_patterns).cpu()\n",
    "corr_inspect_patterns = torch.cat(corr_inspect_patterns).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clean_inspect_patterns, f'patches/{idx}_patterns_clean_v2.bin')\n",
    "torch.save(corr_inspect_patterns, f'patches/{idx}_patterns_corr_v2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 - Attribute check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['2']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is', ' are']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Answer True or False to the following question. Answer as in the examples.\\n\\nEach yumpus is not transparent. Each shumpus is loud. Every impus is a brimpus. Every shumpus is a tumpus. Each impus is a yumpus. Every impus is not shy. Alex is an impus. Alex is a shumpus.\\nQuestion: Is Alex shy?\\nThink step-by-step.\\n\\n(1) Alex is an impus.\\n(2) Every impus is not shy.\\n(3) Alex is not shy.\\nAnswer: False\\n\\nEach rompus is an impus. Wumpuses are not feisty. Each wumpus is a lorpus. Rompuses are aggressive. Rompuses are gorpuses. Impuses are not small. Sam is a rompus. Sam is a wumpus.\\nQuestion: Is Sam aggressive?\\nThink step-by-step.\\n\\n(1) Sam is a rompus.\\n(2) Rompuses are aggressive.\\n(3) Sam is aggressive.\\nAnswer: True\\n\\nEach impus is floral. Every gorpus is a lempus. Every jompus is wooden. Brimpuses are lorpuses. Every gorpus is an impus. Each lorpus is a grimpus. Every lempus is hot. Each brimpus is a zumpus. Gorpuses are not dull. Every dumpus is a wumpus. Every lorpus is a numpus. Every numpus is overcast. Every dumpus is not fast. Every zumpus is melodic. Each impus is a jompus. Every lorpus is aggressive. Impuses are brimpuses. Each brimpus is large. Sam is a dumpus. Sam is an impus.\\nQuestion: Is Sam floral?\\nThink step-by-step.\\n\\n(1) Sam is an impus.\\n(2) Each impus is floral.\\n(3) Sam is floral.\\nAnswer: True\\n\\nWumpuses are bitter. Each grimpus is a zumpus. Tumpuses are small. Every wumpus is an impus. Grimpuses are not transparent. Wumpuses are tumpuses. Max is a grimpus. Max is a wumpus.\\nQuestion: Is Max bitter?\\nThink step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_steps = cot_gold[1].split()\n",
    "for i, step in enumerate(cot_steps):\n",
    "    if \"is\" in step or \"are\" in step:\n",
    "        attribute = cot_steps[i+1].replace('.', '')\n",
    "        a_clean = ' ' + attribute\n",
    "\n",
    "a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "a_clean = [model.to_single_token(a_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([539], [26242], ' not', ' bitter')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_corr, a_clean, model.to_string(a_corr), model.to_string(a_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 3.683\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 2) | {attribute} - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L8H6-', 'L10H5+', 'L10H7+', 'L13H2+', 'L13H4+', 'L14H0+', 'L14H3-', 'L14H4+', 'L16H0+', 'L16H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "import random\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for s in all_species:\n",
    "    if s in clean_question.lower(): \n",
    "        s_star = s\n",
    "        break\n",
    "    \n",
    "a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "corr_question = clean_question.replace(a_clean, a_corr)\n",
    "\n",
    "for i, c in enumerate(clues):\n",
    "    if a_corr in c:\n",
    "        for s in all_species:\n",
    "            if s in c.lower(): break\n",
    "\n",
    "        clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Answer True or False to the following question. Answer as in the examples.\\n\\nEach yumpus is not transparent. Each shumpus is loud. Every impus is a brimpus. Every shumpus is a tumpus. Each impus is a yumpus. Every impus is not shy. Alex is an impus. Alex is a shumpus.\\nQuestion: Is Alex shy?\\nThink step-by-step.\\n\\n(1) Alex is an impus.\\n(2) Every impus is not shy.\\n(3) Alex is not shy.\\nAnswer: False\\n\\nEach rompus is an impus. Wumpuses are not feisty. Each wumpus is a lorpus. Rompuses are aggressive. Rompuses are gorpuses. Impuses are not small. Sam is a rompus. Sam is a wumpus.\\nQuestion: Is Sam aggressive?\\nThink step-by-step.\\n\\n(1) Sam is a rompus.\\n(2) Rompuses are aggressive.\\n(3) Sam is aggressive.\\nAnswer: True\\n\\nEach impus is floral. Every gorpus is a lempus. Every jompus is wooden. Brimpuses are lorpuses. Every gorpus is an impus. Each lorpus is a grimpus. Every lempus is hot. Each brimpus is a zumpus. Gorpuses are not dull. Every dumpus is a wumpus. Every lorpus is a numpus. Every numpus is overcast. Every dumpus is not fast. Every zumpus is melodic. Each impus is a jompus. Every lorpus is aggressive. Impuses are brimpuses. Each brimpus is large. Sam is a dumpus. Sam is an impus.\\nQuestion: Is Sam floral?\\nThink step-by-step.\\n\\n(1) Sam is an impus.\\n(2) Each impus is floral.\\n(3) Sam is floral.\\nAnswer: True\\n\\nWumpuses are bitter. Each grimpus is a zumpus. Tumpuses are small. Every wumpus is an impus. Wumpuses are not transparent. Wumpuses are tumpuses. Max is a grimpus. Max is a wumpus.\\nIs Max bitter?\\nThink step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_out_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Answer True or False to the following question. Answer as in the examples.\\n\\nEach yumpus is not transparent. Each shumpus is loud. Every impus is a brimpus. Every shumpus is a tumpus. Each impus is a yumpus. Every impus is not shy. Alex is an impus. Alex is a shumpus.\\nQuestion: Is Alex shy?\\nThink step-by-step.\\n\\n(1) Alex is an impus.\\n(2) Every impus is not shy.\\n(3) Alex is not shy.\\nAnswer: False\\n\\nEach rompus is an impus. Wumpuses are not feisty. Each wumpus is a lorpus. Rompuses are aggressive. Rompuses are gorpuses. Impuses are not small. Sam is a rompus. Sam is a wumpus.\\nQuestion: Is Sam aggressive?\\nThink step-by-step.\\n\\n(1) Sam is a rompus.\\n(2) Rompuses are aggressive.\\n(3) Sam is aggressive.\\nAnswer: True\\n\\nEach impus is floral. Every gorpus is a lempus. Every jompus is wooden. Brimpuses are lorpuses. Every gorpus is an impus. Each lorpus is a grimpus. Every lempus is hot. Each brimpus is a zumpus. Gorpuses are not dull. Every dumpus is a wumpus. Every lorpus is a numpus. Every numpus is overcast. Every dumpus is not fast. Every zumpus is melodic. Each impus is a jompus. Every lorpus is aggressive. Impuses are brimpuses. Each brimpus is large. Sam is a dumpus. Sam is an impus.\\nQuestion: Is Sam floral?\\nThink step-by-step.\\n\\n(1) Sam is an impus.\\n(2) Each impus is floral.\\n(3) Sam is floral.\\nAnswer: True\\n\\nWumpuses are bitter. Each grimpus is a zumpus. Tumpuses are small. Every wumpus is an impus. Wumpuses are not transparent. Wumpuses are tumpuses. Max is a grimpus. Max is a wumpus.\\nIs Max transparent?\\nThink step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_out_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 14.318\n",
      "Clean logit difference: -3.556\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|begin_of_text|>', '<|begin_of_text|>'),\n",
       " ('<|begin_of_text|>', '<|begin_of_text|>'),\n",
       " ('Answer', 'Answer'),\n",
       " (' True', ' True'),\n",
       " (' or', ' or'),\n",
       " (' False', ' False'),\n",
       " (' to', ' to'),\n",
       " (' the', ' the'),\n",
       " (' following', ' following'),\n",
       " (' question', ' question'),\n",
       " ('.', '.'),\n",
       " (' Answer', ' Answer'),\n",
       " (' as', ' as'),\n",
       " (' in', ' in'),\n",
       " (' the', ' the'),\n",
       " (' examples', ' examples'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' loud', ' loud'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Alex', ' Alex'),\n",
       " (' shy', ' shy'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' False', ' False'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' fe', ' fe'),\n",
       " ('isty', 'isty'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' g', ' g'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' le', ' le'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' j', ' j'),\n",
       " ('omp', 'omp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' wooden', ' wooden'),\n",
       " ('.', '.'),\n",
       " (' Br', ' Br'),\n",
       " ('imp', 'imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' l', ' l'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' le', ' le'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' hot', ' hot'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' G', ' G'),\n",
       " ('orp', 'orp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' dull', ' dull'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' over', ' over'),\n",
       " ('cast', 'cast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' mel', ' mel'),\n",
       " ('odic', 'odic'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' j', ' j'),\n",
       " ('omp', 'omp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' br', ' br'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' floral', ' floral'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('W', 'W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' bitter', ' bitter'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' T', ' T'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gr', ' gr'),\n",
       " ('imp', 'imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('Is', 'Is'),\n",
       " (' Max', ' Max'),\n",
       " (' bitter', ' transparent'),\n",
       " ('?\\n', '?\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-by', '-by'),\n",
       " ('-step', '-step'),\n",
       " ('.\\n\\n', '.\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Max', ' Max'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.\\n', '.\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model.to_str_tokens(clean_out_new), model.to_str_tokens(corr_out_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "for component in ['resid_pre', 'mlp_out', 'attn_out', 'result']: \n",
    "    atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "    torch.save(atp, f'patches/{model_label}/{idx}_s2_AtP_{component}_v2.bin')\n",
    "    fig = plot_atp(atp, clean_out_new, component=component, prepend_bos=False, n_last_tokens=256)\n",
    "    fig.write_html(f\"fig/{model_label}/{idx}_s2_AtP_{component}_v2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "\n",
    "torch.save(atp_q, f'patches/{model_label}/{idx}_s2_AtP_q.bin')\n",
    "torch.save(atp_k, f'patches/{model_label}/{idx}_s2_AtP_k.bin')\n",
    "torch.save(atp_v, f'patches/{model_label}/{idx}_s2_AtP_v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256, **kwargs):\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Queries\", \"Keys\", \"Values\"), shared_xaxes=True, vertical_spacing=0.05)\n",
    "    \n",
    "    for i, (atp, hook) in enumerate(zip([atp_q, atp_k, atp_v], ['q', 'k', 'v'])):\n",
    "        plot = plot_atp(atp, clean_out, component=hook, prepend_bos=False, n_last_tokens=n_last_tokens)\n",
    "        for trace in plot.data:\n",
    "            fig.add_trace(trace, row=1+i, col=1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        coloraxis1=dict(colorscale='RdBu', cmin=-0.5, cmax=0.5),\n",
    "        showlegend=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s2_AtP_qkv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to_tokens(clean_out_new, prepend_bos=False)[0], f'patches/{model_label}/{idx}_s2_clean_tokens.bin')\n",
    "torch.save(model.to_tokens(corr_out_new, prepend_bos=False)[0], f'patches/{model_label}/{idx}_s2_corr_tokens.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 - The right connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "attribute = question.split()[2][:-1]\n",
    "\n",
    "if 'not' in cot_gold[2]:\n",
    "    a_clean = [model.to_single_token(' not')]\n",
    "    a_corr = [model.to_single_token(' ' + attribute)]\n",
    "    clean_label = 'not'\n",
    "    corr_label = attribute\n",
    "else:\n",
    "    a_clean = [model.to_single_token(' ' + attribute)]\n",
    "    a_corr = [model.to_single_token(' not')]\n",
    "    clean_label = attribute\n",
    "    corr_label = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['3']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.566\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 3) | {clean_label} - {corr_label}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L12H0+', 'L14H0+', 'L14H4-', 'L14H5+', 'L14H6-', 'L16H7-', 'L17H7+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 - Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "a_clean = model.to_single_token(' ' + str(label))\n",
    "a_corr = model.to_single_token(' False' if label else ' True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.200\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, [a_clean], [a_corr], prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 4) | '{model.to_string(a_clean)}' - '{model.to_string(a_corr)}'\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L9H2+', 'L9H4+', 'L10H7+', 'L11H6-' 'L14H0+', 'L14H1-', 'L15H1+', 'L15H4-', 'L17H2+', 'L17H7-']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:41<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "stop_tokens = [' a',]\n",
    "\n",
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "    resid_dlas.append(resid_dla.cpu())\n",
    "    mlp_dlas.append(mlp_dla.cpu())\n",
    "    attn_dlas.append(attn_dla.cpu())\n",
    "    del resid_dla, mlp_dla, attn_dla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    for component in components:\n",
    "        x_clean = correct_preds['prompt'].iloc[idx]\n",
    "        cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "        cot_gold = ast.literal_eval(cot_gold)\n",
    "    \n",
    "        example = x_clean.split('\\n\\n')[-2]\n",
    "        context, question = example.split('Question: ')\n",
    "        \n",
    "        subject = question.split()[1]\n",
    "        species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "        species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "        \n",
    "        for id_, s in enumerate(species):\n",
    "            if s in cot_gold[0]:\n",
    "                break\n",
    "        \n",
    "        a_clean = species_token[id_].cpu()\n",
    "        a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "    \n",
    "        try:\n",
    "            assert len(species) == 2, \"More than two species detected!\"\n",
    "            assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "    \n",
    "            cot_corr = cot_gold.copy()\n",
    "            \n",
    "            cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "            context = x_clean.split('\\n\\n')\n",
    "            \n",
    "            context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "            x_corr = '\\n\\n'.join(context)\n",
    "    \n",
    "            clean_out = generate_until_stop(x_clean, stop_tokens)    \n",
    "            corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 1)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive AtP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [05:28<00:00, 12.62s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    cot_steps = cot_gold[1].split()\n",
    "    try:\n",
    "        for i, step in enumerate(cot_steps):\n",
    "            if \"is\" in step or \"are\" in step:\n",
    "                attribute = cot_steps[i+1].replace('.', '')\n",
    "                a_clean = ' ' + attribute\n",
    "        \n",
    "        a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "        a_clean = [model.to_single_token(a_clean)]\n",
    "        \n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 2) | attribute - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import einops\n",
    "\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "    test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "    context, clean_question = test_example.split('Question: ')\n",
    "    clues = context.split('. ')\n",
    "    \n",
    "    for s in all_species:\n",
    "        if s in clean_question.lower(): \n",
    "            s_star = s\n",
    "            break\n",
    "    \n",
    "    a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "    other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "    other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "    a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "    corr_question = clean_question.replace(a_clean, a_corr)\n",
    "    \n",
    "    for i, c in enumerate(clues):\n",
    "        if a_corr in c:\n",
    "            for s in all_species:\n",
    "                if s in c.lower(): break\n",
    "    \n",
    "            clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "    \n",
    "    clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "    corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question\n",
    "\n",
    "    for component in components:    \n",
    "        try:\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 2)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 2.838\n",
      "Corrupted logit difference: -3.524\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Corrupted logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:22<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    attribute = question.split()[2][:-1]\n",
    "\n",
    "    try:\n",
    "        if 'not' in cot_gold[2]:\n",
    "            a_clean = [model.to_single_token(' not')]\n",
    "            a_corr = [model.to_single_token(' ' + attribute)]\n",
    "            clean_label = 'not'\n",
    "            corr_label = attribute\n",
    "        else:\n",
    "            a_clean = [model.to_single_token(' ' + attribute)]\n",
    "            a_corr = [model.to_single_token(' not')]\n",
    "            clean_label = attribute\n",
    "            corr_label = 'not'\n",
    "\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 3) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [08:43<00:00, 20.14s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "    a_clean = model.to_single_token(' ' + str(label))\n",
    "    a_corr = model.to_single_token(' False' if label else ' True')\n",
    "        \n",
    "    try:\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0).unsqueeze(-1)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0).unsqueeze(-1)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 4) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA_agg.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
