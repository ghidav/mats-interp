{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c0495ffde2437e8a05589f12fe4f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name, torch_dtype=torch.float32, n_devices=1)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt):\n",
    "    return model.tokenizer.apply_chat_template([\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ], tokenize=False)\n",
    "\n",
    "def generate_until_stop(prompt, stop_tokens, max_tokens=64, verbose=False, prepend_bos=True):\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = [model.to_single_token(tok) for tok in stop_tokens]\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    gen = True\n",
    "    while gen:\n",
    "        with torch.no_grad():\n",
    "            new_tok = model(tokens).argmax(-1)[:, -1]\n",
    "        \n",
    "        if verbose: print(model.to_string(new_tok), end='')\n",
    "        tokens = torch.cat([tokens, new_tok[None].to(tokens.device)], dim=-1)\n",
    "        if new_tok.item() in stop_tokens or max_tokens == 0:\n",
    "            gen = False\n",
    "        max_tokens -= 1\n",
    "\n",
    "    return model.to_string(tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Let's think step-by-step:\n",
      "\n",
      "(1) Wren is a dumpus.\n",
      "(2) Dumpuses are bright.\n",
      "(3) Wren is bright.\n",
      "\n",
      "Answer: True<|eot_id|>"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer True or False to the following question. Answer as in the examples.\n",
    "\n",
    "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
    "Question: Is Polly muffled?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Polly is an impus.\n",
    "(2) Every impus is muffled.\n",
    "(3) Polly is muffled.\n",
    "Answer: True\n",
    "\n",
    "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
    "Question: Is Alex rainy?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Alex is a rompus.\n",
    "(2) Rompuses are rainy.\n",
    "(3) Alex is rainy.\n",
    "Answer: True\n",
    "\n",
    "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
    "Question: Is Fae slow?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Fae is a vumpus.\n",
    "(2) Each vumpus is not slow.\n",
    "(3) Fae is not slow.\n",
    "Answer: False\n",
    "\n",
    "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
    "Question: Is Wren bright?\n",
    "Think step-by-step.\"\"\"\n",
    "\n",
    "out = generate_until_stop(apply_template(prompt), stop_tokens=['<|eot_id|>'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cot(x):\n",
    "    try:\n",
    "        check = all([x == y for x, y in zip(x['cot_gold'], x['cot_pred'])])\n",
    "    except: check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = \"3shots\"\n",
    "\n",
    "result = pd.read_csv(f'results/results_{n_shots}.csv')\n",
    "result['correct_pred'] = result['label'] == result['pred']\n",
    "result['correct_cot'] = result.apply(check_cot, axis=1)\n",
    "\n",
    "correct_preds = result[result['correct_pred'] & result['correct_cot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
      "Question: Is Polly muffled?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is an impus.\n",
      "(2) Every impus is muffled.\n",
      "(3) Polly is muffled.\n",
      "Answer: True\n",
      "\n",
      "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
      "Question: Is Alex rainy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a rompus.\n",
      "(2) Rompuses are rainy.\n",
      "(3) Alex is rainy.\n",
      "Answer: True\n",
      "\n",
      "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
      "Question: Is Fae slow?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Fae is a vumpus.\n",
      "(2) Each vumpus is not slow.\n",
      "(3) Fae is not slow.\n",
      "Answer: False\n",
      "\n",
      "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
      "Question: Is Wren cold?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dla(prompt, component, a_clean, a_corr=None, prepend_bos=True):\n",
    "    \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    dlas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, component)\n",
    "\n",
    "    cache = ActivationCache(cache, model).to('cpu')\n",
    "    act = cache.stack_activation(component)[:, :, -1]\n",
    "    if len(act.shape) == 4:\n",
    "        act = cache.stack_head_results(-1)[:, :, -1]\n",
    "\n",
    "    dla = model.unembed(act.to(model.W_U.device)).cpu()\n",
    "    del cache\n",
    "    \n",
    "    return dla[..., a_clean].mean(-1) - dla[..., a_corr].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def plot_dla(resid_dla, mlp_dla, attn_dla, max_val=50):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Residual Stream\", \"MLP\", \"Attention Heads\"))\n",
    "\n",
    "    # Add images to the subplots\n",
    "    fig.add_trace(px.imshow(resid_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=1)\n",
    "    fig.add_trace(px.imshow(mlp_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=2)\n",
    "    fig.add_trace(px.imshow(attn_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=3)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='RdBu',\n",
    "            cmin=-max_val,\n",
    "            cmax=max_val\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title_text=\"Direct Logit Attribution\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_patterns(prompt, patterns, n_cols, query_offset, key_offset):\n",
    "    \n",
    "    n_rows = len(patterns) // n_cols + int(len(patterns) % n_cols != 0)\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=patterns)\n",
    "\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    labels = [f\"{tok} ({i})\" for i, tok in enumerate(str_tokens)]\n",
    "    query_labels = labels[query_offset:]\n",
    "    key_labels = labels[key_offset:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, 'attn')\n",
    "\n",
    "    for i, pid in enumerate(patterns):\n",
    "        layer, head = pid.split('H')\n",
    "        layer = int(layer[1:])\n",
    "        head = int(head[:-1])\n",
    "\n",
    "        pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head, query_offset:, key_offset:].cpu()\n",
    "\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "    \n",
    "        fig.add_trace(px.imshow(\n",
    "            pattern,\n",
    "            labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "            x=key_labels,\n",
    "            y=query_labels\n",
    "        ).data[0], row=row, col=col)\n",
    "\n",
    "        fig.update_xaxes(tickangle=35)\n",
    "        fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='Blues',\n",
    "            cmin=1,\n",
    "            cmax=0\n",
    "        ),\n",
    "        height=700 * n_rows,\n",
    "        width=800 * n_cols,\n",
    "        title_text=\"Attention Patterns\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def get_cache_fw_and_bw(tokens, a_clean, a_corr, corr_logits, component='all'):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "        \n",
    "    model.reset_hooks()\n",
    "    \n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(filter, bw_cache_hook, \"bwd\")\n",
    "\n",
    "    clean_logits = model(tokens).cpu()\n",
    "    value = logits_diff(clean_logits, a_clean, a_corr) #- logits_diff(corr_logits.cpu(), a_clean, a_corr)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG\n",
    "import torch\n",
    "import einops\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def attribution_patching(x_clean, x_corr, a_clean, a_corr, component, prepend_bos=True, method='standard', num_alphas=5, n_last_tokens=128):\n",
    "    if isinstance(x_clean, str):\n",
    "        clean_tokens = model.to_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        clean_tokens = x_clean\n",
    "\n",
    "    if isinstance(x_corr, str):\n",
    "        corr_tokens = model.to_tokens(x_corr, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        corr_tokens = x_corr\n",
    "\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "\n",
    "    if isinstance(a_corr, str):\n",
    "        a_corr = model.to_single_token(a_corr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corr_logits, corr_cache = get_cache_fw(corr_tokens, component)\n",
    "\n",
    "    if method == 'standard':\n",
    "        logits_diff_, clean_cache, clean_grad_cache = get_cache_fw_and_bw(clean_tokens, a_clean, a_corr, corr_logits, component=component)\n",
    "    elif method == 'ig':\n",
    "        with torch.no_grad():\n",
    "            clean_logits, clean_cache = get_cache_fw(clean_tokens, component)\n",
    "\n",
    "    clean_cache = ActivationCache(clean_cache, model).to('cpu')\n",
    "    corr_cache = ActivationCache(corr_cache, model).to('cpu')\n",
    "\n",
    "    corr_act = clean_cache.stack_activation(component)[:, 0, -n_last_tokens:]\n",
    "    clean_act = corr_cache.stack_activation(component)[:, 0, -n_last_tokens:] # comp, pos dm\n",
    "    del clean_cache, corr_cache\n",
    "    \n",
    "    if clean_act.ndim > 3:\n",
    "        clean_act = clean_act.reshape(-1, clean_act.size(1), clean_act.size(3))\n",
    "        corr_act = corr_act.reshape(-1, corr_act.size(1), corr_act.size(3))\n",
    "        \n",
    "    if method == 'standard':\n",
    "        clean_grad_cache = ActivationCache(clean_grad_cache, model).to('cpu')\n",
    "        clean_grad_act = clean_grad_cache.stack_activation(component).squeeze()\n",
    "        if clean_grad_act.ndim > 3:\n",
    "            clean_grad_act = clean_grad_act.reshape(-1, clean_grad_act.size(1), clean_grad_act.size(3))\n",
    "        clean_grad_act = clean_grad_act[:, -n_last_tokens:].cpu()\n",
    "    elif method == 'ig':\n",
    "        clean_grad_act = []\n",
    "        alphas = torch.linspace(0, 1, num_alphas)\n",
    "        k = clean_act.shape[0] // model.cfg.n_layers\n",
    "        for l in tqdm(range(model.cfg.n_layers)):\n",
    "            ig_patch = torch.zeros_like(clean_act[k*l:k*(l+1)], device=clean_act.device)\n",
    "            for alpha in alphas:\n",
    "                a_alpha = alpha * clean_act[k*l:k*(l+1)] + (1 - alpha) * corr_act[k*l:k*(l+1)]\n",
    "                logits_alpha, grad_alpha = get_cache_fw_with_modified_activations(clean_tokens, a_alpha, a_clean, a_corr, l, component)\n",
    "                if grad_alpha.ndim > 3:\n",
    "                    grad_alpha = grad_alpha.reshape(-1, grad_alpha.size(1), grad_alpha.size(3))\n",
    "                grad_alpha = grad_alpha[:, -n_last_tokens:].cpu()\n",
    "                ig_patch += grad_alpha * (clean_act[k*l:k*(l+1)] - corr_act[k*l:k*(l+1)])\n",
    "                del a_alpha, logits_alpha, grad_alpha\n",
    "            clean_grad_act.append(ig_patch / num_alphas)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        clean_grad_act = torch.cat(clean_grad_act, dim=0)\n",
    "\n",
    "    print(\"Gradients collected! Computing the patch...\")\n",
    "    patch = einops.reduce(\n",
    "        clean_grad_act * (corr_act - clean_act),\n",
    "        \"component pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    del clean_act, corr_act, clean_grad_act\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return patch\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    filter = lambda name: utils.get_act_name(component) in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def logits_diff(logits, a_clean, a_corr=None):\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "    if a_corr:\n",
    "        if isinstance(a_corr, str):\n",
    "            a_corr = [model.to_single_token(a_corr)]\n",
    "        \n",
    "        return logits[0, -1, a_clean] - logits[0, -1, a_corr].mean(-1)\n",
    "    else:\n",
    "        return logits[0, -1, a_clean]\n",
    "\n",
    "def get_cache_fw_with_modified_activations(tokens, x_int, a_clean, a_corr, layer, component):\n",
    "    hook_point = utils.get_act_name(component, layer)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    def fw_hook(act, mod_act, hook):\n",
    "        act = mod_act\n",
    "\n",
    "    fw_hook_fn = partial(fw_hook, mod_act=x_int.squeeze())\n",
    "    model.add_hook(hook_point, fw_hook_fn, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(hook_point, bw_cache_hook, \"bwd\")\n",
    "    logits = model(tokens)\n",
    "    value = logits_diff(logits, a_corr, a_clean)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return value.item(), grad_cache[hook_point]\n",
    "\n",
    "def plot_atp(atp, x_clean, component, n_last_tokens=128, val=1, prepend_bos=True):\n",
    "\n",
    "    str_tokens = model.to_str_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    xs = [f\"{tok} | {i}\" for i, tok in enumerate(str_tokens[-n_last_tokens:])]\n",
    "    \n",
    "    if component in ['z', 'q']:\n",
    "        ys = [f'L{i}H{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "    else:\n",
    "        ys = [f\"L{l} {component.upper()}\" for l in range(model.cfg.n_layers)]\n",
    "        \n",
    "    fig = px.imshow(\n",
    "        atp[:, -n_last_tokens:].cpu().numpy(), \n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        color_continuous_scale='RdBu', zmin=-val, zmax=val, aspect='auto'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtasks\n",
    "\n",
    "We then explore each subtask mechanistically to understand which are the components responsible for each choice made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = ['grimpus', 'lorpus', 'wumpus', 'zumpus', 'sterpus', 'numpus', 'jompus', 'brimpus', 'yumpus', 'tumpus', 'dumpus', 'vumpus', 'rompus', 'lempus', 'gorpus', 'shumpus', 'impus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "data = result # or correct_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 - Choosing the right species\n",
    "The first step is choosing the right species to focus on. This is a key step since it preceeds the attribute check. It is also the most difficult one since the choice doesn't depend only on the species and the entity, but has to be made already considering the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = data['prompt'].iloc[idx]\n",
    "cot_gold = data['cot_gold'].iloc[idx]\n",
    "label = data['label'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "\n",
    "for id_, s in enumerate(species):\n",
    "    if s in cot_gold[0]:\n",
    "        break\n",
    "\n",
    "a_clean = species_token[id_].cpu()\n",
    "a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean_ = apply_template(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Let's think step-by-step:\n",
      "\n",
      "(1) Polly is a"
     ]
    }
   ],
   "source": [
    "stop_tokens = [' a', ' an']\n",
    "clean_out = generate_until_stop(x_clean_, stop_tokens, prepend_bos=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla = compute_dla(clean_out, 'resid_pre', a_clean, a_corr, prepend_bos=False)\n",
    "mlp_dla = compute_dla(clean_out, 'mlp_out', a_clean, a_corr, prepend_bos=False)\n",
    "attn_dla = compute_dla(clean_out, 'result', a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla.reshape(model.cfg.n_layers, -1), max_val=5)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L10H7+', 'L14H0+', 'L14H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=2, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "assert len(species) == 2, \"More than two species detected!\"\n",
    "assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_corr = cot_gold.copy()\n",
    "\n",
    "cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "context = x_clean.split('\\n\\n')\n",
    "\n",
    "context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "x_corr = '\\n\\n'.join(context)\n",
    "x_corr_ = apply_template(x_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Let's think step-by-step:\n",
      "\n",
      "(1) Polly is an"
     ]
    }
   ],
   "source": [
    "corr_out = generate_until_stop(x_corr_, stop_tokens, prepend_bos=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m atp_rsp \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_patching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresid_pre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstandard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_last_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mattribution_patching\u001b[0;34m(x_clean, x_corr, a_clean, a_corr, component, prepend_bos, method, num_alphas, n_last_tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m     corr_logits, corr_cache \u001b[38;5;241m=\u001b[39m get_cache_fw(corr_tokens, component)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     logits_diff_, clean_cache, clean_grad_cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_cache_fw_and_bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mig\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mget_cache_fw_and_bw\u001b[0;34m(tokens, a_clean, a_corr, corr_logits, component)\u001b[0m\n\u001b[1;32m     45\u001b[0m clean_logits \u001b[38;5;241m=\u001b[39m model(tokens)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     46\u001b[0m value \u001b[38;5;241m=\u001b[39m logits_diff(clean_logits, a_clean, a_corr) \u001b[38;5;66;03m#- logits_diff(corr_logits.cpu(), a_clean, a_corr)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     51\u001b[0m     value\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     52\u001b[0m     ActivationCache(cache, model),\n\u001b[1;32m     53\u001b[0m     ActivationCache(grad_cache, model),\n\u001b[1;32m     54\u001b[0m )\n",
      "File \u001b[0;32m/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU "
     ]
    }
   ],
   "source": [
    "atp_rsp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='resid_pre', prepend_bos=False, method='standard', n_last_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256, **kwargs):\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Queries\", \"Keys\", \"Values\"), shared_xaxes=True, vertical_spacing=0.05)\n",
    "    \n",
    "    for i, (atp, hook) in enumerate(zip([atp_q, atp_k, atp_v], ['q', 'k', 'v'])):\n",
    "        plot = plot_atp(atp, clean_out, component=hook, prepend_bos=False, n_last_tokens=n_last_tokens)\n",
    "        for trace in plot.data:\n",
    "            fig.add_trace(trace, row=1+i, col=1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        coloraxis1=dict(colorscale='RdBu', cmin=-0.5, cmax=0.5),\n",
    "        showlegend=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 torch.Size([144, 256])\n",
      "256 torch.Size([18, 256])\n",
      "256 torch.Size([18, 256])\n"
     ]
    }
   ],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256)\n",
    "fig.write_html(\"fig/s1_AtP_qkv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [02:48<00:00,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:53<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:53<00:00,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v)\n",
    "fig.write_html(f\"fig/s1_AtP_qkv_ig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:50,  5.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m a_clean \u001b[38;5;241m=\u001b[39m species_token[id_]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     20\u001b[0m a_corr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(species_token[:id_] \u001b[38;5;241m+\u001b[39m species_token[id_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 22\u001b[0m clean_out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_until_stop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m cot_corr \u001b[38;5;241m=\u001b[39m cot_gold\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m cot_corr \u001b[38;5;241m=\u001b[39m [step\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(species[id_][\u001b[38;5;241m1\u001b[39m:], species[\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mid_][\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m cot_gold]\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mgenerate_until_stop\u001b[0;34m(prompt, stop_tokens, max_tokens, verbose, prepend_bos)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mto_string(new_tok), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([tokens, new_tok[\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(tokens\u001b[38;5;241m.\u001b[39mdevice)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnew_tok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m stop_tokens \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m max_tokens \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_last_tokens = 512\n",
    "\n",
    "for idx in tqdm(range(10)):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    cot_corr = cot_gold.copy()\n",
    "\n",
    "    cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "    context = x_clean.split('\\n\\n')\n",
    "    \n",
    "    context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "    x_corr = '\\n\\n'.join(context)\n",
    "    corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "    try:\n",
    "        assert len(species) == 2, \"More than two species detected!\"\n",
    "        assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv.html\")\n",
    "\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:46<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "stop_tokens = [' a',]\n",
    "n_last_tokens = 512\n",
    "\n",
    "for idx in tqdm(range(10)):\n",
    "    idx = 2\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "\n",
    "    subject  = cot_gold[0].split()[0]\n",
    "    a_clean = cot_gold[0].split()[-1][:-1]\n",
    "    \n",
    "    icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "    test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "    context, clean_question = test_example.split('Question: ')\n",
    "    clues = context.split('. ')\n",
    "\n",
    "    for c in clues:\n",
    "        if subject in c and a_clean not in c:\n",
    "            a_corr = c.split()[-1][:-1]\n",
    "\n",
    "    clean_attr = clean_question.split()[2][:-1]\n",
    "    \n",
    "    for c in clues:\n",
    "        if a_corr in c.lower():\n",
    "            corr_attr = c.split()[-1]\n",
    "            if not_a_species(corr_attr):\n",
    "                break\n",
    "\n",
    "    corr_question = clean_question.replace(clean_attr, corr_attr)\n",
    "    \n",
    "    clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + clean_question\n",
    "    corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + corr_question\n",
    "    \n",
    "    a_clean = model.to_tokens(' ' + a_clean, prepend_bos=False)[:, 0].cpu()\n",
    "    a_corr = model.to_tokens(' ' + a_corr, prepend_bos=False)[:, 0].cpu()\n",
    "\n",
    "    assert len(model.to_tokens(' ' + clean_attr)[0]) == len(model.to_tokens(' ' + corr_attr)[0]), \"Attributes with different shapes!\"\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        assert len(species) == 2, \"More than two species detected!\"\n",
    "        assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        \n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv.html\")\n",
    "        \n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        \n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")\n",
    "    except: pass\n",
    "    '''\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc2f09d4e1b48eba08c34f206f75ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am interested in [A 3 bedroom house in a quiet area]\n",
      "A 3 bedroom house in a quiet area\n",
      "A 3 bedroom house in\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"Hello\", 32, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Wumpuses are shumpuses. Sterpuses are not discordant. Each yumpus is feisty. Each wumpus is a yumpus. Each yumpus is a lempus. Each yumpus is a sterpus. Each sterpus is a vumpus. Lempuses are fast. Shumpuses are large. Impuses are not metallic. Each zumpus is dull. Each zumpus is a brimpus. Every grimpus is an impus. Every grimpus is a gorpus. Vumpuses are not hot. Grimpuses are sunny. Wumpuses are not mean. Sterpuses are grimpuses. Fae is a zumpus. Fae is a grimpus.\n",
      "Question: Is Fae sunny?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Fae is a grimpus.\n",
      "(2) Grimpuses are sunny.\n",
      "(3) Fae is sunny.\n",
      "Answer: True\n",
      "\n",
      "Impuses are lorpuses. Jompuses are not windy. Each brimpus is a jompus. Each sterpus is opaque. Every jompus is an impus. Every numpus is cold. Impuses are lempuses. Lempuses are not happy. Each impus is luminous. Numpuses are wumpuses. Brimpuses are dull. Jompuses are sterpuses. Every zumpus is melodic. Every brimpus is a zumpus. Sam is a brimpus. Sam is a numpus.\n",
      "Question: Is Sam dull?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is a brimpus.\n",
      "(2) Brimpuses are dull.\n",
      "(3) Sam is dull.\n",
      "Answer: True\n",
      "\n",
      "Dumpuses are not muffled. Each zumpus is a sterpus. Each numpus is a jompus. Each zumpus is brown. Every sterpus is not large. Every zumpus is a dumpus. Every numpus is metallic. Every dumpus is a tumpus. Lorpuses are not temperate. Every dumpus is a lorpus. Wren is a dumpus. Wren is a numpus.\n",
      "Question: Is Wren muffled?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Wren is a dumpus.\n",
      "(2) Dumpuses are not muffled.\n",
      "(3) Wren is not muffled.\n",
      "Answer: False\n",
      "\n",
      "Every gorpus is a rompus. Each dumpus is a wumpus. Each dumpus is liquid. Every gorpus is transparent. Dumpuses are tumpuses. Every sterpus is a dumpus. Lempuses are not small. Sterpuses are lempuses. Every wumpus is not orange. Sterpuses are earthy. Sam is a gorpus. Sam is a dumpus.\n",
      "Question: Is Sam liquid?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is a dumpus.\n",
      "(2) Each dumpus is liquid.\n",
      "(3) Sam is liquid.\n",
      "Answer: True\n",
      "\n",
      "Each numpus is cold. Numpuses are impuses. Numpuses are zumpuses. Each rompus is windy. Rompuses are wumpuses. Impuses are small. Rex is a rompus. Rex is a numpus.\n",
      "Question: Is Rex cold?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Rex is a numpus.\n",
      "(2) Each numpus is cold.\n",
      "(3) Rex is cold.\n",
      "Answer: True\n",
      "\n",
      "Each tumpus is brown. Every wumpus is floral. Tumpuses are yumpuses. Every vumpus is not small. Every dumpus is a sterpus. Dumpuses are not loud. Each zumpus is happy. Jompuses are fast. Every wumpus is a rompus. Each dumpus is a vumpus. Every sterpus is a wumpus. Wumpuses are jompuses. Sterpuses are spicy. Every sterpus is a zumpus. Sally is a dumpus. Sally is a tumpus.\n",
      "Question: Is Sally loud?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sally is a dumpus.\n",
      "(2) Dumpuses are not loud.\n",
      "(3) Sally is not loud.\n",
      "Answer: False\n",
      "\n",
      "Numpuses are slow. Every lempus is a vumpus. Each lempus is loud. Dumpuses are not opaque. Dumpuses are numpuses. Dumpuses are lorpuses. Sally is a lempus. Sally is a dumpus.\n",
      "Question: Is Sally opaque?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sally is a dumpus.\n",
      "(2) Dumpuses are not opaque.\n",
      "(3) Sally is not opaque.\n",
      "Answer: False\n",
      "\n",
      "Each jompus is dull. Tumpuses are bitter. Each jompus is a zumpus. Vumpuses are rompuses. Each vumpus is not fast. Every jompus is a tumpus. Rex is a jompus. Rex is a vumpus.\n",
      "Question: Is Rex dull?\n",
      "Think step-by-step.\n",
      "\n",
      "Answer..ComponentPlacement..ComponentPlacement.:// the same.://.ComponentPlacement..swinge\n",
      "Questioning the same.\n",
      "Question..ComponentPlacement..ComponentPlacement.://.swinging the\n",
      "Question is a\n"
     ]
    }
   ],
   "source": [
    "print(clean_out_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(corr_out_new, 32, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 1.204\n",
      "Clean logit difference: 1.208\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8704]), ' gri', tensor([21430]), ' dump')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_clean, model.to_string(a_clean), a_corr, model.to_string(a_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(678), ' j')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits[0, -1].argmax(), model.to_string(clean_logits[0, -1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Yumpuses are grimpuses. Impuses are sterpuses. Sterpuses are dumpuses. Each sterpus is a numpus. Each impus is windy. Lempuses are melodic. Rompuses are lorpuses. Rompuses are not earthy. Numpuses are sweet. Rompuses are impuses. Sterpuses are transparent. Every yumpus is not dull. Lorpuses are luminous. Impuses are lempuses. Alex is a rompus. Alex is a yumpus.\n",
      "Question: Is Alex earthy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a rompus.\n",
      "(2) Rompuses are not earthy.\n",
      "(3) Alex is not earthy.\n",
      "Answer: False\n",
      "\n",
      "Every impus is a tumpus. Sterpuses are jompuses. Impuses are grimpuses. Each sterpus is not large. Grimpuses are not fast. Every impus is not red. Lorpuses are not happy. Every tumpus is a rompus. Every tumpus is a lorpus. Every lorpus is a sterpus. Rompuses are metallic. Each tumpus is muffled. Every jompus is not aggressive. Gorpuses are sunny. Every lorpus is a gorpus. Shumpuses are earthy. Shumpuses are zumpuses. Every sterpus is a yumpus. Alex is a shumpus. Alex is a sterpus.\n",
      "Question: Is Alex large?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a sterpus.\n",
      "(2) Each sterpus is not large.\n",
      "(3) Alex is not large.\n",
      "Answer: False\n",
      "\n",
      "Each tumpus is a brimpus. Wumpuses are shumpuses. Each shumpus is fast. Each jompus is a gorpus. Every tumpus is dull. Wumpuses are shy. Jompuses are not transparent. Jompuses are lempuses. Gorpuses are orange. Each wumpus is a jompus. Max is a wumpus. Max is a tumpus.\n",
      "Question: Is Max shy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Max is a wumpus.\n",
      "(2) Wumpuses are shy.\n",
      "(3) Max is shy.\n",
      "Answer: True\n",
      "\n",
      "Shumpuses are slow. Each wumpus is an impus. Every grimpus is not mean. Each grimpus is a shumpus. Each jompus is a lorpus. Vumpuses are loud. Every jompus is opaque. Impuses are earthy. Every vumpus is a lempus. Each grimpus is a vumpus. Every vumpus is a brimpus. Wumpuses are grimpuses. Each lempus is nervous. Each wumpus is not wooden. Alex is a jompus. Alex is a grimpus.\n",
      "Question: Is Alex mean?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a grimpus.\n",
      "(2) Every grimpus is not mean.\n",
      "(3) Alex is not mean.\n",
      "Answer: False\n",
      "\n",
      "Each brimpus is a zumpus. Every lempus is feisty. Every brimpus is a lempus. Grimpuses are sterpuses. Every dumpus is not dull. Each sterpus is not overcast. Every grimpus is a tumpus. Every wumpus is a brimpus. Each zumpus is not fruity. Each lempus is a grimpus. Every lempus is an impus. Each jompus is not small. Numpuses are kind. Every brimpus is liquid. Tumpuses are temperate. Each dumpus is a shumpus. Every tumpus is a gorpus. Each tumpus is a numpus. Every impus is melodic. Wumpuses are jompuses. Grimpuses are not sour. Each wumpus is not transparent. Polly is a grimpus. Polly is a dumpus.\n",
      "Question: Is Polly sour?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is a grimpus.\n",
      "(2) Grimpuses are not sour.\n",
      "(3) Polly is not sour.\n",
      "Answer: False\n",
      "\n",
      "Tumpuses are not bright. Grimpuses are not cold. Tumpuses are numpuses. Every numpus is metallic. Tumpuses are lorpuses. Grimpuses are zumpuses. Wren is a tumpus. Wren is a grimpus.\n",
      "Question: Is Wren bright?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Wren is a tumpus.\n",
      "(2) Tumpuses are not bright.\n",
      "(3) Wren is not bright.\n",
      "Answer: False\n",
      "\n",
      "Every wumpus is a dumpus. Lempuses are liquid. Every brimpus is not brown. Tumpuses are lempuses. Every impus is not windy. Each lorpus is a brimpus. Tumpuses are shumpuses. Every shumpus is cold. Each rompus is fast. Every rompus is a wumpus. Each shumpus is a gorpus. Zumpuses are yumpuses. Every wumpus is not feisty. Gorpuses are floral. Wumpuses are numpuses. Shumpuses are rompuses. Zumpuses are not transparent. Lorpuses are tumpuses. Tumpuses are not muffled. Rompuses are impuses. Dumpuses are aggressive. Lorpuses are bright. Max is a zumpus. Max is a shumpus.\n",
      "Question: Is Max cold?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Max is a shumpus.\n",
      "(2) Every shumpus is cold.\n",
      "(3) Max is cold.\n",
      "Answer: True\n",
      "\n",
      "Impuses are not orange. Each rompus is a tumpus. Each lorpus is a dumpus. Each tumpus is large. Every tumpus is a lorpus. Lorpuses are not luminous. Numpuses are not bitter. Every impus is a jompus. Lorpuses are numpuses. Every numpus is a yumpus. Lempuses are melodic. Vumpuses are not angry. Each tumpus is a vumpus. Each yumpus is bright. Rompuses are lempuses. Dumpuses are not hot. Numpuses are zumpuses. Every rompus is not fast. Polly is a lorpus. Polly is an impus.\n",
      "Question: Is Polly orange?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is a lorpus.\n",
      "(2) Lorpuses are not orange.\n",
      "(3) Polly is not orange.\n",
      "Answer: False\n"
     ]
    }
   ],
   "source": [
    "print(generate_until_stop(corr_out_new, stop_tokens=[' True', ' False'], prepend_bos=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 - Attribute check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['2']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is', ' are']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_steps = cot_gold[1].split()\n",
    "for i, step in enumerate(cot_steps):\n",
    "    if \"is\" in step or \"are\" in step:\n",
    "        attribute = cot_steps[i+1].replace('.', '')\n",
    "        a_clean = ' ' + attribute\n",
    "\n",
    "a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "a_clean = [model.to_single_token(a_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 3.683\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 2) | {attribute} - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L8H6-', 'L10H5+', 'L10H7+', 'L13H2+', 'L13H4+', 'L14H0+', 'L14H3-', 'L14H4+', 'L16H0+', 'L16H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "import random\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for s in all_species:\n",
    "    if s in clean_question.lower(): \n",
    "        s_star = s\n",
    "        break\n",
    "    \n",
    "a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "corr_question = clean_question.replace(a_clean, a_corr)\n",
    "\n",
    "for i, c in enumerate(clues):\n",
    "    if a_corr in c:\n",
    "        for s in all_species:\n",
    "            if s in c.lower(): break\n",
    "\n",
    "        clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Answer True or False to the following question. Answer as in the examples.\\n\\nYumpuses are grimpuses. Impuses are sterpuses. Sterpuses are dumpuses. Each sterpus is a numpus. Each impus is windy. Lempuses are melodic. Rompuses are lorpuses. Rompuses are not earthy. Numpuses are sweet. Rompuses are impuses. Sterpuses are transparent. Every yumpus is not dull. Lorpuses are luminous. Impuses are lempuses. Alex is a rompus. Alex is a yumpus.\\nQuestion: Is Alex earthy?\\nThink step-by-step.\\n\\n(1) Alex is a rompus.\\n(2) Rompuses are not earthy.\\n(3) Alex is not earthy.\\nAnswer: False\\n\\nEvery impus is a tumpus. Sterpuses are jompuses. Impuses are grimpuses. Each sterpus is not large. Grimpuses are not fast. Every impus is not red. Lorpuses are not happy. Every tumpus is a rompus. Every tumpus is a lorpus. Every lorpus is a sterpus. Rompuses are metallic. Each tumpus is muffled. Every jompus is not aggressive. Gorpuses are sunny. Every lorpus is a gorpus. Shumpuses are earthy. Shumpuses are zumpuses. Every sterpus is a yumpus. Alex is a shumpus. Alex is a sterpus.\\nQuestion: Is Alex large?\\nThink step-by-step.\\n\\n(1) Alex is a sterpus.\\n(2) Each sterpus is not large.\\n(3) Alex is not large.\\nAnswer: False\\n\\nEach tumpus is a brimpus. Wumpuses are shumpuses. Each shumpus is fast. Each jompus is a gorpus. Every tumpus is dull. Wumpuses are shy. Jompuses are not transparent. Jompuses are lempuses. Gorpuses are orange. Each wumpus is a jompus. Max is a wumpus. Max is a tumpus.\\nQuestion: Is Max shy?\\nThink step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are shy.\\n(3) Max is shy.\\nAnswer: True\\n\\nShumpuses are slow. Each wumpus is an impus. Every grimpus is not mean. Each grimpus is a shumpus. Each jompus is a lorpus. Vumpuses are loud. Every jompus is opaque. Impuses are earthy. Every vumpus is a lempus. Each grimpus is a vumpus. Every vumpus is a brimpus. Wumpuses are grimpuses. Each lempus is nervous. Each wumpus is not wooden. Alex is a jompus. Alex is a grimpus.\\nQuestion: Is Alex mean?\\nThink step-by-step.\\n\\n(1) Alex is a grimpus.\\n(2) Every grimpus is not mean.\\n(3) Alex is not mean.\\nAnswer: False\\n\\nEach brimpus is a zumpus. Every lempus is feisty. Every brimpus is a lempus. Grimpuses are sterpuses. Every dumpus is not dull. Each sterpus is not overcast. Every grimpus is a tumpus. Every wumpus is a brimpus. Each zumpus is not fruity. Each lempus is a grimpus. Every lempus is an impus. Each jompus is not small. Numpuses are kind. Every brimpus is liquid. Tumpuses are temperate. Each dumpus is a shumpus. Every tumpus is a gorpus. Each tumpus is a numpus. Every impus is melodic. Wumpuses are jompuses. Grimpuses are not sour. Each wumpus is not transparent. Polly is a grimpus. Polly is a dumpus.\\nQuestion: Is Polly sour?\\nThink step-by-step.\\n\\n(1) Polly is a grimpus.\\n(2) Grimpuses are not sour.\\n(3) Polly is not sour.\\nAnswer: False\\n\\nTumpuses are not bright. Grimpuses are not cold. Tumpuses are numpuses. Every numpus is metallic. Tumpuses are lorpuses. Grimpuses are zumpuses. Wren is a tumpus. Wren is a grimpus.\\nQuestion: Is Wren bright?\\nThink step-by-step.\\n\\n(1) Wren is a tumpus.\\n(2) Tumpuses are not bright.\\n(3) Wren is not bright.\\nAnswer: False\\n\\nEvery wumpus is a dumpus. Lempuses are liquid. Every brimpus is not brown. Tumpuses are lempuses. Every impus is not windy. Each lorpus is a brimpus. Tumpuses are shumpuses. Every shumpus is cold. Each rompus is fast. Every rompus is a wumpus. Each shumpus is a gorpus. Zumpuses are yumpuses. Every wumpus is not feisty. Gorpuses are floral. Wumpuses are numpuses. Shumpuses are rompuses. Zumpuses are not transparent. Lorpuses are tumpuses. Tumpuses are not muffled. Rompuses are impuses. Dumpuses are aggressive. Lorpuses are bright. Max is a zumpus. Max is a shumpus.\\nQuestion: Is Max cold?\\nThink step-by-step.\\n\\n(1) Max is a shumpus.\\n(2) Every shumpus is cold.\\n(3) Max is cold.\\nAnswer: True\\n\\nImpuses are not orange. Each rompus is a tumpus. Each lorpus is a dumpus. Each tumpus is large. Every tumpus is a lorpus. Lorpuses are not luminous. Numpuses are not bitter. Every impus is a jompus. Lorpuses are numpuses. Every numpus is a yumpus. Lempuses are melodic. Vumpuses are not angry. Each tumpus is a vumpus. Each yumpus is bright. Rompuses are lempuses. Dumpuses are not hot. Numpuses are zumpuses. Every lorpus is not fast. Polly is a lorpus. Polly is an impus.\\nIs Polly fast?\\nThink step-by-step.\\n\\n(1) Polly is a lorpus.\\n(2) Lorpuses are'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_out_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='resid', prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_atp(atp, clean_out_new, component=\"resid\", prepend_bos=False)\n",
    "fig.write_html(\"fig/s2_AtP_resid.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 - The right connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "attribute = question.split()[2][:-1]\n",
    "\n",
    "if 'not' in cot_gold[2]:\n",
    "    a_clean = [model.to_single_token(' not')]\n",
    "    a_corr = [model.to_single_token(' ' + attribute)]\n",
    "    clean_label = 'not'\n",
    "    corr_label = attribute\n",
    "else:\n",
    "    a_clean = [model.to_single_token(' ' + attribute)]\n",
    "    a_corr = [model.to_single_token(' not')]\n",
    "    clean_label = attribute\n",
    "    corr_label = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['3']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.566\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 3) | {clean_label} - {corr_label}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L12H0+', 'L14H0+', 'L14H4-', 'L14H5+', 'L14H6-', 'L16H7-', 'L17H7+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 - Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "a_clean = model.to_single_token(' ' + str(label))\n",
    "a_corr = model.to_single_token(' False' if label else ' True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.200\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, [a_clean], [a_corr], prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 4) | '{model.to_string(a_clean)}' - '{model.to_string(a_corr)}'\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L9H2+', 'L9H4+', 'L10H7+', 'L11H6-' 'L14H0+', 'L14H1-', 'L15H1+', 'L15H4-', 'L17H2+', 'L17H7-']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:41<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "stop_tokens = [' a',]\n",
    "\n",
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "    resid_dlas.append(resid_dla.cpu())\n",
    "    mlp_dlas.append(mlp_dla.cpu())\n",
    "    attn_dlas.append(attn_dla.cpu())\n",
    "    del resid_dla, mlp_dla, attn_dla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    for component in components:\n",
    "        x_clean = correct_preds['prompt'].iloc[idx]\n",
    "        cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "        cot_gold = ast.literal_eval(cot_gold)\n",
    "    \n",
    "        example = x_clean.split('\\n\\n')[-2]\n",
    "        context, question = example.split('Question: ')\n",
    "        \n",
    "        subject = question.split()[1]\n",
    "        species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "        species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "        \n",
    "        for id_, s in enumerate(species):\n",
    "            if s in cot_gold[0]:\n",
    "                break\n",
    "        \n",
    "        a_clean = species_token[id_].cpu()\n",
    "        a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "    \n",
    "        try:\n",
    "            assert len(species) == 2, \"More than two species detected!\"\n",
    "            assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "    \n",
    "            cot_corr = cot_gold.copy()\n",
    "            \n",
    "            cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "            context = x_clean.split('\\n\\n')\n",
    "            \n",
    "            context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "            x_corr = '\\n\\n'.join(context)\n",
    "    \n",
    "            clean_out = generate_until_stop(x_clean, stop_tokens)    \n",
    "            corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 1)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive AtP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [05:28<00:00, 12.62s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    cot_steps = cot_gold[1].split()\n",
    "    try:\n",
    "        for i, step in enumerate(cot_steps):\n",
    "            if \"is\" in step or \"are\" in step:\n",
    "                attribute = cot_steps[i+1].replace('.', '')\n",
    "                a_clean = ' ' + attribute\n",
    "        \n",
    "        a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "        a_clean = [model.to_single_token(a_clean)]\n",
    "        \n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 2) | attribute - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import einops\n",
    "\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "    test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "    context, clean_question = test_example.split('Question: ')\n",
    "    clues = context.split('. ')\n",
    "    \n",
    "    for s in all_species:\n",
    "        if s in clean_question.lower(): \n",
    "            s_star = s\n",
    "            break\n",
    "    \n",
    "    a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "    other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "    other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "    a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "    corr_question = clean_question.replace(a_clean, a_corr)\n",
    "    \n",
    "    for i, c in enumerate(clues):\n",
    "        if a_corr in c:\n",
    "            for s in all_species:\n",
    "                if s in c.lower(): break\n",
    "    \n",
    "            clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "    \n",
    "    clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "    corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question\n",
    "\n",
    "    for component in components:    \n",
    "        try:\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 2)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 2.838\n",
      "Corrupted logit difference: -3.524\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Corrupted logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:22<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    attribute = question.split()[2][:-1]\n",
    "\n",
    "    try:\n",
    "        if 'not' in cot_gold[2]:\n",
    "            a_clean = [model.to_single_token(' not')]\n",
    "            a_corr = [model.to_single_token(' ' + attribute)]\n",
    "            clean_label = 'not'\n",
    "            corr_label = attribute\n",
    "        else:\n",
    "            a_clean = [model.to_single_token(' ' + attribute)]\n",
    "            a_corr = [model.to_single_token(' not')]\n",
    "            clean_label = attribute\n",
    "            corr_label = 'not'\n",
    "\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 3) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [08:43<00:00, 20.14s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "    a_clean = model.to_single_token(' ' + str(label))\n",
    "    a_corr = model.to_single_token(' False' if label else ' True')\n",
    "        \n",
    "    try:\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0).unsqueeze(-1)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0).unsqueeze(-1)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 4) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA_agg.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
