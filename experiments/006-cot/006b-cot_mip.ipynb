{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model_name = 'google/gemma-2b'\n",
    "\n",
    "model_label = model_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c863912921a435f96f7f94335af0a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name, torch_dtype=torch.float32, n_devices=1)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt):\n",
    "    return model.tokenizer.apply_chat_template([\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ], tokenize=False)\n",
    "\n",
    "def generate_until_stop(prompt, stop_tokens, max_tokens=64, verbose=False, prepend_bos=True):\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = [model.to_single_token(tok) for tok in stop_tokens]\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    gen = True\n",
    "    while gen:\n",
    "        with torch.no_grad():\n",
    "            new_tok = model(tokens).argmax(-1)[:, -1]\n",
    "        \n",
    "        if verbose: print(model.to_string(new_tok), end='')\n",
    "        tokens = torch.cat([tokens, new_tok[None].to(tokens.device)], dim=-1)\n",
    "        if new_tok.item() in stop_tokens or max_tokens == 0:\n",
    "            gen = False\n",
    "        max_tokens -= 1\n",
    "\n",
    "    return model.to_string(tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Wren is a dumpus.\n",
      "(2) Every dumpus is a jompus.\n",
      "(3) Wren is a jompus.\n",
      "Answer: True"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer True or False to the following question. Answer as in the examples.\n",
    "\n",
    "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
    "Question: Is Polly muffled?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Polly is an impus.\n",
    "(2) Every impus is muffled.\n",
    "(3) Polly is muffled.\n",
    "Answer: True\n",
    "\n",
    "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
    "Question: Is Alex rainy?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Alex is a rompus.\n",
    "(2) Rompuses are rainy.\n",
    "(3) Alex is rainy.\n",
    "Answer: True\n",
    "\n",
    "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
    "Question: Is Fae slow?\n",
    "Think step-by-step.\n",
    "\n",
    "(1) Fae is a vumpus.\n",
    "(2) Each vumpus is not slow.\n",
    "(3) Fae is not slow.\n",
    "Answer: False\n",
    "\n",
    "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
    "Question: Is Wren bright?\n",
    "Think step-by-step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#out = generate_until_stop(apply_template(prompt), stop_tokens=['<|eot_id|>'], verbose=True)\n",
    "out = generate_until_stop(prompt, stop_tokens=[' True', ' False'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cot(x):\n",
    "    try:\n",
    "        check = all([x == y for x, y in zip(x['cot_gold'], x['cot_pred'])])\n",
    "    except: check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = \"7shots\"\n",
    "model_label = model_name.split('/')[-1]\n",
    "\n",
    "result = pd.read_csv(f'results/{model_label}/{n_shots}_cot.csv')\n",
    "result['correct_pred'] = result['label'] == result['pred']\n",
    "result['correct_cot'] = result.apply(check_cot, axis=1)\n",
    "\n",
    "correct_preds = result[result['correct_pred'] & result['correct_cot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Dumpuses are numpuses. Impuses are jompuses. Each yumpus is not spicy. Every dumpus is mean. Lorpuses are snowy. Each lempus is not transparent. Numpuses are tumpuses. Numpuses are moderate. Every tumpus is luminous. Jompuses are not blue. Impuses are gorpuses. Every gorpus is not hot. Each dumpus is a yumpus. Every gorpus is a lempus. Lorpuses are sterpuses. Every impus is muffled. Every numpus is an impus. Gorpuses are rompuses. Polly is an impus. Polly is a lorpus.\n",
      "Question: Is Polly muffled?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is an impus.\n",
      "(2) Every impus is muffled.\n",
      "(3) Polly is muffled.\n",
      "Answer: True\n",
      "\n",
      "Every lempus is a rompus. Each rompus is a jompus. Each jompus is a lorpus. Each rompus is a tumpus. Grimpuses are feisty. Jompuses are cold. Each dumpus is transparent. Each lempus is a dumpus. Rompuses are rainy. Vumpuses are gorpuses. Each tumpus is earthy. Every vumpus is sweet. Jompuses are grimpuses. Lempuses are angry. Alex is a rompus. Alex is a vumpus.\n",
      "Question: Is Alex rainy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a rompus.\n",
      "(2) Rompuses are rainy.\n",
      "(3) Alex is rainy.\n",
      "Answer: True\n",
      "\n",
      "Sterpuses are tumpuses. Each sterpus is large. Vumpuses are zumpuses. Zumpuses are not spicy. Each vumpus is not slow. Each vumpus is a brimpus. Fae is a sterpus. Fae is a vumpus.\n",
      "Question: Is Fae slow?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Fae is a vumpus.\n",
      "(2) Each vumpus is not slow.\n",
      "(3) Fae is not slow.\n",
      "Answer: False\n",
      "\n",
      "Zumpuses are bright. Each yumpus is fast. Gorpuses are tumpuses. Every vumpus is not sweet. Every zumpus is an impus. Impuses are not large. Every gorpus is not transparent. Gorpuses are vumpuses. Each jompus is a grimpus. Each brimpus is rainy. Each jompus is muffled. Each impus is a gorpus. Each zumpus is a yumpus. Each impus is a brimpus. Stella is a jompus. Stella is a gorpus.\n",
      "Question: Is Stella transparent?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Stella is a gorpus.\n",
      "(2) Every gorpus is not transparent.\n",
      "(3) Stella is not transparent.\n",
      "Answer: False\n",
      "\n",
      "Lempuses are sterpuses. Every vumpus is an impus. Lempuses are temperate. Rompuses are transparent. Vumpuses are rompuses. Each vumpus is snowy. Rex is a lempus. Rex is a vumpus.\n",
      "Question: Is Rex snowy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Rex is a vumpus.\n",
      "(2) Each vumpus is snowy.\n",
      "(3) Rex is snowy.\n",
      "Answer: True\n",
      "\n",
      "Every tumpus is not loud. Every impus is transparent. Tumpuses are dumpuses. Impuses are gorpuses. Impuses are lorpuses. Gorpuses are not brown. Wren is an impus. Wren is a tumpus.\n",
      "Question: Is Wren transparent?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Wren is an impus.\n",
      "(2) Every impus is transparent.\n",
      "(3) Wren is transparent.\n",
      "Answer: True\n",
      "\n",
      "Every jompus is a lempus. Each shumpus is a wumpus. Shumpuses are not sweet. Each jompus is a sterpus. Each sterpus is not large. Sterpuses are tumpuses. Jompuses are rainy. Every sterpus is a gorpus. Each tumpus is not kind. Each lempus is liquid. Sally is a jompus. Sally is a shumpus.\n",
      "Question: Is Sally rainy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sally is a jompus.\n",
      "(2) Jompuses are rainy.\n",
      "(3) Sally is rainy.\n",
      "Answer: True\n",
      "\n",
      "Gorpuses are sterpuses. Gorpuses are grimpuses. Every dumpus is a jompus. Every grimpus is a shumpus. Gorpuses are not small. Sterpuses are liquid. Every shumpus is not muffled. Dumpuses are bright. Each grimpus is a brimpus. Every grimpus is not cold. Wren is a dumpus. Wren is a grimpus.\n",
      "Question: Is Wren cold?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dla(prompt, component, a_clean, a_corr=None, prepend_bos=True):\n",
    "    \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    dlas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, component)\n",
    "\n",
    "    cache = ActivationCache(cache, model).to('cpu')\n",
    "    act = cache.stack_activation(component)[:, :, -1]\n",
    "    if len(act.shape) == 4:\n",
    "        act = cache.stack_head_results(-1)[:, :, -1]\n",
    "\n",
    "    dla = model.unembed(act.to(model.W_U.device)).cpu()\n",
    "    del cache\n",
    "    \n",
    "    return dla[..., a_clean].mean(-1) - dla[..., a_corr].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def plot_dla(resid_dla, mlp_dla, attn_dla, max_val=50):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Residual Stream\", \"MLP\", \"Attention Heads\"))\n",
    "\n",
    "    # Add images to the subplots\n",
    "    fig.add_trace(px.imshow(resid_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=1)\n",
    "    fig.add_trace(px.imshow(mlp_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=2)\n",
    "    fig.add_trace(px.imshow(attn_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=3)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='RdBu',\n",
    "            cmin=-max_val,\n",
    "            cmax=max_val\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title_text=\"Direct Logit Attribution\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_patterns(prompt, patterns, n_cols, query_offset, key_offset):\n",
    "    \n",
    "    n_rows = len(patterns) // n_cols + int(len(patterns) % n_cols != 0)\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=patterns)\n",
    "\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    labels = [f\"{tok} ({i})\" for i, tok in enumerate(str_tokens)]\n",
    "    query_labels = labels[query_offset:]\n",
    "    key_labels = labels[key_offset:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, 'attn')\n",
    "\n",
    "    for i, pid in enumerate(patterns):\n",
    "        layer, head = pid.split('H')\n",
    "        layer = int(layer[1:])\n",
    "        head = int(head[:-1])\n",
    "\n",
    "        pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head, query_offset:, key_offset:].cpu()\n",
    "\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "    \n",
    "        fig.add_trace(px.imshow(\n",
    "            pattern,\n",
    "            labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "            x=key_labels,\n",
    "            y=query_labels\n",
    "        ).data[0], row=row, col=col)\n",
    "\n",
    "        fig.update_xaxes(tickangle=35)\n",
    "        fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='Blues',\n",
    "            cmin=1,\n",
    "            cmax=0\n",
    "        ),\n",
    "        height=700 * n_rows,\n",
    "        width=800 * n_cols,\n",
    "        title_text=\"Attention Patterns\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG\n",
    "import torch\n",
    "import einops\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def attribution_patching(x_clean, x_corr, a_clean, a_corr, component, prepend_bos=True, method='standard', num_alphas=5, n_last_tokens=128):\n",
    "    if isinstance(x_clean, str):\n",
    "        clean_tokens = model.to_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        clean_tokens = x_clean\n",
    "\n",
    "    if isinstance(x_corr, str):\n",
    "        corr_tokens = model.to_tokens(x_corr, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        corr_tokens = x_corr\n",
    "\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "\n",
    "    if isinstance(a_corr, str):\n",
    "        a_corr = model.to_single_token(a_corr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corr_logits, corr_cache = get_cache_fw(corr_tokens, component)\n",
    "\n",
    "    if method == 'standard':\n",
    "        logits_diff_, clean_cache, clean_grad_cache = get_cache_fw_and_bw(clean_tokens, a_clean, a_corr, corr_logits, component=component)\n",
    "    elif method == 'ig':\n",
    "        with torch.no_grad():\n",
    "            clean_logits, clean_cache = get_cache_fw(clean_tokens, component)\n",
    "\n",
    "    clean_cache = ActivationCache(clean_cache, model).to('cpu')\n",
    "    corr_cache = ActivationCache(corr_cache, model).to('cpu')\n",
    "\n",
    "    corr_act = clean_cache.stack_activation(component)[:, 0, -n_last_tokens:]\n",
    "    clean_act = corr_cache.stack_activation(component)[:, 0, -n_last_tokens:] # comp, pos dm\n",
    "    del clean_cache, corr_cache\n",
    "    \n",
    "    if clean_act.ndim > 3:\n",
    "        clean_act = clean_act.transpose(1, 2)\n",
    "        corr_act = corr_act.transpose(1, 2)\n",
    "        clean_act = clean_act.reshape(-1, clean_act.size(2), clean_act.size(3))\n",
    "        corr_act = corr_act.reshape(-1, corr_act.size(2), corr_act.size(3))\n",
    "        \n",
    "    if method == 'standard':\n",
    "        clean_grad_cache = ActivationCache(clean_grad_cache, model).to('cpu')\n",
    "        clean_grad_act = clean_grad_cache.stack_activation(component).squeeze()\n",
    "        if clean_grad_act.ndim > 3:\n",
    "            clean_grad_act = clean_grad_act.transpose(1, 2)\n",
    "            clean_grad_act = clean_grad_act.reshape(-1, clean_grad_act.size(2), clean_grad_act.size(3))\n",
    "        clean_grad_act = clean_grad_act[:, -n_last_tokens:].cpu()\n",
    "    elif method == 'ig':\n",
    "        clean_grad_act = []\n",
    "        alphas = torch.linspace(0, 1, num_alphas)\n",
    "        k = clean_act.shape[0] // model.cfg.n_layers\n",
    "        for l in tqdm(range(model.cfg.n_layers)):\n",
    "            ig_patch = torch.zeros_like(clean_act[k*l:k*(l+1)], device=clean_act.device)\n",
    "            for alpha in alphas:\n",
    "                a_alpha = alpha * clean_act[k*l:k*(l+1)] + (1 - alpha) * corr_act[k*l:k*(l+1)]\n",
    "                logits_alpha, grad_alpha = get_cache_fw_with_modified_activations(clean_tokens, a_alpha, a_clean, a_corr, l, component)\n",
    "                if grad_alpha.ndim > 3:\n",
    "                    grad_alpha = grad_alpha.reshape(-1, grad_alpha.size(1), grad_alpha.size(3))\n",
    "                grad_alpha = grad_alpha[:, -n_last_tokens:].cpu()\n",
    "                ig_patch += grad_alpha * (clean_act[k*l:k*(l+1)] - corr_act[k*l:k*(l+1)])\n",
    "                del a_alpha, logits_alpha, grad_alpha\n",
    "            clean_grad_act.append(ig_patch / num_alphas)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        clean_grad_act = torch.cat(clean_grad_act, dim=0)\n",
    "\n",
    "    print(\"Gradients collected! Computing the patch...\")\n",
    "    patch = einops.reduce(\n",
    "        clean_grad_act * (corr_act - clean_act),\n",
    "        \"component pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    del clean_act, corr_act, clean_grad_act\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return patch\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    filter = lambda name: utils.get_act_name(component) in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def get_cache_fw_and_bw(tokens, a_clean, a_corr, corr_logits, component='all'):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "        \n",
    "    model.reset_hooks()\n",
    "    \n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(filter, bw_cache_hook, \"bwd\")\n",
    "\n",
    "    clean_logits = model(tokens).cpu()\n",
    "    value = logits_diff(clean_logits, a_clean, a_corr) #- logits_diff(corr_logits.cpu(), a_clean, a_corr)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n",
    "\n",
    "def logits_diff(logits, a_clean, a_corr=None):\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "    if a_corr:\n",
    "        if isinstance(a_corr, str):\n",
    "            a_corr = [model.to_single_token(a_corr)]\n",
    "        \n",
    "        return logits[0, -1, a_clean] - logits[0, -1, a_corr].mean(-1)\n",
    "    else:\n",
    "        return logits[0, -1, a_clean]\n",
    "\n",
    "def get_cache_fw_with_modified_activations(tokens, x_int, a_clean, a_corr, layer, component):\n",
    "    hook_point = utils.get_act_name(component, layer)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    def fw_hook(act, mod_act, hook):\n",
    "        act = mod_act\n",
    "\n",
    "    fw_hook_fn = partial(fw_hook, mod_act=x_int.squeeze())\n",
    "    model.add_hook(hook_point, fw_hook_fn, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(hook_point, bw_cache_hook, \"bwd\")\n",
    "    logits = model(tokens)\n",
    "    value = logits_diff(logits, a_corr, a_clean)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return value.item(), grad_cache[hook_point]\n",
    "\n",
    "def plot_atp(atp, x_clean, component, n_last_tokens=128, val=1, prepend_bos=True):\n",
    "\n",
    "    str_tokens = model.to_str_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    xs = [f\"{tok} | {i}\" for i, tok in enumerate(str_tokens[-n_last_tokens:])]\n",
    "    \n",
    "    if component in ['z', 'q', 'result']:\n",
    "        ys = [f'L{i}H{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "    elif component in ['k', 'v']:\n",
    "        ys = [f'L{i}{component.upper()}{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_key_value_heads)]\n",
    "    else:\n",
    "        ys = [f\"L{l} {component.upper()}\" for l in range(model.cfg.n_layers)]\n",
    "        \n",
    "    fig = px.imshow(\n",
    "        atp[:, -n_last_tokens:].cpu().numpy(), \n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        color_continuous_scale='RdBu', zmin=-val, zmax=val, aspect='auto'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtasks\n",
    "\n",
    "We then explore each subtask mechanistically to understand which are the components responsible for each choice made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = ['grimpus', 'lorpus', 'wumpus', 'zumpus', 'sterpus', 'numpus', 'jompus', 'brimpus', 'yumpus', 'tumpus', 'dumpus', 'vumpus', 'rompus', 'lempus', 'gorpus', 'shumpus', 'impus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Each yumpus is not transparent. Each shumpus is loud. Every impus is a brimpus. Every shumpus is a tumpus. Each impus is a yumpus. Every impus is not shy. Alex is an impus. Alex is a shumpus.\n",
      "Question: Is Alex shy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is an impus.\n",
      "(2) Every impus is not shy.\n",
      "(3) Alex is not shy.\n",
      "Answer: False\n",
      "\n",
      "Each rompus is an impus. Wumpuses are not feisty. Each wumpus is a lorpus. Rompuses are aggressive. Rompuses are gorpuses. Impuses are not small. Sam is a rompus. Sam is a wumpus.\n",
      "Question: Is Sam aggressive?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is a rompus.\n",
      "(2) Rompuses are aggressive.\n",
      "(3) Sam is aggressive.\n",
      "Answer: True\n",
      "\n",
      "Each impus is floral. Every gorpus is a lempus. Every jompus is wooden. Brimpuses are lorpuses. Every gorpus is an impus. Each lorpus is a grimpus. Every lempus is hot. Each brimpus is a zumpus. Gorpuses are not dull. Every dumpus is a wumpus. Every lorpus is a numpus. Every numpus is overcast. Every dumpus is not fast. Every zumpus is melodic. Each impus is a jompus. Every lorpus is aggressive. Impuses are brimpuses. Each brimpus is large. Sam is a dumpus. Sam is an impus.\n",
      "Question: Is Sam floral?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is an impus.\n",
      "(2) Each impus is floral.\n",
      "(3) Sam is floral.\n",
      "Answer: True\n",
      "\n",
      "Numpuses are rompuses. Wumpuses are yumpuses. Every numpus is not dull. Yumpuses are not shy. Wumpuses are vumpuses. Each wumpus is not moderate. Rex is a wumpus. Rex is a numpus.\n",
      "Question: Is Rex moderate?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Rex is a wumpus.\n",
      "(2) Each wumpus is not moderate.\n",
      "(3) Rex is not moderate.\n",
      "Answer: False\n",
      "\n",
      "Numpuses are dumpuses. Each yumpus is a grimpus. Grimpuses are numpuses. Yumpuses are slow. Numpuses are not transparent. Each gorpus is blue. Each rompus is a lorpus. Every impus is not small. Every grimpus is a gorpus. Every yumpus is an impus. Dumpuses are cold. Rompuses are mean. Numpuses are wumpuses. Every grimpus is not feisty. Polly is a rompus. Polly is a yumpus.\n",
      "Question: Is Polly slow?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is a yumpus.\n",
      "(2) Yumpuses are slow.\n",
      "(3) Polly is slow.\n",
      "Answer: True\n",
      "\n",
      "Each shumpus is a tumpus. Each shumpus is large. Each yumpus is a gorpus. Every sterpus is a rompus. Yumpuses are numpuses. Each tumpus is earthy. Sterpuses are overcast. Every numpus is nervous. Yumpuses are not discordant. Shumpuses are yumpuses. Sally is a shumpus. Sally is a sterpus.\n",
      "Question: Is Sally large?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sally is a shumpus.\n",
      "(2) Each shumpus is large.\n",
      "(3) Sally is large.\n",
      "Answer: True\n",
      "\n",
      "Every brimpus is windy. Every sterpus is fast. Jompuses are rompuses. Sterpuses are brimpuses. Every jompus is cold. Sterpuses are tumpuses. Alex is a jompus. Alex is a sterpus.\n",
      "Question: Is Alex fast?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Alex is a sterpus.\n",
      "(2) Every sterpus is fast.\n",
      "(3) Alex is fast.\n",
      "Answer: True\n",
      "\n",
      "Wumpuses are bitter. Each grimpus is a zumpus. Tumpuses are small. Every wumpus is an impus. Grimpuses are not transparent. Wumpuses are tumpuses. Max is a grimpus. Max is a wumpus.\n",
      "Question: Is Max bitter?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "data = result # or correct_preds\n",
    "print(data['prompt'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 - Choosing the right species\n",
    "The first step is choosing the right species to focus on. This is a key step since it preceeds the attribute check. It is also the most difficult one since the choice doesn't depend only on the species and the entity, but has to be made already considering the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = data['prompt'].iloc[idx]\n",
    "cot_gold = data['cot_gold'].iloc[idx]\n",
    "label = data['label'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "\n",
    "for id_, s in enumerate(species):\n",
    "    if s in cot_gold[0]:\n",
    "        break\n",
    "\n",
    "a_clean = species_token[id_].cpu()\n",
    "a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_clean = apply_template(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Max is a"
     ]
    }
   ],
   "source": [
    "stop_tokens = [' a', ' an']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla = compute_dla(clean_out, 'resid_pre', a_clean, a_corr, prepend_bos=False)\n",
    "mlp_dla = compute_dla(clean_out, 'mlp_out', a_clean, a_corr, prepend_bos=False)\n",
    "attn_dla = compute_dla(clean_out, 'result', a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla.reshape(model.cfg.n_layers, -1), max_val=5)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.write_html(f'fig/{model_label}/{idx}_s1_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "assert len(species) == 2, \"More than two species detected!\"\n",
    "assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_corr = cot_gold.copy()\n",
    "\n",
    "cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "context = x_clean.split('\\n\\n')\n",
    "\n",
    "context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "x_corr = '\\n\\n'.join(context)\n",
    "\n",
    "#x_corr = apply_template(x_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Max is a"
     ]
    }
   ],
   "source": [
    "corr_out = generate_until_stop(x_corr, stop_tokens, prepend_bos=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bos before\n",
    "#ans = clean_out.split('<|end_header_id|>\\n\\n')[-1]\n",
    "#corr_out = '<|end_header_id|>\\n\\n'.join(corr_out.split('<|end_header_id|>\\n\\n')[:-1]) + '<|end_header_id|>\\n\\n' + ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<bos>', '<bos>'),\n",
       " ('Answer', 'Answer'),\n",
       " (' True', ' True'),\n",
       " (' or', ' or'),\n",
       " (' False', ' False'),\n",
       " (' to', ' to'),\n",
       " (' the', ' the'),\n",
       " (' following', ' following'),\n",
       " (' question', ' question'),\n",
       " ('.', '.'),\n",
       " (' Answer', ' Answer'),\n",
       " (' as', ' as'),\n",
       " (' in', ' in'),\n",
       " (' the', ' the'),\n",
       " (' examples', ' examples'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' loud', ' loud'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Alex', ' Alex'),\n",
       " (' shy', ' shy'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' False', ' False'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' fe', ' fe'),\n",
       " ('isty', 'isty'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' gor', ' gor'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' lemp', ' lemp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' j', ' j'),\n",
       " ('om', 'om'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' wooden', ' wooden'),\n",
       " ('.', '.'),\n",
       " (' Bri', ' Bri'),\n",
       " ('mp', 'mp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' lor', ' lor'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gri', ' gri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lemp', ' lemp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' hot', ' hot'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Gor', ' Gor'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' dull', ' dull'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' overcast', ' overcast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' melodic', ' melodic'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' j', ' j'),\n",
       " ('om', 'om'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' aggressive', ' aggressive'),\n",
       " ('.', '.'),\n",
       " (' Imp', ' Imp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' dump', ' dump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sam', ' Sam'),\n",
       " (' floral', ' floral'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Each', ' Each'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sam', ' Sam'),\n",
       " (' is', ' is'),\n",
       " (' floral', ' floral'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('N', 'N'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' romp', ' romp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' dull', ' dull'),\n",
       " ('.', '.'),\n",
       " (' Y', ' Y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' shy', ' shy'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' v', ' v'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' moderate', ' moderate'),\n",
       " ('.', '.'),\n",
       " (' Rex', ' Rex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Rex', ' Rex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Rex', ' Rex'),\n",
       " (' moderate', ' moderate'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Rex', ' Rex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Each', ' Each'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' moderate', ' moderate'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Rex', ' Rex'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' moderate', ' moderate'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' False', ' False'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('N', 'N'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' dump', ' dump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gri', ' gri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Gri', ' Gri'),\n",
       " ('mp', 'mp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Y', ' Y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' slow', ' slow'),\n",
       " ('.', '.'),\n",
       " (' N', ' N'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' blue', ' blue'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' lor', ' lor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gri', ' gri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Dump', ' Dump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' cold', ' cold'),\n",
       " ('.', '.'),\n",
       " (' Rom', ' Rom'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' mean', ' mean'),\n",
       " ('.', '.'),\n",
       " (' N', ' N'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' gri', ' gri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' not', ' not'),\n",
       " (' fe', ' fe'),\n",
       " ('isty', 'isty'),\n",
       " ('.', '.'),\n",
       " (' Polly', ' Polly'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Polly', ' Polly'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Polly', ' Polly'),\n",
       " (' slow', ' slow'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Polly', ' Polly'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Y', ' Y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' slow', ' slow'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Polly', ' Polly'),\n",
       " (' is', ' is'),\n",
       " (' slow', ' slow'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Each', 'Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' gor', ' gor'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' rom', ' rom'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Y', ' Y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' earthy', ' earthy'),\n",
       " ('.', '.'),\n",
       " (' Ster', ' Ster'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' overcast', ' overcast'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' n', ' n'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' nervous', ' nervous'),\n",
       " ('.', '.'),\n",
       " (' Y', ' Y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' discord', ' discord'),\n",
       " ('ant', 'ant'),\n",
       " ('.', '.'),\n",
       " (' Sh', ' Sh'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' y', ' y'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Sally', ' Sally'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Sally', ' Sally'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Sally', ' Sally'),\n",
       " (' large', ' large'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Sally', ' Sally'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Each', ' Each'),\n",
       " (' sh', ' sh'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Sally', ' Sally'),\n",
       " (' is', ' is'),\n",
       " (' large', ' large'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('Every', 'Every'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' windy', ' windy'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " (' J', ' J'),\n",
       " ('omp', 'omp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' romp', ' romp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Ster', ' Ster'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' bri', ' bri'),\n",
       " ('mp', 'mp'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' j', ' j'),\n",
       " ('om', 'om'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' cold', ' cold'),\n",
       " ('.', '.'),\n",
       " (' Ster', ' Ster'),\n",
       " ('p', 'p'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' t', ' t'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' j', ' j'),\n",
       " ('om', 'om'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Question', 'Question'),\n",
       " (':', ':'),\n",
       " (' Is', ' Is'),\n",
       " (' Alex', ' Alex'),\n",
       " (' fast', ' fast'),\n",
       " ('?', '?'),\n",
       " ('\\n', '\\n'),\n",
       " ('Think', 'Think'),\n",
       " (' step', ' step'),\n",
       " ('-', '-'),\n",
       " ('by', 'by'),\n",
       " ('-', '-'),\n",
       " ('step', 'step'),\n",
       " ('.', '.'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('1', '1'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('2', '2'),\n",
       " (')', ')'),\n",
       " (' Every', ' Every'),\n",
       " (' ster', ' ster'),\n",
       " ('pus', 'pus'),\n",
       " (' is', ' is'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('(', '('),\n",
       " ('3', '3'),\n",
       " (')', ')'),\n",
       " (' Alex', ' Alex'),\n",
       " (' is', ' is'),\n",
       " (' fast', ' fast'),\n",
       " ('.', '.'),\n",
       " ('\\n', '\\n'),\n",
       " ('Answer', 'Answer'),\n",
       " (':', ':'),\n",
       " (' True', ' True'),\n",
       " ('\\n\\n', '\\n\\n'),\n",
       " ('W', 'G'),\n",
       " ('ump', 'rimp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' bitter', ' bitter'),\n",
       " ('.', '.'),\n",
       " (' Each', ' Each'),\n",
       " (' gri', ' gri'),\n",
       " ('mp', 'mp'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' a', ' a'),\n",
       " (' z', ' z'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' T', ' T'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' small', ' small'),\n",
       " ('.', '.'),\n",
       " (' Every', ' Every'),\n",
       " (' w', ' w'),\n",
       " ('ump', 'ump'),\n",
       " ('us', 'us'),\n",
       " (' is', ' is'),\n",
       " (' an', ' an'),\n",
       " (' imp', ' imp'),\n",
       " ('us', 'us'),\n",
       " ('.', '.'),\n",
       " (' Gri', ' Gri'),\n",
       " ('mp', 'mp'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' not', ' not'),\n",
       " (' transparent', ' transparent'),\n",
       " ('.', '.'),\n",
       " (' W', ' W'),\n",
       " ('ump', 'ump'),\n",
       " ('uses', 'uses'),\n",
       " (' are', ' are'),\n",
       " (' t', ' t'),\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model.to_str_tokens(clean_out, prepend_bos=False), model.to_str_tokens(corr_out, prepend_bos=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in ['result']: #['resid_pre', 'mlp_out', 'attn_out', 'result']: \n",
    "    atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "    torch.save(atp, f'patches/{model_label}/{idx}_s1_AtP_{component}.bin')\n",
    "    fig = plot_atp(atp, clean_out, component=component, prepend_bos=False, n_last_tokens=256)\n",
    "    fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_{component}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "\n",
    "torch.save(atp_q, f'patches/{model_label}/{idx}_s1_AtP_q.bin')\n",
    "torch.save(atp_k, f'patches/{model_label}/{idx}_s1_AtP_k.bin')\n",
    "torch.save(atp_v, f'patches/{model_label}/{idx}_s1_AtP_v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256, **kwargs):\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Queries\", \"Keys\", \"Values\"), shared_xaxes=True, vertical_spacing=0.05)\n",
    "    \n",
    "    for i, (atp, hook) in enumerate(zip([atp_q, atp_k, atp_v], ['q', 'k', 'v'])):\n",
    "        plot = plot_atp(atp, clean_out, component=hook, prepend_bos=False, n_last_tokens=n_last_tokens)\n",
    "        for trace in plot.data:\n",
    "            fig.add_trace(trace, row=1+i, col=1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        coloraxis1=dict(colorscale='RdBu', cmin=-0.5, cmax=0.5),\n",
    "        showlegend=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=256)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [02:45<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [02:43<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [02:39<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=256)\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv_ig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to_tokens(clean_out, prepend_bos=False)[0], f'patches/{model_label}/{idx}_clean_tokens.bin')\n",
    "torch.save(model.to_tokens(corr_out, prepend_bos=False)[0], f'patches/{model_label}/{idx}_corr_tokens.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, clean_patterns = get_cache_fw(model.to_tokens(clean_out, prepend_bos=False)[0], 'pattern')\n",
    "    _, corr_patterns = get_cache_fw(model.to_tokens(corr_out, prepend_bos=False)[0], 'pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_v1 = {\n",
    "    2: ['L9H29+', 'L10H24+', 'L11H29+', 'L13H29+', 'L13H31-', 'L14H31+', 'L15H29+', 'L15H31+', 'L16H31+', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L22H31+', 'L24H31+', 'L26H31-', 'L30H31+', 'L31H31+'],\n",
    "    6: ['L1H25+', 'L2H25-', 'L2H27-', 'L3H25-', 'L4H27+', 'L8H27+', 'L10H27-', 'L11H31-', 'L12H31+', 'L13H30+', 'L13H31-', 'L14H30+', 'L14H31-', 'L15H29+', 'L15H31+', 'L16H31+', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L21H31-', 'L24H31+', 'L26H31-', 'L30H31+', 'L31H31-']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inspect_patterns = []\n",
    "corr_inspect_patterns = []\n",
    "\n",
    "for lab in heads_v1[idx]:\n",
    "    l, h = lab.split('H')\n",
    "    l = int(l[1:])\n",
    "    h = int(h[:-1])\n",
    "    clean_inspect_patterns.append(clean_patterns[f'blocks.{l}.attn.hook_pattern'][:, h])\n",
    "    corr_inspect_patterns.append(corr_patterns[f'blocks.{l}.attn.hook_pattern'][:, h])\n",
    "    \n",
    "clean_inspect_patterns = torch.cat(clean_inspect_patterns).cpu()\n",
    "corr_inspect_patterns = torch.cat(corr_inspect_patterns).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clean_inspect_patterns, f'patches/{idx}_patterns_clean_v1.bin')\n",
    "torch.save(corr_inspect_patterns, f'patches/{idx}_patterns_corr_v1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_last_tokens = 512\n",
    "\n",
    "for idx in tqdm(range(10)):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    cot_corr = cot_gold.copy()\n",
    "\n",
    "    cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "    context = x_clean.split('\\n\\n')\n",
    "    \n",
    "    context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "    x_corr = '\\n\\n'.join(context)\n",
    "    corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "    try:\n",
    "        assert len(species) == 2, \"More than two species detected!\"\n",
    "        assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv.html\")\n",
    "\n",
    "        atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "        atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "\n",
    "        fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "        fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "stop_tokens = [' a',]\n",
    "n_last_tokens = 512\n",
    "\n",
    "\n",
    "idx = 2\n",
    "x_clean = data['prompt'].iloc[idx]\n",
    "cot_gold = data['cot_gold'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "\n",
    "subject  = cot_gold[0].split()[0]\n",
    "a_clean = cot_gold[0].split()[-1][:-1]\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for c in clues:\n",
    "    if subject in c and a_clean not in c:\n",
    "        a_corr = c.split()[-1]\n",
    "        a_corr = a_corr[:-1] if a_corr[-1] == '.' else a_corr\n",
    "\n",
    "clean_attr = clean_question.split()[2][:-1]\n",
    "\n",
    "for c in clues:\n",
    "    if a_corr in c.lower():\n",
    "        corr_attr = c.split()[-1]\n",
    "        if not_a_species(corr_attr):\n",
    "            break\n",
    "\n",
    "corr_question = clean_question.replace(clean_attr, corr_attr)\n",
    "\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + \"Question: \" + corr_question\n",
    "\n",
    "a_clean = model.to_tokens(' ' + a_clean, prepend_bos=False)[:, 0].cpu()\n",
    "a_corr = model.to_tokens(' ' + a_corr, prepend_bos=False)[:, 0].cpu()\n",
    "\n",
    "assert len(model.to_tokens(' ' + clean_attr)[0]) == len(model.to_tokens(' ' + corr_attr)[0]), \"Attributes with different shapes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' w', ' gri')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(a_clean), model.to_string(a_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: -0.959\n",
      "Clean logit difference: -1.184\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "for component in ['resid_pre', 'mlp_out', 'attn_out', 'result']: \n",
    "    atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False, method='standard', n_last_tokens=256)\n",
    "    torch.save(atp, f'patches/{model_label}/{idx}_s1_AtP_{component}_v2.bin')\n",
    "    fig = plot_atp(atp, clean_out_new, component=component, prepend_bos=False, n_last_tokens=256)\n",
    "    fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_{component}_v2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n",
      "Gradients collected! Computing the patch...\n"
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='q', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "atp_k = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='k', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "atp_v = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='v', prepend_bos=False, method='standard', n_last_tokens=n_last_tokens)\n",
    "torch.save(atp_q, f'patches/{model_label}/{idx}_s1_AtP_q_v2.bin')\n",
    "torch.save(atp_k, f'patches/{model_label}/{idx}_s1_AtP_k_v2.bin')\n",
    "torch.save(atp_v, f'patches/{model_label}/{idx}_s1_AtP_v_v2.bin')\n",
    "\n",
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "fig.write_html(f\"fig/{model_label}/{idx}_s1_AtP_qkv_v2.html\")\n",
    "\n",
    "#atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "#atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "#atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='ig', n_last_tokens=n_last_tokens)\n",
    "\n",
    "#fig = plot_qkv_atp(atp_q, atp_k, atp_v, n_last_tokens=n_last_tokens)\n",
    "#fig.write_html(f\"fig/{idx}_s1_AtP_qkv_ig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to_tokens(clean_out_new, prepend_bos=False)[0], f'patches/{idx}_clean_tokens_v2.bin')\n",
    "torch.save(model.to_tokens(corr_out_new, prepend_bos=False)[0], f'patches/{idx}_corr_tokens_v2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, clean_patterns_v2 = get_cache_fw(model.to_tokens(clean_out_new, prepend_bos=False)[0], 'pattern')\n",
    "    _, corr_patterns_v2 = get_cache_fw(model.to_tokens(corr_out_new, prepend_bos=False)[0], 'pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_v2 = {\n",
    "    2: ['L10H30+', 'L12H30+', 'L12H31+', 'L13H30-', 'L14H31+', 'L15H31+', 'L16H31-', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L21H31+', 'L22H31+', 'L24H31+', 'L26H31-', 'L27H31+', 'L30H31+', 'L31H31+'],\n",
    "    6: ['L9H30+', 'L10H30-', 'L11H30+', 'L11H31-', 'L12H30+', 'L12H31+', 'L13H30+', 'L13H31-', 'L14H31-', 'L15H31+', 'L16H31-', 'L17H31+', 'L18H31-', 'L19H31-', 'L20H31-', 'L21H31+', 'L22H31+', 'L24H31+', 'L26H31-', 'L30H31+', 'L31H31+']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inspect_patterns = []\n",
    "corr_inspect_patterns = []\n",
    "\n",
    "for lab in heads_v2[idx]:\n",
    "    l, h = lab.split('H')\n",
    "    l = int(l[1:])\n",
    "    h = int(h[:-1])\n",
    "    clean_inspect_patterns.append(clean_patterns_v2[f'blocks.{l}.attn.hook_pattern'][:, h])\n",
    "    corr_inspect_patterns.append(corr_patterns_v2[f'blocks.{l}.attn.hook_pattern'][:, h])\n",
    "    \n",
    "clean_inspect_patterns = torch.cat(clean_inspect_patterns).cpu()\n",
    "corr_inspect_patterns = torch.cat(corr_inspect_patterns).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clean_inspect_patterns, f'patches/{idx}_patterns_clean_v2.bin')\n",
    "torch.save(corr_inspect_patterns, f'patches/{idx}_patterns_corr_v2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 - Attribute check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['2']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is', ' are']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_steps = cot_gold[1].split()\n",
    "for i, step in enumerate(cot_steps):\n",
    "    if \"is\" in step or \"are\" in step:\n",
    "        attribute = cot_steps[i+1].replace('.', '')\n",
    "        a_clean = ' ' + attribute\n",
    "\n",
    "a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "a_clean = [model.to_single_token(a_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 3.683\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 2) | {attribute} - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L8H6-', 'L10H5+', 'L10H7+', 'L13H2+', 'L13H4+', 'L14H0+', 'L14H3-', 'L14H4+', 'L16H0+', 'L16H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "import random\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for s in all_species:\n",
    "    if s in clean_question.lower(): \n",
    "        s_star = s\n",
    "        break\n",
    "    \n",
    "a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "corr_question = clean_question.replace(a_clean, a_corr)\n",
    "\n",
    "for i, c in enumerate(clues):\n",
    "    if a_corr in c:\n",
    "        for s in all_species:\n",
    "            if s in c.lower(): break\n",
    "\n",
    "        clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Answer True or False to the following question. Answer as in the examples.\\n\\nYumpuses are grimpuses. Impuses are sterpuses. Sterpuses are dumpuses. Each sterpus is a numpus. Each impus is windy. Lempuses are melodic. Rompuses are lorpuses. Rompuses are not earthy. Numpuses are sweet. Rompuses are impuses. Sterpuses are transparent. Every yumpus is not dull. Lorpuses are luminous. Impuses are lempuses. Alex is a rompus. Alex is a yumpus.\\nQuestion: Is Alex earthy?\\nThink step-by-step.\\n\\n(1) Alex is a rompus.\\n(2) Rompuses are not earthy.\\n(3) Alex is not earthy.\\nAnswer: False\\n\\nEvery impus is a tumpus. Sterpuses are jompuses. Impuses are grimpuses. Each sterpus is not large. Grimpuses are not fast. Every impus is not red. Lorpuses are not happy. Every tumpus is a rompus. Every tumpus is a lorpus. Every lorpus is a sterpus. Rompuses are metallic. Each tumpus is muffled. Every jompus is not aggressive. Gorpuses are sunny. Every lorpus is a gorpus. Shumpuses are earthy. Shumpuses are zumpuses. Every sterpus is a yumpus. Alex is a shumpus. Alex is a sterpus.\\nQuestion: Is Alex large?\\nThink step-by-step.\\n\\n(1) Alex is a sterpus.\\n(2) Each sterpus is not large.\\n(3) Alex is not large.\\nAnswer: False\\n\\nEach tumpus is a brimpus. Wumpuses are shumpuses. Each shumpus is fast. Each jompus is a gorpus. Every tumpus is dull. Wumpuses are shy. Jompuses are not transparent. Jompuses are lempuses. Gorpuses are orange. Each wumpus is a jompus. Max is a wumpus. Max is a tumpus.\\nQuestion: Is Max shy?\\nThink step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are shy.\\n(3) Max is shy.\\nAnswer: True\\n\\nShumpuses are slow. Each wumpus is an impus. Every grimpus is not mean. Each grimpus is a shumpus. Each jompus is a lorpus. Vumpuses are loud. Every jompus is opaque. Impuses are earthy. Every vumpus is a lempus. Each grimpus is a vumpus. Every vumpus is a brimpus. Wumpuses are grimpuses. Each lempus is nervous. Each wumpus is not wooden. Alex is a jompus. Alex is a grimpus.\\nQuestion: Is Alex mean?\\nThink step-by-step.\\n\\n(1) Alex is a grimpus.\\n(2) Every grimpus is not mean.\\n(3) Alex is not mean.\\nAnswer: False\\n\\nEach brimpus is a zumpus. Every lempus is feisty. Every brimpus is a lempus. Grimpuses are sterpuses. Every dumpus is not dull. Each sterpus is not overcast. Every grimpus is a tumpus. Every wumpus is a brimpus. Each zumpus is not fruity. Each lempus is a grimpus. Every lempus is an impus. Each jompus is not small. Numpuses are kind. Every brimpus is liquid. Tumpuses are temperate. Each dumpus is a shumpus. Every tumpus is a gorpus. Each tumpus is a numpus. Every impus is melodic. Wumpuses are jompuses. Grimpuses are not sour. Each wumpus is not transparent. Polly is a grimpus. Polly is a dumpus.\\nQuestion: Is Polly sour?\\nThink step-by-step.\\n\\n(1) Polly is a grimpus.\\n(2) Grimpuses are not sour.\\n(3) Polly is not sour.\\nAnswer: False\\n\\nTumpuses are not bright. Grimpuses are not cold. Tumpuses are numpuses. Every numpus is metallic. Tumpuses are lorpuses. Grimpuses are zumpuses. Wren is a tumpus. Wren is a grimpus.\\nQuestion: Is Wren bright?\\nThink step-by-step.\\n\\n(1) Wren is a tumpus.\\n(2) Tumpuses are not bright.\\n(3) Wren is not bright.\\nAnswer: False\\n\\nEvery wumpus is a dumpus. Lempuses are liquid. Every brimpus is not brown. Tumpuses are lempuses. Every impus is not windy. Each lorpus is a brimpus. Tumpuses are shumpuses. Every shumpus is cold. Each rompus is fast. Every rompus is a wumpus. Each shumpus is a gorpus. Zumpuses are yumpuses. Every wumpus is not feisty. Gorpuses are floral. Wumpuses are numpuses. Shumpuses are rompuses. Zumpuses are not transparent. Lorpuses are tumpuses. Tumpuses are not muffled. Rompuses are impuses. Dumpuses are aggressive. Lorpuses are bright. Max is a zumpus. Max is a shumpus.\\nQuestion: Is Max cold?\\nThink step-by-step.\\n\\n(1) Max is a shumpus.\\n(2) Every shumpus is cold.\\n(3) Max is cold.\\nAnswer: True\\n\\nImpuses are not orange. Each rompus is a tumpus. Each lorpus is a dumpus. Each tumpus is large. Every tumpus is a lorpus. Lorpuses are not luminous. Numpuses are not bitter. Every impus is a jompus. Lorpuses are numpuses. Every numpus is a yumpus. Lempuses are melodic. Vumpuses are not angry. Each tumpus is a vumpus. Each yumpus is bright. Rompuses are lempuses. Dumpuses are not hot. Numpuses are zumpuses. Every lorpus is not fast. Polly is a lorpus. Polly is an impus.\\nIs Polly fast?\\nThink step-by-step.\\n\\n(1) Polly is a lorpus.\\n(2) Lorpuses are'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_out_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='resid', prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_atp(atp, clean_out_new, component=\"resid\", prepend_bos=False)\n",
    "fig.write_html(\"fig/s2_AtP_resid.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 - The right connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "attribute = question.split()[2][:-1]\n",
    "\n",
    "if 'not' in cot_gold[2]:\n",
    "    a_clean = [model.to_single_token(' not')]\n",
    "    a_corr = [model.to_single_token(' ' + attribute)]\n",
    "    clean_label = 'not'\n",
    "    corr_label = attribute\n",
    "else:\n",
    "    a_clean = [model.to_single_token(' ' + attribute)]\n",
    "    a_corr = [model.to_single_token(' not')]\n",
    "    clean_label = attribute\n",
    "    corr_label = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['3']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.566\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 3) | {clean_label} - {corr_label}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L12H0+', 'L14H0+', 'L14H4-', 'L14H5+', 'L14H6-', 'L16H7-', 'L17H7+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 - Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "a_clean = model.to_single_token(' ' + str(label))\n",
    "a_corr = model.to_single_token(' False' if label else ' True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.200\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, [a_clean], [a_corr], prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 4) | '{model.to_string(a_clean)}' - '{model.to_string(a_corr)}'\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L9H2+', 'L9H4+', 'L10H7+', 'L11H6-' 'L14H0+', 'L14H1-', 'L15H1+', 'L15H4-', 'L17H2+', 'L17H7-']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [02:41<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "stop_tokens = [' a',]\n",
    "\n",
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "    resid_dlas.append(resid_dla.cpu())\n",
    "    mlp_dlas.append(mlp_dla.cpu())\n",
    "    attn_dlas.append(attn_dla.cpu())\n",
    "    del resid_dla, mlp_dla, attn_dla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    for component in components:\n",
    "        x_clean = correct_preds['prompt'].iloc[idx]\n",
    "        cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "        cot_gold = ast.literal_eval(cot_gold)\n",
    "    \n",
    "        example = x_clean.split('\\n\\n')[-2]\n",
    "        context, question = example.split('Question: ')\n",
    "        \n",
    "        subject = question.split()[1]\n",
    "        species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "        species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "        \n",
    "        for id_, s in enumerate(species):\n",
    "            if s in cot_gold[0]:\n",
    "                break\n",
    "        \n",
    "        a_clean = species_token[id_].cpu()\n",
    "        a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "    \n",
    "        try:\n",
    "            assert len(species) == 2, \"More than two species detected!\"\n",
    "            assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "    \n",
    "            cot_corr = cot_gold.copy()\n",
    "            \n",
    "            cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "            context = x_clean.split('\\n\\n')\n",
    "            \n",
    "            context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "            x_corr = '\\n\\n'.join(context)\n",
    "    \n",
    "            clean_out = generate_until_stop(x_clean, stop_tokens)    \n",
    "            corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 1)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive AtP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [05:28<00:00, 12.62s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    cot_steps = cot_gold[1].split()\n",
    "    try:\n",
    "        for i, step in enumerate(cot_steps):\n",
    "            if \"is\" in step or \"are\" in step:\n",
    "                attribute = cot_steps[i+1].replace('.', '')\n",
    "                a_clean = ' ' + attribute\n",
    "        \n",
    "        a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "        a_clean = [model.to_single_token(a_clean)]\n",
    "        \n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 2) | attribute - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import einops\n",
    "\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "    test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "    context, clean_question = test_example.split('Question: ')\n",
    "    clues = context.split('. ')\n",
    "    \n",
    "    for s in all_species:\n",
    "        if s in clean_question.lower(): \n",
    "            s_star = s\n",
    "            break\n",
    "    \n",
    "    a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "    other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "    other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "    a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "    corr_question = clean_question.replace(a_clean, a_corr)\n",
    "    \n",
    "    for i, c in enumerate(clues):\n",
    "        if a_corr in c:\n",
    "            for s in all_species:\n",
    "                if s in c.lower(): break\n",
    "    \n",
    "            clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "    \n",
    "    clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "    corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question\n",
    "\n",
    "    for component in components:    \n",
    "        try:\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 2)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 2.838\n",
      "Corrupted logit difference: -3.524\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Corrupted logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [02:22<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    attribute = question.split()[2][:-1]\n",
    "\n",
    "    try:\n",
    "        if 'not' in cot_gold[2]:\n",
    "            a_clean = [model.to_single_token(' not')]\n",
    "            a_corr = [model.to_single_token(' ' + attribute)]\n",
    "            clean_label = 'not'\n",
    "            corr_label = attribute\n",
    "        else:\n",
    "            a_clean = [model.to_single_token(' ' + attribute)]\n",
    "            a_corr = [model.to_single_token(' not')]\n",
    "            clean_label = attribute\n",
    "            corr_label = 'not'\n",
    "\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 3) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [08:43<00:00, 20.14s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "    a_clean = model.to_single_token(' ' + str(label))\n",
    "    a_corr = model.to_single_token(' False' if label else ' True')\n",
    "        \n",
    "    try:\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0).unsqueeze(-1)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0).unsqueeze(-1)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 4) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA_agg.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
