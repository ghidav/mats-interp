{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma-2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd4020b431d49cf80c8ce128f1fef75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name, torch_dtype=torch.float32, n_devices=1)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_until_stop(prompt, stop_tokens, max_tokens=64, verbose=False, prepend_bos=True):\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = [model.to_single_token(tok) for tok in stop_tokens]\n",
    "        \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    gen = True\n",
    "    while gen:\n",
    "        with torch.no_grad():\n",
    "            new_tok = model(tokens).argmax(-1)[:, -1]\n",
    "        \n",
    "        if verbose: print(model.to_string(new_tok), end='')\n",
    "        tokens = torch.cat([tokens, new_tok[None].to(tokens.device)], dim=-1)\n",
    "        if new_tok.item() in stop_tokens or max_tokens == 0:\n",
    "            gen = False\n",
    "        max_tokens -= 1\n",
    "\n",
    "    return model.to_string(tokens)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cot(x):\n",
    "    try:\n",
    "        check = all([x == y for x, y in zip(x['cot_gold'], x['cot_pred'])])\n",
    "    except: check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = \"7shots\"\n",
    "\n",
    "result = pd.read_csv(f'results/results_{n_shots}.csv')\n",
    "result['correct_pred'] = result['label'] == result['pred']\n",
    "result['correct_cot'] = result.apply(check_cot, axis=1)\n",
    "\n",
    "correct_preds = result[result['correct_pred'] & result['correct_cot']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dla(prompt, component, a_clean, a_corr=None, prepend_bos=True):\n",
    "    \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
    "    dlas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, component)\n",
    "\n",
    "    cache = ActivationCache(cache, model).to('cpu')\n",
    "    act = cache.stack_activation(component)[:, :, -1]\n",
    "    if len(act.shape) == 4:\n",
    "        act = cache.stack_head_results(-1)[:, :, -1]\n",
    "\n",
    "    dla = model.unembed(act.to(model.W_U.device)).cpu()\n",
    "    del cache\n",
    "    \n",
    "    return dla[..., a_clean].mean(-1) - dla[..., a_corr].mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def plot_dla(resid_dla, mlp_dla, attn_dla, max_val=50):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Residual Stream\", \"MLP\", \"Attention Heads\"))\n",
    "\n",
    "    # Add images to the subplots\n",
    "    fig.add_trace(px.imshow(resid_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=1)\n",
    "    fig.add_trace(px.imshow(mlp_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=2)\n",
    "    fig.add_trace(px.imshow(attn_dla.detach().cpu(), zmin=-max_val, zmax=max_val).data[0], row=1, col=3)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='RdBu',\n",
    "            cmin=-max_val,\n",
    "            cmax=max_val\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title_text=\"Direct Logit Attribution\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_patterns(prompt, patterns, n_cols, query_offset, key_offset):\n",
    "    \n",
    "    n_rows = len(patterns) // n_cols + int(len(patterns) % n_cols != 0)\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=patterns)\n",
    "\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    labels = [f\"{tok} ({i})\" for i, tok in enumerate(str_tokens)]\n",
    "    query_labels = labels[query_offset:]\n",
    "    key_labels = labels[key_offset:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, cache = get_cache_fw(tokens, 'attn')\n",
    "\n",
    "    for i, pid in enumerate(patterns):\n",
    "        layer, head = pid.split('H')\n",
    "        layer = int(layer[1:])\n",
    "        head = int(head[:-1])\n",
    "\n",
    "        pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head, query_offset:, key_offset:].cpu()\n",
    "\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "    \n",
    "        fig.add_trace(px.imshow(\n",
    "            pattern,\n",
    "            labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "            x=key_labels,\n",
    "            y=query_labels\n",
    "        ).data[0], row=row, col=col)\n",
    "\n",
    "        fig.update_xaxes(tickangle=35)\n",
    "        fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        coloraxis=dict(\n",
    "            colorscale='Blues',\n",
    "            cmin=1,\n",
    "            cmax=0\n",
    "        ),\n",
    "        height=700 * n_rows,\n",
    "        width=800 * n_cols,\n",
    "        title_text=\"Attention Patterns\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "def logits_diff(logits, a_clean, a_corr=None):\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "    if a_corr:\n",
    "        if isinstance(a_corr, str):\n",
    "            a_corr = [model.to_single_token(a_corr)]\n",
    "        \n",
    "        return logits[0, -1, a_clean] - logits[0, -1, a_corr].mean(-1)\n",
    "    else:\n",
    "        return logits[0, -1, a_clean]\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def get_cache_fw_and_bw(tokens, a_clean, a_corr, corr_logits, component='all'):\n",
    "    if component == 'all':\n",
    "        filter = lambda name: \"_input\" not in name\n",
    "    elif component == 'qkv':\n",
    "        filter = lambda name: name.split('.')[-1].strip() in ['hook_q', 'hook_k', 'hook_v'] and \"_input\" not in name\n",
    "    else:\n",
    "        filter = lambda name: component in name\n",
    "        \n",
    "    model.reset_hooks()\n",
    "    \n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(filter, bw_cache_hook, \"bwd\")\n",
    "\n",
    "    clean_logits = model(tokens).cpu()\n",
    "    value = logits_diff(clean_logits, a_clean, a_corr) #- logits_diff(corr_logits.cpu(), a_clean, a_corr)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n",
    "\n",
    "def stack_qkv_results(cache):\n",
    "    q = cache.stack_activation('q')\n",
    "    k = cache.stack_activation('k')\n",
    "    v = cache.stack_activation('v')\n",
    "\n",
    "    c, b, pos, h, d = q.shape\n",
    "    q = q.reshape(c * h, b, pos, d)\n",
    "    k = k.reshape(c, b, pos, d)\n",
    "    v = v.reshape(c, b, pos, d)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def attribution_patching(x_clean, x_corr, a_clean, a_corr, component='all', prepend_bos=True):\n",
    "\n",
    "    if isinstance(x_clean, str):\n",
    "        clean_tokens = model.to_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        clean_tokens = x_clean\n",
    "\n",
    "    if isinstance(x_corr, str):\n",
    "        corr_tokens = model.to_tokens(x_corr, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        corr_tokens = x_corr\n",
    "\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "\n",
    "    if isinstance(a_corr, str):\n",
    "        a_corr = model.to_single_token(a_corr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corr_logits, corr_cache = get_cache_fw(corr_tokens, component)\n",
    "    \n",
    "    corr_cache = corr_cache\n",
    "    logits_diff_, clean_cache, clean_grad_cache = get_cache_fw_and_bw(clean_tokens, a_clean, a_corr, corr_logits, component=component)\n",
    "\n",
    "    clean_grad_cache = ActivationCache(clean_grad_cache, model).to('cpu')\n",
    "    clean_cache = ActivationCache(clean_cache, model).to('cpu')\n",
    "    corr_cache = ActivationCache(corr_cache, model).to('cpu')\n",
    "\n",
    "    corr_act = []\n",
    "    clean_act = []\n",
    "    clean_grad_act = []\n",
    "\n",
    "    if component in ['all', 'resid']:\n",
    "        corr_act.append(corr_cache.accumulated_resid(-1, incl_mid=True, return_labels=False))\n",
    "        clean_act.append(clean_cache.accumulated_resid(-1, incl_mid=True, return_labels=False))\n",
    "        clean_grad_act.append(clean_grad_cache.accumulated_resid(-1, incl_mid=True, return_labels=False))\n",
    "    if component in ['all', 'mlp']:\n",
    "        clean_act.append(clean_cache.stack_activation('mlp_out'))\n",
    "        corr_act.append(corr_cache.stack_activation('mlp_out'))\n",
    "        clean_grad_act.append(clean_grad_cache.stack_activation('mlp_out'))\n",
    "    if component in ['all', 'attn']:\n",
    "        clean_act.append(clean_cache.stack_head_results(-1))\n",
    "        corr_act.append(corr_cache.stack_head_results(-1))\n",
    "        clean_grad_act.append(clean_grad_cache.stack_head_results(-1))\n",
    "    if component in ['qkv']:\n",
    "        clean_q, clean_k, clean_v = stack_qkv_results(clean_cache)\n",
    "        corr_q, corr_k, corr_v = stack_qkv_results(corr_cache)\n",
    "        clean_grad_q, clean_grad_k, clean_grad_v = stack_qkv_results(clean_grad_cache)\n",
    "        \n",
    "        clean_act.append(clean_q)\n",
    "        corr_act.append(corr_q)\n",
    "        clean_grad_act.append(clean_grad_q)\n",
    "        clean_act.append(clean_k)\n",
    "        corr_act.append(corr_k)\n",
    "        clean_grad_act.append(clean_grad_k)\n",
    "        clean_act.append(clean_v)\n",
    "        corr_act.append(corr_v)\n",
    "        clean_grad_act.append(clean_grad_v)\n",
    "\n",
    "    patches = []\n",
    "    for corr, clean, clean_grad in zip(corr_act, clean_act, clean_grad_act):\n",
    "        patches.append(einops.reduce(\n",
    "                clean_grad * (clean - corr),\n",
    "                \"component batch pos d_model -> component pos\",\n",
    "                \"sum\",\n",
    "            ))\n",
    "    if len(patches) == 1:\n",
    "        return patches[0]\n",
    "    else:\n",
    "        return patches\n",
    "\n",
    "def plot_atp(atp, x_clean, component, n_last_tokens=128, val=1, prepend_bos=True):\n",
    "\n",
    "    str_tokens = model.to_str_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    xs = [f\"{tok} | {i}\" for i, tok in enumerate(str_tokens[-n_last_tokens:])]\n",
    "    \n",
    "    if component in ['z', 'q']:\n",
    "        ys = [f'L{i}H{j}' for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "    else:\n",
    "        ys = [f\"L{l} {component.upper()}\" for l in range(model.cfg.n_layers)]\n",
    "        \n",
    "    fig = px.imshow(\n",
    "        atp[:, -n_last_tokens:].cpu().numpy(), \n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        color_continuous_scale='RdBu', zmin=-val, zmax=val, aspect='auto'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG\n",
    "import torch\n",
    "import einops\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def attribution_patching(x_clean, x_corr, a_clean, a_corr, component, prepend_bos=True, method='standard', num_alphas=5, n_last_tokens=128):\n",
    "    if isinstance(x_clean, str):\n",
    "        clean_tokens = model.to_tokens(x_clean, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        clean_tokens = x_clean\n",
    "\n",
    "    if isinstance(x_corr, str):\n",
    "        corr_tokens = model.to_tokens(x_corr, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        corr_tokens = x_corr\n",
    "\n",
    "    if isinstance(a_clean, str):\n",
    "        a_clean = model.to_single_token(a_clean)\n",
    "\n",
    "    if isinstance(a_corr, str):\n",
    "        a_corr = model.to_single_token(a_corr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corr_logits, corr_cache = get_cache_fw(corr_tokens, component)\n",
    "\n",
    "    if method == 'standard':\n",
    "        logits_diff_, clean_cache, clean_grad_cache = get_cache_fw_and_bw(clean_tokens, a_clean, a_corr, corr_logits, component=component)\n",
    "    elif method == 'ig':\n",
    "        with torch.no_grad():\n",
    "            clean_logits, clean_cache = get_cache_fw(clean_tokens, component)\n",
    "\n",
    "    clean_cache = ActivationCache(clean_cache, model).to('cpu')\n",
    "    corr_cache = ActivationCache(corr_cache, model).to('cpu')\n",
    "\n",
    "    corr_act = clean_cache.stack_activation(component)[:, 0, -n_last_tokens:]\n",
    "    clean_act = corr_cache.stack_activation(component)[:, 0, -n_last_tokens:] # comp, pos dm\n",
    "    del clean_cache, corr_cache\n",
    "    \n",
    "    if clean_act.ndim > 3:\n",
    "        clean_act = clean_act.reshape(-1, clean_act.size(1), clean_act.size(3))\n",
    "        corr_act = corr_act.reshape(-1, corr_act.size(1), corr_act.size(3))\n",
    "        \n",
    "    if method == 'standard':\n",
    "        clean_grad_cache = ActivationCache(clean_grad_cache, model).to('cpu')\n",
    "        clean_grad_act = clean_grad_cache.stack_activation(component).squeeze()\n",
    "        if clean_grad_act.ndim > 3:\n",
    "            clean_grad_act = clean_grad_act.reshape(-1, clean_grad_act.size(1), clean_grad_act.size(3))\n",
    "        clean_grad_act = clean_grad_act[:, -n_last_tokens:].cpu()\n",
    "    elif method == 'ig':\n",
    "        clean_grad_act = []\n",
    "        alphas = torch.linspace(0, 1, num_alphas)\n",
    "        k = clean_act.shape[0] // model.cfg.n_layers\n",
    "        for l in tqdm(range(1, model.cfg.n_layers+1)):\n",
    "            ig_patch = torch.zeros_like(clean_act[:k * l], device=clean_act.device)\n",
    "            for alpha in alphas:\n",
    "                a_alpha = alpha * clean_act[:k * l] + (1 - alpha) * corr_act[:k * l]\n",
    "                logits_alpha, grad_alpha = get_cache_fw_with_modified_activations(clean_tokens, a_alpha, a_clean, a_corr, l, component)\n",
    "                if grad_alpha.ndim > 3:\n",
    "                    grad_alpha = grad_alpha.reshape(-1, grad_alpha.size(1), grad_alpha.size(3))\n",
    "                grad_alpha = grad_alpha[:, -n_last_tokens:].cpu()\n",
    "                ig_patch += grad_alpha * (clean_act[:k * l] - corr_act[:k * l])\n",
    "                del a_alpha, logits_alpha, grad_alpha\n",
    "            clean_grad_act.append(ig_patch / num_alphas)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        clean_grad_act = torch.cat(clean_grad_act, dim=0)\n",
    "\n",
    "    print(\"Gradients collected! Computing the patch...\")\n",
    "    patch = einops.reduce(\n",
    "        clean_grad_act * (clean_act - corr_act),\n",
    "        \"component pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    del clean_act, corr_act, clean_grad_act\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return patch\n",
    "\n",
    "def get_cache_fw(tokens, component):\n",
    "    filter = lambda name: utils.get_act_name(component) in name\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "    def fw_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter, fw_cache_hook, \"fwd\")\n",
    "    logits = model(tokens)\n",
    "    model.reset_hooks()\n",
    "    return logits, ActivationCache(cache, model)\n",
    "\n",
    "def get_cache_fw_with_modified_activations(tokens, x_int, a_clean, a_corr, layer, component):\n",
    "    hook_point = utils.get_act_name(component, layer)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    def fw_hook(act, mod_act, hook):\n",
    "        act = mod_act\n",
    "\n",
    "    fw_hook_fn = partial(fw_hook, mod_act=x_int.squeeze())\n",
    "    model.add_hook(hook_point, fw_hook_fn, \"fwd\")\n",
    "    \n",
    "    grad_cache = {}\n",
    "    def bw_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    \n",
    "    model.add_hook(hook_point, bw_cache_hook, \"bwd\")\n",
    "    logits = model(tokens)\n",
    "    value = logits_diff(logits, a_clean, a_corr)\n",
    "    value.backward()\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    return value.item(), grad_cache[hook_point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtasks\n",
    "\n",
    "We then explore each subtask mechanistically to understand which are the components responsible for each choice made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 - Choosing the right species\n",
    "The first step is choosing the right species to focus on. This is a key step since it preceeds the attribute check. It is also the most difficult one since the choice doesn't depend only on the species and the entity, but has to be made already considering the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_species = ['grimpus', 'lorpus', 'wumpus', 'zumpus', 'sterpus', 'numpus', 'jompus', 'brimpus', 'yumpus', 'tumpus', 'dumpus', 'vumpus', 'rompus', 'lempus', 'gorpus', 'shumpus', 'impus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer True or False to the following question. Answer as in the examples.\n",
      "\n",
      "Every shumpus is opaque. Gorpuses are numpuses. Numpuses are brown. Lorpuses are not earthy. Jompuses are small. Each numpus is a shumpus. Each gorpus is a jompus. Lorpuses are tumpuses. Each gorpus is wooden. Shumpuses are wumpuses. Every shumpus is a zumpus. Each wumpus is not fast. Impuses are not cold. Numpuses are impuses. Rex is a numpus. Rex is a lorpus.\n",
      "Question: Is Rex brown?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Rex is a numpus.\n",
      "(2) Numpuses are brown.\n",
      "(3) Rex is brown.\n",
      "Answer: True\n",
      "\n",
      "Every grimpus is large. Sterpuses are brown. Lorpuses are not cold. Every sterpus is a dumpus. Each sterpus is a grimpus. Each lorpus is a wumpus. Stella is a sterpus. Stella is a lorpus.\n",
      "Question: Is Stella brown?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Stella is a sterpus.\n",
      "(2) Sterpuses are brown.\n",
      "(3) Stella is brown.\n",
      "Answer: True\n",
      "\n",
      "Every rompus is not discordant. Yumpuses are brimpuses. Dumpuses are moderate. Yumpuses are shy. Yumpuses are rompuses. Every dumpus is a wumpus. Sam is a yumpus. Sam is a dumpus.\n",
      "Question: Is Sam shy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sam is a yumpus.\n",
      "(2) Yumpuses are shy.\n",
      "(3) Sam is shy.\n",
      "Answer: True\n",
      "\n",
      "Every dumpus is a sterpus. Dumpuses are wooden. Numpuses are dull. Every yumpus is a jompus. Dumpuses are zumpuses. Each vumpus is not aggressive. Brimpuses are sweet. Rompuses are vumpuses. Rompuses are fruity. Every zumpus is not discordant. Each brimpus is a rompus. Impuses are not cold. Every yumpus is a dumpus. Yumpuses are transparent. Rompuses are yumpuses. Each brimpus is a numpus. Impuses are shumpuses. Jompuses are rainy. Sally is an impus. Sally is a brimpus.\n",
      "Question: Is Sally sweet?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Sally is a brimpus.\n",
      "(2) Brimpuses are sweet.\n",
      "(3) Sally is sweet.\n",
      "Answer: True\n",
      "\n",
      "Each lorpus is bright. Each gorpus is a sterpus. Every brimpus is floral. Gorpuses are dumpuses. Wumpuses are brimpuses. Gorpuses are snowy. Dumpuses are not large. Lorpuses are vumpuses. Wumpuses are gorpuses. Wumpuses are not spicy. Max is a lorpus. Max is a wumpus.\n",
      "Question: Is Max spicy?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Max is a wumpus.\n",
      "(2) Wumpuses are not spicy.\n",
      "(3) Max is not spicy.\n",
      "Answer: False\n",
      "\n",
      "Each sterpus is a vumpus. Every wumpus is not fast. Lorpuses are not opaque. Lorpuses are sterpuses. Lorpuses are wumpuses. Every shumpus is not angry. Gorpuses are fruity. Sterpuses are shumpuses. Every sterpus is not cold. Every gorpus is a rompus. Wren is a lorpus. Wren is a gorpus.\n",
      "Question: Is Wren opaque?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Wren is a lorpus.\n",
      "(2) Lorpuses are not opaque.\n",
      "(3) Wren is not opaque.\n",
      "Answer: False\n",
      "\n",
      "Every brimpus is a rompus. Each grimpus is a lempus. Lempuses are not hot. Every lempus is a sterpus. Lempuses are impuses. Every grimpus is a wumpus. Each wumpus is opaque. Every brimpus is orange. Each sterpus is wooden. Grimpuses are not feisty. Polly is a lempus. Polly is a brimpus.\n",
      "Question: Is Polly hot?\n",
      "Think step-by-step.\n",
      "\n",
      "(1) Polly is a lempus.\n",
      "(2) Lempuses are not hot.\n",
      "(3) Polly is not hot.\n",
      "Answer: False\n",
      "\n",
      "Impuses are rainy. Grimpuses are moderate. Each lempus is an impus. Tumpuses are lorpuses. Each lempus is not liquid. Tumpuses are blue. Each tumpus is a rompus. Each lempus is a tumpus. Each lorpus is small. Every grimpus is a brimpus. Wren is a tumpus. Wren is a grimpus.\n",
      "Question: Is Wren blue?\n",
      "Think step-by-step.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "x_clean = correct_preds['prompt'].iloc[idx]\n",
    "cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "label = correct_preds['label'].iloc[idx]\n",
    "cot_gold = ast.literal_eval(cot_gold)\n",
    "print(x_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "\n",
    "for id_, s in enumerate(species):\n",
    "    if s in cot_gold[0]:\n",
    "        break\n",
    "\n",
    "a_clean = species_token[id_].cpu()\n",
    "a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n"
     ]
    }
   ],
   "source": [
    "stop_tokens = [' a',]\n",
    "\n",
    "print(\"Generating...\")\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla = compute_dla(clean_out, 'resid_pre', a_clean, a_corr, prepend_bos=False)\n",
    "mlp_dla = compute_dla(clean_out, 'mlp_out', a_clean, a_corr, prepend_bos=False)\n",
    "attn_dla = compute_dla(clean_out, 'result', a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L10H7+', 'L14H0+', 'L14H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=2, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "assert len(species) == 2, \"More than two species detected!\"\n",
    "assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_corr = cot_gold.copy()\n",
    "\n",
    "cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "context = x_clean.split('\\n\\n')\n",
    "\n",
    "context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "x_corr = '\\n\\n'.join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_out = generate_until_stop(x_corr, stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients collected! Computing the patch...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m atp_q \u001b[38;5;241m=\u001b[39m attribution_patching(clean_out, corr_out, a_clean, a_corr, component\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m atp_k \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_patching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstandard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m atp_v \u001b[38;5;241m=\u001b[39m attribution_patching(clean_out, corr_out, a_clean, a_corr, component\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mattribution_patching\u001b[0;34m(x_clean, x_corr, a_clean, a_corr, component, prepend_bos, method, num_alphas, n_last_tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m     corr_logits, corr_cache \u001b[38;5;241m=\u001b[39m get_cache_fw(corr_tokens, component)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     logits_diff_, clean_cache, clean_grad_cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_cache_fw_and_bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mig\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mget_cache_fw_and_bw\u001b[0;34m(tokens, a_clean, a_corr, corr_logits, component)\u001b[0m\n\u001b[1;32m     56\u001b[0m clean_logits \u001b[38;5;241m=\u001b[39m model(tokens)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     57\u001b[0m value \u001b[38;5;241m=\u001b[39m logits_diff(clean_logits, a_clean, a_corr) \u001b[38;5;66;03m#- logits_diff(corr_logits.cpu(), a_clean, a_corr)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     value\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     63\u001b[0m     ActivationCache(cache, model),\n\u001b[1;32m     64\u001b[0m     ActivationCache(grad_cache, model),\n\u001b[1;32m     65\u001b[0m )\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU "
     ]
    }
   ],
   "source": [
    "atp_q = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='q', prepend_bos=False, method='standard')\n",
    "atp_k = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='k', prepend_bos=False, method='standard')\n",
    "atp_v = attribution_patching(clean_out, corr_out, a_clean, a_corr, component='v', prepend_bos=False, method='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_qkv_atp(atp_q, atp_k, atp_v, **kwargs):\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Queries\", \"Keys\", \"Values\"), shared_xaxes=True, vertical_spacing=0.05)\n",
    "    \n",
    "    for i, (atp, hook) in enumerate(zip([atp_q, atp_k, atp_v], ['q', 'k', 'v'])):\n",
    "        plot = plot_atp(atp, clean_out, component=hook, prepend_bos=False)\n",
    "        for trace in plot.data:\n",
    "            fig.add_trace(trace, row=1+i, col=1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        coloraxis1=dict(colorscale='RdBu', cmin=-0.5, cmax=0.5),\n",
    "        showlegend=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_qkv_atp(atp_q, atp_k, atp_v)\n",
    "fig.write_html(\"fig/s1_AtP_qkv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 256])\n",
      "torch.Size([1, 1443, 1, 256])\n",
      "torch.Size([1, 128, 256]) torch.Size([1, 128, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.38 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m atp \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_patching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 58\u001b[0m, in \u001b[0;36mattribution_patching\u001b[0;34m(x_clean, x_corr, a_clean, a_corr, component, prepend_bos, method, num_alphas, n_last_tokens)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m     57\u001b[0m     a_alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m clean_act[:k \u001b[38;5;241m*\u001b[39m l] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m corr_act[:k \u001b[38;5;241m*\u001b[39m l]\n\u001b[0;32m---> 58\u001b[0m     logits_alpha, grad_alpha \u001b[38;5;241m=\u001b[39m \u001b[43mget_cache_fw_with_modified_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_alpha\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     60\u001b[0m         grad_alpha \u001b[38;5;241m=\u001b[39m grad_alpha\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, grad_alpha\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), grad_alpha\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn[19], line 109\u001b[0m, in \u001b[0;36mget_cache_fw_with_modified_activations\u001b[0;34m(tokens, x_int, a_clean, a_corr, layer, component)\u001b[0m\n\u001b[1;32m    106\u001b[0m     grad_cache[hook\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m act\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    108\u001b[0m model\u001b[38;5;241m.\u001b[39madd_hook(hook_point, bw_cache_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m value \u001b[38;5;241m=\u001b[39m logits_diff(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m, a_clean, a_corr)\n\u001b[1;32m    110\u001b[0m value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    112\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:568\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munembed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformer_lens/components.py:81\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m, residual: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_vocab_out\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 81\u001b[0m         \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_U\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_U\n\u001b[1;32m     87\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(operands[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[38;5;241m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_equation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, equation, \u001b[38;5;241m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.38 GiB. GPU "
     ]
    }
   ],
   "source": [
    "component = 'k'\n",
    "atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False, method='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(atp, 'tmp/atp_k.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "component = 'k'\n",
    "fig = plot_atp(atp, clean_out, component, prepend_bos=False)\n",
    "fig.write_html(f\"fig/s1_AtP_ig_{component}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 - Attribute check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['2']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is', ' are']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_steps = cot_gold[1].split()\n",
    "for i, step in enumerate(cot_steps):\n",
    "    if \"is\" in step or \"are\" in step:\n",
    "        attribute = cot_steps[i+1].replace('.', '')\n",
    "        a_clean = ' ' + attribute\n",
    "\n",
    "a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "a_clean = [model.to_single_token(a_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 3.683\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 2) | {attribute} - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L8H6-', 'L10H5+', 'L10H7+', 'L13H2+', 'L13H4+', 'L14H0+', 'L14H3-', 'L14H4+', 'L16H0+', 'L16H4+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_patterns.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtP\n",
    "import random\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "context, clean_question = test_example.split('Question: ')\n",
    "clues = context.split('. ')\n",
    "\n",
    "for s in all_species:\n",
    "    if s in clean_question.lower(): \n",
    "        s_star = s\n",
    "        break\n",
    "\n",
    "a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "corr_question = clean_question.replace(a_clean, a_corr)\n",
    "\n",
    "for i, c in enumerate(clues):\n",
    "    if a_corr in c:\n",
    "        for s in all_species:\n",
    "            if s in c.lower(): break\n",
    "\n",
    "        clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "\n",
    "clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component='resid', prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_atp(atp, clean_out_new, component=\"resid\", prepend_bos=False)\n",
    "fig.write_html(\"fig/s2_AtP_resid.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 - The right connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_clean.split('\\n\\n')[-2]\n",
    "context, question = example.split('Question: ')\n",
    "\n",
    "subject = question.split()[1]\n",
    "attribute = question.split()[2][:-1]\n",
    "\n",
    "if 'not' in cot_gold[2]:\n",
    "    a_clean = [model.to_single_token(' not')]\n",
    "    a_corr = [model.to_single_token(' ' + attribute)]\n",
    "    clean_label = 'not'\n",
    "    corr_label = attribute\n",
    "else:\n",
    "    a_clean = [model.to_single_token(' ' + attribute)]\n",
    "    a_corr = [model.to_single_token(' not')]\n",
    "    clean_label = attribute\n",
    "    corr_label = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = ['3']\n",
    "clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "stop_tokens = [' is']\n",
    "clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.566\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 3) | {clean_label} - {corr_label}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L12H0+', 'L14H0+', 'L14H4-', 'L14H5+', 'L14H6-', 'L16H7-', 'L17H7+']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 - Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "a_clean = model.to_single_token(' ' + str(label))\n",
    "a_corr = model.to_single_token(' False' if label else ' True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 0.200\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, [a_clean], [a_corr], prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla, mlp_dla, attn_dla)\n",
    "fig.update_layout(title_text=f\"Direct Logit Attribution (Subtask 4) | '{model.to_string(a_clean)}' - '{model.to_string(a_corr)}'\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['L9H2+', 'L9H4+', 'L10H7+', 'L11H6-' 'L14H0+', 'L14H1-', 'L15H1+', 'L15H4-', 'L17H2+', 'L17H7-']\n",
    "fig = plot_patterns(clean_out, patterns, n_cols=3, query_offset=1000, key_offset=1000)\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_patterns.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [02:41<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "stop_tokens = [' a',]\n",
    "\n",
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "\n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "    species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "    \n",
    "    for id_, s in enumerate(species):\n",
    "        if s in cot_gold[0]:\n",
    "            break\n",
    "    \n",
    "    a_clean = species_token[id_].cpu()\n",
    "    a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens, prepend_bos=True)\n",
    "    resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "    resid_dlas.append(resid_dla.cpu())\n",
    "    mlp_dlas.append(mlp_dla.cpu())\n",
    "    attn_dlas.append(attn_dla.cpu())\n",
    "    del resid_dla, mlp_dla, attn_dla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 1) | {species[id_]} -{species[1-id_]}\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    for component in components:\n",
    "        x_clean = correct_preds['prompt'].iloc[idx]\n",
    "        cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "        cot_gold = ast.literal_eval(cot_gold)\n",
    "    \n",
    "        example = x_clean.split('\\n\\n')[-2]\n",
    "        context, question = example.split('Question: ')\n",
    "        \n",
    "        subject = question.split()[1]\n",
    "        species = [' ' + x.strip().split()[-1] for x in context.split('.') if subject in x]\n",
    "        species_token = [model.to_tokens(s, prepend_bos=False)[:, 0] for s in species]\n",
    "        \n",
    "        for id_, s in enumerate(species):\n",
    "            if s in cot_gold[0]:\n",
    "                break\n",
    "        \n",
    "        a_clean = species_token[id_].cpu()\n",
    "        a_corr = torch.cat(species_token[:id_] + species_token[id_+1:]).cpu()\n",
    "    \n",
    "        try:\n",
    "            assert len(species) == 2, \"More than two species detected!\"\n",
    "            assert len(model.to_tokens(species[0])[0]) == len(model.to_tokens(species[1])[0]), \"Species with different token length!\"\n",
    "    \n",
    "            cot_corr = cot_gold.copy()\n",
    "            \n",
    "            cot_corr = [step.lower().replace(species[id_][1:], species[1-id_][1:]).capitalize() for step in cot_gold]\n",
    "            context = x_clean.split('\\n\\n')\n",
    "            \n",
    "            context[-2] = context[-2].replace(cot_gold[1], cot_corr[1])\n",
    "            x_corr = '\\n\\n'.join(context)\n",
    "    \n",
    "            clean_out = generate_until_stop(x_clean, stop_tokens)    \n",
    "            corr_out = generate_until_stop(x_corr, stop_tokens)\n",
    "\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out, corr_out, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 1)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s1_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive AtP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [05:28<00:00, 12.62s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    cot_steps = cot_gold[1].split()\n",
    "    try:\n",
    "        for i, step in enumerate(cot_steps):\n",
    "            if \"is\" in step or \"are\" in step:\n",
    "                attribute = cot_steps[i+1].replace('.', '')\n",
    "                a_clean = ' ' + attribute\n",
    "        \n",
    "        a_corr = [model.to_single_token(tok) for tok in [' not',]]\n",
    "        a_clean = [model.to_single_token(a_clean)]\n",
    "        \n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 2) | attribute - not\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import einops\n",
    "\n",
    "def not_a_species(x):\n",
    "    for s in all_species:\n",
    "        if s in x.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [' a',]\n",
    "components = ['resid', 'mlp_out', 'attn']\n",
    "\n",
    "resid_atps = []\n",
    "mlp_atps = []\n",
    "attn_atps = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    stop_tokens = ['2']\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens)\n",
    "    stop_tokens = [' is', ' are']\n",
    "    clean_out = generate_until_stop(clean_out, stop_tokens, prepend_bos=False)\n",
    "\n",
    "    icl_examples = '\\n\\n'.join(clean_out.split('\\n\\n')[:-2])\n",
    "    test_example = '\\n\\n'.join(clean_out.split('\\n\\n')[-2:])\n",
    "    context, clean_question = test_example.split('Question: ')\n",
    "    clues = context.split('. ')\n",
    "    \n",
    "    for s in all_species:\n",
    "        if s in clean_question.lower(): \n",
    "            s_star = s\n",
    "            break\n",
    "    \n",
    "    a_clean = ' ' + clean_question.split()[2][:-1]\n",
    "    other_attributes = [' '+c.split()[-1] for c in clues if not_a_species(c.split()[-1])]\n",
    "    other_attributes = [a for a in other_attributes if is_single_token(a) and a != a_clean]\n",
    "    a_corr = other_attributes[random.randint(0, len(other_attributes)-1)]\n",
    "    corr_question = clean_question.replace(a_clean, a_corr)\n",
    "    \n",
    "    for i, c in enumerate(clues):\n",
    "        if a_corr in c:\n",
    "            for s in all_species:\n",
    "                if s in c.lower(): break\n",
    "    \n",
    "            clues[i] = c.lower().replace(s, s_star).capitalize()\n",
    "    \n",
    "    clean_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + clean_question\n",
    "    corr_out_new = icl_examples + '\\n\\n' + '. '.join(clues) + corr_question\n",
    "\n",
    "    for component in components:    \n",
    "        try:\n",
    "            assert len(corr_tokens[0]) == len(clean_tokens[0]), f\"Clean and corrupted tokens have different length, {len(clean_tokens[0])} and {len(corr_tokens[0])}, respectively.\"\n",
    "    \n",
    "            atp = attribution_patching(clean_out_new, corr_out_new, a_clean, a_corr, component=component, prepend_bos=False)\n",
    "\n",
    "            if 'resid' in component:\n",
    "                resid_atps.append(atp)\n",
    "            elif 'mlp' in component:\n",
    "                mlp_atps.append(atp)\n",
    "            elif 'attn' in component:\n",
    "                attn_atps.append(atp)\n",
    "        \n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_atp_agg = torch.stack([atp.max(dim=-1).values for atp in resid_atps]).mean(0).unsqueeze(-1)\n",
    "mlp_atp_agg = torch.stack([atp.max(dim=-1).values for atp in mlp_atps]).mean(0).unsqueeze(-1)\n",
    "attn_atp_agg = torch.stack([atp.max(dim=-1).values for atp in attn_atps]).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_atp_agg, mlp_atp_agg, attn_atp_agg, max_val=1)\n",
    "fig.update_layout(title_text=f\"Aggregated Attribution Patching (Subtask 2)\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s2_AtP_agg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 2.838\n",
      "Corrupted logit difference: -3.524\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = model.to_tokens(clean_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(clean_tokens).cpu()\n",
    "\n",
    "clean_logit_diff = logits_diff(clean_logits, a_clean, a_corr)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corr_tokens = model.to_tokens(corr_out_new, prepend_bos=False)\n",
    "with torch.no_grad():\n",
    "    corr_logits = model(corr_tokens).cpu()\n",
    "\n",
    "corr_logit_diff = logits_diff(corr_logits, a_clean, a_corr)\n",
    "print(f\"Corrupted logit difference: {corr_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [02:22<00:00,  5.46s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    cot_gold = correct_preds['cot_gold'].iloc[idx]\n",
    "    cot_gold = ast.literal_eval(cot_gold)\n",
    "        \n",
    "    example = x_clean.split('\\n\\n')[-2]\n",
    "    context, question = example.split('Question: ')\n",
    "    \n",
    "    subject = question.split()[1]\n",
    "    attribute = question.split()[2][:-1]\n",
    "\n",
    "    try:\n",
    "        if 'not' in cot_gold[2]:\n",
    "            a_clean = [model.to_single_token(' not')]\n",
    "            a_corr = [model.to_single_token(' ' + attribute)]\n",
    "            clean_label = 'not'\n",
    "            corr_label = attribute\n",
    "        else:\n",
    "            a_clean = [model.to_single_token(' ' + attribute)]\n",
    "            a_corr = [model.to_single_token(' not')]\n",
    "            clean_label = attribute\n",
    "            corr_label = 'not'\n",
    "\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 3) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s3_DLA_agg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26/26 [08:43<00:00, 20.14s/it]\n"
     ]
    }
   ],
   "source": [
    "resid_dlas = []\n",
    "mlp_dlas = []\n",
    "attn_dlas = []\n",
    "\n",
    "for idx in tqdm(range(len(correct_preds))):\n",
    "    x_clean = correct_preds['prompt'].iloc[idx]\n",
    "    label = correct_preds['label'].iloc[idx]\n",
    "\n",
    "    clean_out = generate_until_stop(x_clean, stop_tokens=[':'])\n",
    "    a_clean = model.to_single_token(' ' + str(label))\n",
    "    a_corr = model.to_single_token(' False' if label else ' True')\n",
    "        \n",
    "    try:\n",
    "        resid_dla, mlp_dla, attn_dla = compute_dla(clean_out, a_clean, a_corr, prepend_bos=False)\n",
    "        resid_dlas.append(resid_dla.cpu())\n",
    "        mlp_dlas.append(mlp_dla.cpu())\n",
    "        attn_dlas.append(attn_dla.cpu())\n",
    "        del resid_dla, mlp_dla, attn_dla\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_dla_agg = torch.stack(resid_dlas).mean(0).unsqueeze(-1)\n",
    "mlp_dla_agg = torch.stack(mlp_dlas).mean(0).unsqueeze(-1)\n",
    "attn_dla_agg = torch.stack(attn_dlas).mean(0).reshape(model.cfg.n_layers, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_dla(resid_dla_agg, mlp_dla_agg, attn_dla_agg)\n",
    "fig.update_layout(title_text=f\"Aggregated Direct Logit Attribution (Subtask 4) | a1 - a2\")\n",
    "fig.show()\n",
    "fig.write_html('fig/s4_DLA_agg.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
