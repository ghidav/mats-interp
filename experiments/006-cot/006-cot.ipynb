{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/asaparov/prontoqa.git\n",
    "cd prontoqa\n",
    "mkdir json\n",
    "unzip generated_ood_data.zip -d json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "data = load_json(\"prontoqa/json/1hop_ProofsOnly_random_noadj.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "labels = [\"is\", \"is not\"]\n",
    "\n",
    "def prepare_example(example, n_shots=5):\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for i in range(n_shots):\n",
    "        change = randint(0, 1)\n",
    "        prompt += example[f\"in_context_example{i}\"]['question'] + \"\\n\"\n",
    "        query = example[f\"in_context_example{i}\"]['query']\n",
    "        \n",
    "        if \"is not\" in query:\n",
    "            label = \"True\" if change == 1 else \"False\"\n",
    "            query = query.replace(\"is not\", labels[change])\n",
    "        else:\n",
    "            label = \"True\" if change == 0 else \"False\"\n",
    "            query = query.replace(\"is\", labels[change])\n",
    "    \n",
    "        prompt += query.replace(\"Prove: \", \"True or False: \") + \" Think step-by-step.\" + \"\\n\\n\"\n",
    "    \n",
    "        for j, step in enumerate(example[f\"in_context_example{i}\"]['chain_of_thought']):\n",
    "            prompt += f\"({j+1}) {step}\\n\"\n",
    "        \n",
    "        prompt += f\"\\nAnswer: {label}\\n\\n\\n\"\n",
    "    \n",
    "    change = randint(0, 1)\n",
    "    prompt += example[\"test_example\"]['question'] + \"\\n\"\n",
    "    query = example[\"test_example\"]['query']\n",
    "    cot = example[\"test_example\"]['chain_of_thought']\n",
    "    \n",
    "    if \"is not\" in query:\n",
    "        label = \"True\" if change == 1 else \"False\"\n",
    "        query = query.replace(\"is not\", labels[change])\n",
    "    else:\n",
    "        label = \"True\" if change == 0 else \"False\"\n",
    "        query = query.replace(\"is\", labels[change])\n",
    "    \n",
    "    prompt += query.replace(\"Prove: \", \"True or False: \") + \" Think step-by-step.\" + \"\\n\\n\"\n",
    "        \n",
    "    return prompt, label, cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yumpuses are grimpuses. Impuses are sterpuses. Sterpuses are dumpuses. Each sterpus is a numpus. Each impus is windy. Lempuses are melodic. Rompuses are lorpuses. Rompuses are not earthy. Numpuses are sweet. Rompuses are impuses. Sterpuses are transparent. Every yumpus is not dull. Lorpuses are luminous. Impuses are lempuses. Alex is a rompus. Alex is a yumpus.\\nTrue or False: Alex is earthy. Think step-by-step.\\n\\n(1) Alex is a rompus.\\n(2) Rompuses are not earthy.\\n(3) Alex is not earthy.\\n\\nAnswer: False\\n\\n\\nEvery impus is a tumpus. Sterpuses are jompuses. Impuses are grimpuses. Each sterpus is not large. Grimpuses are not fast. Every impus is not red. Lorpuses are not happy. Every tumpus is a rompus. Every tumpus is a lorpus. Every lorpus is a sterpus. Rompuses are metallic. Each tumpus is muffled. Every jompus is not aggressive. Gorpuses are sunny. Every lorpus is a gorpus. Shumpuses are earthy. Shumpuses are zumpuses. Every sterpus is a yumpus. Alex is a shumpus. Alex is a sterpus.\\nTrue or False: Alex is not large. Think step-by-step.\\n\\n(1) Alex is a sterpus.\\n(2) Each sterpus is not large.\\n(3) Alex is not large.\\n\\nAnswer: True\\n\\n\\nEach tumpus is a brimpus. Wumpuses are shumpuses. Each shumpus is fast. Each jompus is a gorpus. Every tumpus is dull. Wumpuses are shy. Jompuses are not transparent. Jompuses are lempuses. Gorpuses are orange. Each wumpus is a jompus. Max is a wumpus. Max is a tumpus.\\nTrue or False: Max is not shy. Think step-by-step.\\n\\n(1) Max is a wumpus.\\n(2) Wumpuses are shy.\\n(3) Max is shy.\\n\\nAnswer: False\\n\\n\\nShumpuses are slow. Each wumpus is an impus. Every grimpus is not mean. Each grimpus is a shumpus. Each jompus is a lorpus. Vumpuses are loud. Every jompus is opaque. Impuses are earthy. Every vumpus is a lempus. Each grimpus is a vumpus. Every vumpus is a brimpus. Wumpuses are grimpuses. Each lempus is nervous. Each wumpus is not wooden. Alex is a jompus. Alex is a grimpus.\\nTrue or False: Alex is mean. Think step-by-step.\\n\\n(1) Alex is a grimpus.\\n(2) Every grimpus is not mean.\\n(3) Alex is not mean.\\n\\nAnswer: False\\n\\n\\nEach brimpus is a zumpus. Every lempus is feisty. Every brimpus is a lempus. Grimpuses are sterpuses. Every dumpus is not dull. Each sterpus is not overcast. Every grimpus is a tumpus. Every wumpus is a brimpus. Each zumpus is not fruity. Each lempus is a grimpus. Every lempus is an impus. Each jompus is not small. Numpuses are kind. Every brimpus is liquid. Tumpuses are temperate. Each dumpus is a shumpus. Every tumpus is a gorpus. Each tumpus is a numpus. Every impus is melodic. Wumpuses are jompuses. Grimpuses are not sour. Each wumpus is not transparent. Polly is a grimpus. Polly is a dumpus.\\nTrue or False: Polly is not sour. Think step-by-step.\\n\\n(1) Polly is a grimpus.\\n(2) Grimpuses are not sour.\\n(3) Polly is not sour.\\n\\nAnswer: True\\n\\n\\nImpuses are not orange. Each rompus is a tumpus. Each lorpus is a dumpus. Each tumpus is large. Every tumpus is a lorpus. Lorpuses are not luminous. Numpuses are not bitter. Every impus is a jompus. Lorpuses are numpuses. Every numpus is a yumpus. Lempuses are melodic. Vumpuses are not angry. Each tumpus is a vumpus. Each yumpus is bright. Rompuses are lempuses. Dumpuses are not hot. Numpuses are zumpuses. Every rompus is not fast. Polly is a lorpus. Polly is an impus.\\nTrue or False: Polly is not luminous. Think step-by-step.\\n\\n', 'True', ['Polly is a lorpus.', 'Lorpuses are not luminous.', 'Polly is not luminous.'])\n"
     ]
    }
   ],
   "source": [
    "prompts = [prepare_example(x, n_shots=5) for x in data.values()]\n",
    "print(prompts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823ed649a36348199bb1a525e0cba8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gemma-2b')\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(text):\n",
    "    pattern = r'Answer:\\s*(.*)'\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "    else:\n",
    "        answer = \"NaN\"\n",
    "\n",
    "    return answer\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    prompt, label = prompts[i]\n",
    "    #out = model.generate(prompt, 64, temperature=0, verbose=False)[len(prompt):]\n",
    "    out = generate_chat(prompt, max_new_tokens=64, temperature=0, verbose=False)[len(prompt):]\n",
    "\n",
    "    prediction = extract_answer(out)\n",
    "    scores.append(prediction == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Score: {np.array(scores).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an accuracy of 66% with 5 shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
