{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405fb172-d43a-4daa-b9a4-7cd383c4ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4737d0-979d-4102-8795-e36d8eb5e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbb7cf79a4842daa8646d0006eb18de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gemma-2b', device=device)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ced5553-af4a-477d-96f9-d00cd55ac619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "atp_dir = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "sys.path.append(atp_dir)\n",
    "\n",
    "from atp import Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61d370b-0452-4953-b9b4-a11f1aee137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {\n",
    "        'name': 'Indirect Object Identification',\n",
    "        'description': 'Find the correct token to predict when dealing with repeated objects in the sentence',\n",
    "        'clean_prompt': 'John and Mary went to the store, then John gave a bottle of milk to',\n",
    "        'corrupted_prompt': 'John and Mary went to the store, then Mary gave a bottle of milk to',\n",
    "        'clean_answer': ' Mary',\n",
    "        'corrupted_answer': ' John'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Numbers Addition',\n",
    "        'description': 'Find the right outcome given by the addition of two numbers',\n",
    "        'clean_prompt': '10 + 3 = 1',\n",
    "        'corrupted_prompt': '10 + 6 = 1',\n",
    "        'clean_answer': '3',\n",
    "        'corrupted_answer': '6'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Geographic Knowledge',\n",
    "        'description': 'Find the right country a city is located into.',\n",
    "        'clean_prompt': 'Paris is in',\n",
    "        'corrupted_prompt': 'Rome is in',\n",
    "        'clean_answer': 'France',\n",
    "        'corrupted_answer': 'Italy'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ICL - Interpolation',\n",
    "        'description': 'Given the coordinates of two 2D points, find the y of a third point located in between.',\n",
    "        'clean_prompt': '(-2, 1) (4, -2) (2, -1)\\n(-1, 0) (3, 4) (1, 2)\\n(1, 3) (5, 1) (3, ',\n",
    "        'corrupted_prompt': '(-2, 1) (4, -2) (2, -1)\\n(-1, 0) (3, 4) (1, 2)\\n(1, 3) (5, 5) (3, ',\n",
    "        'clean_answer': '2',\n",
    "        'corrupted_answer': '4'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Question Answering',\n",
    "        'description': 'Answering multiple choice questions.',\n",
    "        'clean_prompt': 'Which one is red?\\n\\n(A) the heart\\n(B) the sun\\n(C) the sea\\n\\nAnswer: (',\n",
    "        'corrupted_prompt': 'Which one is blue?\\n\\n(A) the heart\\n(B) the sun\\n(C) the sea\\n\\nAnswer: (',\n",
    "        'clean_answer': 'C',\n",
    "        'corrupted_answer': 'A'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Gender Agreement',\n",
    "        'description': 'Predicting the right pronoun based on the gender.',\n",
    "        'clean_prompt': 'John saw Marc at the park and waved at',\n",
    "        'corrupted_prompt': 'John saw Mary at the park and waved at',\n",
    "        'clean_answer': ' him',\n",
    "        'corrupted_answer': ' her'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ICL - Contrary Identification',\n",
    "        'description': 'Identify the contrary of a given word.',\n",
    "        'clean_prompt': 'happy -> sad\\nexciting -> boring\\nfast ->',\n",
    "        'corrupted_prompt': 'happy -> sad\\nexciting -> boring\\nslow ->',\n",
    "        'clean_answer': 'slow',\n",
    "        'corrupted_answer': 'fast'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ICL - Sentiment Analysis',\n",
    "        'description': 'Identify the sentiment of sentences.',\n",
    "        'clean_prompt': \"This is amazing | Positive\\nIt has been the worst thing I've ever seen | Negative\\nIt was good overall | \",\n",
    "        'corrupted_prompt': \"This is amazing | Positive\\nIt has been the worst thing I've ever seen | Negative\\nIt was bad overall | \",\n",
    "        'clean_answer': 'Positive',\n",
    "        'corrupted_answer': 'Negative'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b9648e-dae4-4250-835c-bf4a8c966811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0 - Passed\n",
      "TASK 1 - Passed\n",
      "TASK 2 - Passed\n",
      "TASK 3 - Passed\n",
      "TASK 4 - Passed\n",
      "TASK 5 - Passed\n",
      "TASK 6 - Passed\n",
      "TASK 7 - Passed\n"
     ]
    }
   ],
   "source": [
    "#  Checks\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"TASK {i} - \", end='')\n",
    "    assert len(model.to_str_tokens(task['clean_prompt'])) == len(model.to_str_tokens(task['corrupted_prompt']))\n",
    "    assert model.to_single_token(task['clean_answer'])\n",
    "    assert model.to_single_token(task['corrupted_answer'])\n",
    "\n",
    "    if not os.path.exists(f\"task-{i}\"):\n",
    "        os.makedirs(f\"task-{i}\")\n",
    "        \n",
    "    print(\"Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7598cb4b-06fe-4cb4-883a-0af5a8981312",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 0\n",
    "task = tasks[task_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e759ab9a-d9e5-4e58-a40f-4d775a30be83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0887aa211b4329b6cacbfcda436d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d5bae31b914b55b72c7846d91043d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean\n",
      "John and Mary went to the store, then John gave a bottle of milk to Mary.\n",
      "\n",
      "Corrupted\n",
      "John and Mary went to the store, then Mary gave a bottle of milk to John.\n"
     ]
    }
   ],
   "source": [
    "clean_out = model.generate(\n",
    "    task['clean_prompt'], \n",
    "    max_new_tokens=2,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "corrupted_out = model.generate(\n",
    "    task['corrupted_prompt'], \n",
    "    max_new_tokens=2,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Clean\\n\", clean_out, sep='')\n",
    "print(\"\\nCorrupted\\n\", corrupted_out, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "498f367d-856a-4437-a73b-2b0755c12d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 3.917\n",
      "Corrupted logit difference: -4.342\n",
      "Patching...\n"
     ]
    }
   ],
   "source": [
    "method = 'atp'\n",
    "component = 'attn_all'\n",
    "\n",
    "patching = Patching(model, method)\n",
    "\n",
    "patching.patching(\n",
    "    task['clean_prompt'], task['clean_answer'], \n",
    "    task['corrupted_prompt'], task['corrupted_answer'], \n",
    "    component=component\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d74a14-7f80-415d-b1c9-a045169141d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patching.patch[patching.patch.abs() < 0.2] = 0\n",
    "fig = patching.plot()\n",
    "fig.write_html(f\"task-{task_id}/{method}_{component}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84471231-1879-4490-9aa3-a1c7b512beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = [(l, h) for h in range(8) for l in range(18) if patching.patch.max(-1).values[8*l + h] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "456afe33-86df-4b27-91e9-301b9fdc1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, cache = model.run_with_cache(model.to_tokens(task['clean_prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86be736c-eb1d-467c-ae6f-caaef688f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(f\"task-{task_id}/patterns\"):\n",
    "    os.makedirs(f\"task-{task_id}/patterns\")\n",
    "\n",
    "labels = [f\"{tok} ({i})\" for i, tok in enumerate(model.to_str_tokens(task['clean_prompt']))]\n",
    "\n",
    "for l, h in heads:\n",
    "    data = cache[f'blocks.{l}.attn.hook_pattern'][0, h].cpu()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        data,\n",
    "        labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "        x=labels,\n",
    "        y=labels,\n",
    "        title=f'Attention patter at head {h} of layer {l}',\n",
    "        color_continuous_scale=\"Blues\"\n",
    "    )\n",
    "    \n",
    "    # Adjust the layout for better readability\n",
    "    fig.update_xaxes(tickangle=35)\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"))\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.write_html(f\"task-{task_id}/patterns/L{l}H{h}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774ef37-c8c8-4cff-82c0-f6330d5ca6cd",
   "metadata": {},
   "source": [
    "## SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb279ce-2ce5-4e0b-bc9f-e4907877eea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8828c9ef447b42ad8c23d8216304ee9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Release jbloom/Gemma-2b-Residual-Stream-SAEs not found in pretrained SAEs directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m HookedTransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mSparseAutoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjbloom/Gemma-2b-Residual-Stream-SAEs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.12.hook_resid_post\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# change this to another specific SAE ID in the release if desired. \u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m activation_store \u001b[38;5;241m=\u001b[39m ActivationsStore\u001b[38;5;241m.\u001b[39mfrom_config(model, sparse_autoencoder\u001b[38;5;241m.\u001b[39mcfg)\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/sae_lens/training/sparse_autoencoder.py:476\u001b[0m, in \u001b[0;36mSparseAutoencoder.from_pretrained\u001b[0;34m(cls, release, sae_id, device)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# get the repo id and path to the SAE\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m release \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sae_directory:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelease \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in pretrained SAEs directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sae_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sae_directory[release]\u001b[38;5;241m.\u001b[39msaes_map:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msae_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in release \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Release jbloom/Gemma-2b-Residual-Stream-SAEs not found in pretrained SAEs directory."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\")\n",
    "sparse_autoencoder = SparseAutoencoder.from_pretrained(\n",
    "  \"gemma-2b-res-jb\", # to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "  \"blocks.12.hook_resid_post\" # change this to another specific SAE ID in the release if desired. \n",
    ")\n",
    "activation_store = ActivationsStore.from_config(model, sparse_autoencoder.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0e42a-e98c-4d8f-8c18-4473a2ebb917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
