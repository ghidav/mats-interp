{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26143130-b700-445a-a190-c2d87aac2c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "tqdm.pandas()\n",
    "\n",
    "init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac36ba4-327b-4ca1-92b1-76cf6a63020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gpt2')\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f1b088-1c52-4507-9373-867b11652b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1f0cc6359d43b581040b3710e8d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "REPO_ID = \"jbloom/GPT2-Small-SAEs-Reformatted\"\n",
    "path = snapshot_download(repo_id=REPO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e3bf98-222e-43de-b52e-8b1b1e9288c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning:\n",
      "\n",
      "The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "\n",
      "  8%|▊         | 1/12 [00:15<02:49, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2/12 [00:26<02:09, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:39<01:55, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [00:46<01:25, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [01:03<01:30, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [01:13<01:11, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [01:29<01:05, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [01:35<00:43, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [01:42<00:28,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [01:48<00:17,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [01:54<00:07,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:21<00:00, 21.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import LMSparseAutoencoderSessionloader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "saes = []\n",
    "\n",
    "for l in tqdm(range(model.cfg.n_layers)):\n",
    "    _, sae_group, activation_store = LMSparseAutoencoderSessionloader.load_pretrained_sae(\n",
    "        path = os.path.join(path, f\"blocks.{l}.hook_resid_pre\"), device=device\n",
    "    )\n",
    "    sae_group.eval()\n",
    "    saes.append(sae_group[f'blocks.{l}.hook_resid_pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f320e36d-ff83-4fe3-b087-e908e825dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning:\n",
      "\n",
      "The repository for eriktks/conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eriktks/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964334c45ea4a1a9659c79a8a5343a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a3685652054a9196d4e38ecc49f949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54be0eaf24240238f6322947ad3fd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c162f98dbcb1477cb5b3d6307afb55fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d509842befc4008b256744727c1498c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706fa46e64254c0a8f5a59c0376c18ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('eriktks/conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9acf36fb-0a47-466c-92ad-70603e14459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14041it [00:13, 1068.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, x in tqdm(enumerate(data['train'])):\n",
    "    prompt = ' '.join(x['tokens'])\n",
    "    \n",
    "    tokens = x['tokens']\n",
    "    model_tokens = model.to_str_tokens(prompt)[1:]\n",
    "\n",
    "    # Map tokens to model tokens\n",
    "    mapping = []\n",
    "    tok_id = 0\n",
    "    for j, tok in enumerate(model_tokens):\n",
    "        if j != 0:\n",
    "            if tok[0] == ' ':\n",
    "                tok_id += 1\n",
    "\n",
    "        mapping.append(tok_id)\n",
    "\n",
    "    # Token reconstruction test\n",
    "    reconstructed_tokens = []\n",
    "    prev_id = 0\n",
    "    token_str = ''\n",
    "    for tok, id_ in zip(model_tokens, mapping):\n",
    "        if id_ == prev_id:\n",
    "            token_str += tok\n",
    "        else:\n",
    "            reconstructed_tokens.append(token_str.strip())\n",
    "            token_str = tok\n",
    "            prev_id = id_\n",
    "\n",
    "    reconstructed_tokens.append(token_str.strip())\n",
    "            \n",
    "    assert tokens == reconstructed_tokens, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "704ebd91-64c9-42be-be16-69118babb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_hook(x, hook, sae):\n",
    "    sae_out, f_act, *_ = sae(x)\n",
    "    \n",
    "    # Function to capture the gradient\n",
    "    def capture_grad(grad):\n",
    "        sae_grad_cache[hook.name] = grad.clone()\n",
    "\n",
    "    # Register the hook to capture the gradient\n",
    "    if f_act.requires_grad:\n",
    "        f_act.register_hook(capture_grad)\n",
    "    \n",
    "    sae_cache[hook.name] = f_act.detach()\n",
    "    return sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39bcdf37-249c-4acb-a4ad-581d5fb78064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sae_cache = {}\n",
    "sae_grad_cache = {}\n",
    "\n",
    "for i, x in tqdm(enumerate(data['train'])):\n",
    "    prompt = ' '.join(x['tokens'])\n",
    "    \n",
    "    tokens = x['tokens']\n",
    "    model_tokens = model.to_str_tokens(prompt)[1:]\n",
    "\n",
    "    # Map tokens to model tokens\n",
    "    mapping = []\n",
    "    tok_id = 0\n",
    "    for j, tok in enumerate(model_tokens):\n",
    "        if j != 0:\n",
    "            if tok[0] == ' ':\n",
    "                tok_id += 1\n",
    "\n",
    "        mapping.append(tok_id)\n",
    "\n",
    "    # Running model cache\n",
    "    out = model.run_with_hooks(\n",
    "            model.to_tokens(prompt),\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    utils.get_act_name('resid_pre', l),\n",
    "                    partial(reconstr_hook, sae=saes[l]),\n",
    "                )\n",
    "            for l in range(model.cfg.n_layers)]\n",
    "        )[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95be6591-63f6-46d6-a640-4714a25adb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 24576])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_cache['blocks.0.hook_resid_pre'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65499a5-3fde-4176-ab59-d29b65c04362",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "# NER CLF {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "203cadb4-890c-4ea8-8770-a5e7586f8b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "619e2190-02e9-4fd6-93e5-4242df3d7439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8165c-3f0d-4867-9a94-49255da489ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
