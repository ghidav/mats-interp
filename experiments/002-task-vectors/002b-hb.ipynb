{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Boost for ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02d4386ddbf4813ab2e7d20d732d195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gemma-2b'\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tasks.json\", \"r\") as f:\n",
    "    tasks = json.load(f)['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "slices = []\n",
    "\n",
    "pos = 0\n",
    "train_prompt = \"\"\n",
    "for i, (inp, out) in enumerate(zip(tasks[idx]['train_input'], tasks[idx]['train_output'])):\n",
    "    train_prompt += inp + '\\n' + out + \"\\n\\n\"\n",
    "    pos += len(model.to_tokens(inp, prepend_bos=False)[0]) + 1\n",
    "    if i != 0:\n",
    "        slices.append((pos, pos + len(model.to_tokens(out, prepend_bos=False)[0])))\n",
    "    pos += len(model.to_tokens(out, prepend_bos=False)[0]) + 1 + int(model_name == 'gpt2')\n",
    "\n",
    "test_prompt = \"\"\n",
    "for inp, out in zip(tasks[idx]['test_input'], tasks[idx]['test_output']):\n",
    "    test_prompt += inp + '\\n' + out + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><html> <h1> <p>\n",
      "</p> </h1> </html>\n",
      "\n",
      "<html> <h1> <div>\u001b[1m\n",
      "\u001b[0m\u001b[1m</\u001b[0m\u001b[1mdiv\u001b[0m\u001b[1m>\u001b[0m\u001b[1m \u001b[0m\u001b[1m</h1>\u001b[0m\u001b[1m </\u001b[0m\u001b[1mhtml\u001b[0m>\n",
      "\n",
      "<h1> <div> <p>\u001b[1m\n",
      "\u001b[0m\u001b[1m</\u001b[0m\u001b[1mp\u001b[0m\u001b[1m>\u001b[0m\u001b[1m </\u001b[0m\u001b[1mdiv\u001b[0m\u001b[1m>\u001b[0m\u001b[1m \u001b[0m</h1>\n",
      "\n",
      "<html> <h1> <p> <div>\u001b[1m\n",
      "\u001b[0m\u001b[1m</\u001b[0m\u001b[1mdiv\u001b[0m\u001b[1m>\u001b[0m\u001b[1m </\u001b[0m\u001b[1mp\u001b[0m\u001b[1m>\u001b[0m\u001b[1m \u001b[0m\u001b[1m</h1>\u001b[0m\u001b[1m </\u001b[0m\u001b[1mhtml\u001b[0m>\n",
      "\n",
      "<h1> <h2> <div> <p>\u001b[1m\n",
      "\u001b[0m\u001b[1m</\u001b[0m\u001b[1mp\u001b[0m\u001b[1m>\u001b[0m\u001b[1m </\u001b[0m\u001b[1mdiv\u001b[0m\u001b[1m>\u001b[0m\u001b[1m \u001b[0m\u001b[1m</h2>\u001b[0m\u001b[1m \u001b[0m</h1>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "for s in slices:\n",
    "    idxs.extend(range(s[0], s[1]))\n",
    "idxs = torch.tensor(idxs, requires_grad=False, device=device)\n",
    "\n",
    "for i, c in enumerate(model.to_str_tokens(train_prompt)):\n",
    "    if i in idxs:\n",
    "        print('\\033[1m' + c + '\\033[0m', sep='', end='')\n",
    "    else:\n",
    "        print(c, sep='', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<|endoftext|>'), (1, '<'), (2, 'html'), (3, '>'), (4, ' <'), (5, 'h'), (6, '1'), (7, '>'), (8, ' <'), (9, 'p'), (10, '>'), (11, '\\n'), (12, '</'), (13, 'p'), (14, '>'), (15, ' </'), (16, 'h'), (17, '1'), (18, '>'), (19, ' </'), (20, 'html'), (21, '>'), (22, '\\n'), (23, '\\n'), (24, '<'), (25, 'html'), (26, '>'), (27, ' <'), (28, 'h'), (29, '1'), (30, '>'), (31, ' <'), (32, 'div'), (33, '>'), (34, '\\n'), (35, '</'), (36, 'div'), (37, '>'), (38, ' </'), (39, 'h'), (40, '1'), (41, '>'), (42, ' </'), (43, 'html'), (44, '>'), (45, '\\n'), (46, '\\n'), (47, '<'), (48, 'h'), (49, '1'), (50, '>'), (51, ' <'), (52, 'div'), (53, '>'), (54, ' <'), (55, 'p'), (56, '>'), (57, '\\n'), (58, '</'), (59, 'p'), (60, '>'), (61, ' </'), (62, 'div'), (63, '>'), (64, ' </'), (65, 'h'), (66, '1'), (67, '>'), (68, '\\n'), (69, '\\n'), (70, '<'), (71, 'html'), (72, '>'), (73, ' <'), (74, 'h'), (75, '1'), (76, '>'), (77, ' <'), (78, 'p'), (79, '>'), (80, ' <'), (81, 'div'), (82, '>'), (83, '\\n'), (84, '</'), (85, 'div'), (86, '>'), (87, ' </'), (88, 'p'), (89, '>'), (90, ' </'), (91, 'h'), (92, '1'), (93, '>'), (94, ' </'), (95, 'html'), (96, '>'), (97, '\\n'), (98, '\\n'), (99, '<'), (100, 'h'), (101, '1'), (102, '>'), (103, ' <'), (104, 'h'), (105, '2'), (106, '>'), (107, ' <'), (108, 'div'), (109, '>'), (110, ' <'), (111, 'p'), (112, '>'), (113, '\\n'), (114, '</'), (115, 'p'), (116, '>'), (117, ' </'), (118, 'div'), (119, '>'), (120, ' </'), (121, 'h'), (122, '2'), (123, '>'), (124, ' </'), (125, 'h'), (126, '1'), (127, '>'), (128, '\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(model.to_str_tokens(train_prompt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_modifier_hook(x, hook, lam):\n",
    "    x = lam[None, None, :, None] * x # b pos head dim\n",
    "    return x\n",
    "\n",
    "def mlp_modifier_hook(x, hook, gam):\n",
    "    x = gam[None, :, None] * x # b pos dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned lambdas\n",
    "\n",
    "start_layer = 6\n",
    "\n",
    "lambdas = torch.nn.Parameter(torch.ones(\n",
    "    (model.cfg.n_layers - start_layer, model.cfg.n_heads), device=device), requires_grad=True\n",
    ")\n",
    "\n",
    "gammas = torch.nn.Parameter(torch.ones(\n",
    "    (model.cfg.n_layers - start_layer), device=device), requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam([lambdas], lr=0.1) \n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokens = model.to_tokens(train_prompt)\n",
    "labels = tokens[:, idxs+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "losses = []\n",
    "l1_coefficient = 0.001  # Set this to regulate l1 penalty\n",
    "\n",
    "for e in tqdm(range(100)):\n",
    "    proba = model.run_with_hooks(\n",
    "                tokens,\n",
    "                fwd_hooks=[(\n",
    "                        f\"blocks.{l}.attn.hook_result\",\n",
    "                        partial(head_modifier_hook, lam=lambdas[l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)] + \n",
    "                    [(\n",
    "                        f\"blocks.{l}.hook_mlp_out\",\n",
    "                        partial(mlp_modifier_hook, gam=gammas[None, l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)]\n",
    "            ).softmax(-1)[:, idxs]\n",
    "    \n",
    "    loss = loss_fn(proba.view(-1, model.cfg.d_vocab), labels.view(-1))\n",
    "    \n",
    "    # Add the L1 regularization term to the loss\n",
    "    l1_norm = lambdas.abs().sum() + gammas.abs().sum()\n",
    "    loss += l1_coefficient * l1_norm\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients for `lambdas`\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(y=losses, title='Loss')\n",
    "fig.write_html('loss.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = lambdas.detach().cpu().numpy()\n",
    "fig = px.imshow(data,\n",
    "                labels=dict(x=\"Heads\", y=\"Layers\", color=\"Lambda\"),\n",
    "                title=\"Lambda values\", aspect='auto', color_continuous_scale='RdBu', zmin=-2, zmax=2)\n",
    "\n",
    "# Update the heatmap to show the annotations\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    yaxis=dict(\n",
    "        tickmode='array', \n",
    "        tickvals=list(range(model.cfg.n_layers - start_layer)),\n",
    "        ticktext=list(range(start_layer, model.cfg.n_layers))\n",
    "    )\n",
    ")\n",
    "fig.update_traces(showscale=True)\n",
    "\n",
    "# Round the annotations to 3 decimal places\n",
    "for i in range(model.cfg.n_heads):\n",
    "    for j in range(model.cfg.n_layers - start_layer):\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=12),\n",
    "                                x=i,\n",
    "                                y=j,\n",
    "                                text=str(round(data[j, i], 3)),\n",
    "                                showarrow=False,\n",
    "                                align='center',\n",
    "                                opacity=0.6))\n",
    "\n",
    "fig.write_html('lambdas.html')\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 9\n",
    "head_id = 7\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, cache = model.run_with_cache(model.to_tokens(train_prompt))\n",
    "\n",
    "data = cache[f'blocks.{layer_id}.attn.hook_pattern'][0, head_id].cpu()\n",
    "\n",
    "labels = [f\"{tok} ({i})\" for i, tok in enumerate(model.to_str_tokens(train_prompt))]\n",
    "\n",
    "# Create the plot using Plotly Express\n",
    "fig = px.imshow(\n",
    "    data,\n",
    "    labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    title=f'Attention patter at head {head_id} of layer {layer_id}',\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    aspect='auto'\n",
    ")\n",
    "\n",
    "# Adjust the layout for better readability\n",
    "fig.update_xaxes(tickangle=35)\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"), height=800)\n",
    "fig.write_html('pattern.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f614e060d84fd4aedfab6580d3e8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html> <h1> <p>\n",
      "</p> </h1> </html>\n",
      "\n",
      "<h1> <div> <p>\n",
      "</p> </div> </h1>\n",
      "\n",
      "<html> <h1> <p> <div>\n",
      "</div> </p> </h1> </html>\n",
      "\n",
      "<h1> <h2> <div> <p>\n",
      "</p> </div> </h2> </h1>\n",
      "\n",
      "<h1> <h2> <h3> <h4>\n",
      "\n",
      "\n",
      "<p>\n",
      "\n",
      "</p> </h2> </h1> </html>\n",
      "\n",
      "<h1> <h2> <p>\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(test_prompt, stop_at_eos=False, temperature=0, max_new_tokens=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 42.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><html> <h1> <p>\n",
      "</p> </h1> </html>\n",
      "\n",
      "<h1> <div> <p>\n",
      "</p> </div> </h1>\n",
      "\n",
      "<html> <h1> <p> <div>\n",
      "</div> </p> </h1> </html>\n",
      "\n",
      "<h1> <h2> <div> <p>\n",
      "</p> </div> </h2> </h1>\n",
      "\n",
      "<h1> <h2> <h3> <h4>\n",
      "\n",
      "\n",
      "<h1> <h3> <h3> <h3> <h3> <h3> <h3> <h3>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = model.to_tokens(test_prompt)\n",
    "max_new_tokens = 32\n",
    "\n",
    "for i in tqdm(range(max_new_tokens)): \n",
    "    with torch.no_grad():\n",
    "        new_tok = model.run_with_hooks(\n",
    "            tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    f\"blocks.{l}.attn.hook_result\",\n",
    "                    partial(head_modifier_hook, lam=lambdas[l - start_layer]),\n",
    "                ) for l in range(start_layer, model.cfg.n_layers)\n",
    "            ]\n",
    "        ).argmax(-1)[:, -1, None]\n",
    "\n",
    "    tokens = torch.cat([tokens, new_tok], dim=-1)\n",
    "\n",
    "print(model.to_string(tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
