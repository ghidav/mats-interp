{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Boost for ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface'\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "model.eval()\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tasks.json\", \"r\") as f:\n",
    "    tasks = json.load(f)['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "slices = []\n",
    "\n",
    "pos = 0\n",
    "train_prompt = \"\"\n",
    "for i, (inp, out) in enumerate(zip(tasks[idx]['train_input'], tasks[idx]['train_output'])):\n",
    "    train_prompt += inp + '\\n' + out + \"\\n\\n\"\n",
    "    pos += len(model.to_tokens(inp, prepend_bos=False)[0]) + 1\n",
    "    if i != 0:\n",
    "        slices.append((pos, pos + len(model.to_tokens(out, prepend_bos=False)[0])))\n",
    "    pos += len(model.to_tokens(out, prepend_bos=False)[0]) + 1 + int(model_name == 'gpt2')\n",
    "\n",
    "test_prompt = \"\"\n",
    "for inp, out in zip(tasks[idx]['test_input'], tasks[idx]['test_output']):\n",
    "    test_prompt += inp + '\\n' + out + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I'm Davide, I'm 20 years old and I live in Rome.\n",
      "{\n",
      "'name': 'Davide',\n",
      "'age': '20',\n",
      "'city': 'Rome'\n",
      "}\n",
      "\n",
      "My name is Susan and I live in San Francisco. I've just turned 12.\u001b[1m\n",
      "\u001b[0m\u001b[1m{\u001b[0m\u001b[1m\n",
      "\u001b[0m\u001b[1m'\u001b[0m\u001b[1mname\u001b[0m\u001b[1m':\u001b[0m\u001b[1m '\u001b[0m\u001b[1mSusan\u001b[0m\u001b[1m',\u001b[0m\u001b[1m\n",
      "\u001b[0m\u001b[1m'\u001b[0m\u001b[1mage\u001b[0m\u001b[1m':\u001b[0m\u001b[1m '\u001b[0m\u001b[1m12\u001b[0m\u001b[1m',\u001b[0m\u001b[1m\n",
      "\u001b[0m\u001b[1m'\u001b[0m\u001b[1mcity\u001b[0m\u001b[1m':\u001b[0m\u001b[1m '\u001b[0m\u001b[1mSan\u001b[0m\u001b[1m Francisco\u001b[0m\u001b[1m'\u001b[0m\u001b[1m\n",
      "\u001b[0m}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "for s in slices:\n",
    "    idxs.extend(range(s[0], s[1]))\n",
    "idxs = torch.tensor(idxs, requires_grad=False, device=device)\n",
    "\n",
    "for i, c in enumerate(model.to_str_tokens(train_prompt)):\n",
    "    if i in idxs:\n",
    "        print('\\033[1m' + c + '\\033[0m', sep='', end='')\n",
    "    else:\n",
    "        print(c, sep='', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<|endoftext|>'), (1, 'I'), (2, \"'m\"), (3, ' Dav'), (4, 'ide'), (5, ','), (6, ' I'), (7, \"'m\"), (8, ' 20'), (9, ' years'), (10, ' old'), (11, ' and'), (12, ' I'), (13, ' live'), (14, ' in'), (15, ' Rome'), (16, '.'), (17, '\\n'), (18, '{'), (19, '\\n'), (20, \"'\"), (21, 'name'), (22, \"':\"), (23, \" '\"), (24, 'D'), (25, 'av'), (26, 'ide'), (27, \"',\"), (28, '\\n'), (29, \"'\"), (30, 'age'), (31, \"':\"), (32, \" '\"), (33, '20'), (34, \"',\"), (35, '\\n'), (36, \"'\"), (37, 'city'), (38, \"':\"), (39, \" '\"), (40, 'R'), (41, 'ome'), (42, \"'\"), (43, '\\n'), (44, '}'), (45, '\\n'), (46, '\\n'), (47, 'My'), (48, ' name'), (49, ' is'), (50, ' Susan'), (51, ' and'), (52, ' I'), (53, ' live'), (54, ' in'), (55, ' San'), (56, ' Francisco'), (57, '.'), (58, ' I'), (59, \"'ve\"), (60, ' just'), (61, ' turned'), (62, ' 12'), (63, '.'), (64, '\\n'), (65, '{'), (66, '\\n'), (67, \"'\"), (68, 'name'), (69, \"':\"), (70, \" '\"), (71, 'Susan'), (72, \"',\"), (73, '\\n'), (74, \"'\"), (75, 'age'), (76, \"':\"), (77, \" '\"), (78, '12'), (79, \"',\"), (80, '\\n'), (81, \"'\"), (82, 'city'), (83, \"':\"), (84, \" '\"), (85, 'San'), (86, ' Francisco'), (87, \"'\"), (88, '\\n'), (89, '}'), (90, '\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(model.to_str_tokens(train_prompt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_modifier_hook(x, hook, lam):\n",
    "    x = lam[None, None, :, None] * x # b pos head dim\n",
    "    return x\n",
    "\n",
    "def mlp_modifier_hook(x, hook, gam):\n",
    "    x = gam[None, :, None] * x # b pos dim\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned lambdas\n",
    "\n",
    "start_layer = 6\n",
    "\n",
    "lambdas = torch.nn.Parameter(torch.ones(\n",
    "    (model.cfg.n_layers - start_layer, model.cfg.n_heads), device=device), requires_grad=True\n",
    ")\n",
    "\n",
    "gammas = torch.nn.Parameter(torch.ones(\n",
    "    (model.cfg.n_layers - start_layer), device=device), requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam([lambdas], lr=0.1) \n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokens = model.to_tokens(train_prompt)\n",
    "labels = tokens[:, idxs+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:02<00:00, 16.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "losses = []\n",
    "l1_coefficient = 0.001  # Set this to regulate l1 penalty\n",
    "l2_coefficient = 0.000\n",
    "\n",
    "for e in tqdm(range(40)):\n",
    "    proba = model.run_with_hooks(\n",
    "                tokens,\n",
    "                fwd_hooks=[(\n",
    "                        f\"blocks.{l}.attn.hook_result\",\n",
    "                        partial(head_modifier_hook, lam=lambdas[l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)] + \n",
    "                    [(\n",
    "                        f\"blocks.{l}.hook_mlp_out\",\n",
    "                        partial(mlp_modifier_hook, gam=gammas[None, l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)]\n",
    "            ).softmax(-1)[:, idxs]\n",
    "    \n",
    "    loss = loss_fn(proba.view(-1, model.cfg.d_vocab), labels.view(-1))\n",
    "    \n",
    "    # Add the L1 regularization term to the loss\n",
    "    l1_norm = lambdas.abs().sum() #+ gammas.abs().sum()\n",
    "    l2_norm = lambdas.norm(p=2) \n",
    "    loss += l1_coefficient * l1_norm\n",
    "    loss += l2_coefficient * l2_norm\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients for `lambdas`\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(y=losses, title='Loss')\n",
    "fig.write_html('loss.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = lambdas.detach().cpu().numpy()\n",
    "fig = px.imshow(data,\n",
    "                labels=dict(x=\"Heads\", y=\"Layers\", color=\"Lambda\"),\n",
    "                title=\"Lambda values\", aspect='auto', color_continuous_scale='RdBu', zmin=-2, zmax=2)\n",
    "\n",
    "# Update the heatmap to show the annotations\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    yaxis=dict(\n",
    "        tickmode='array', \n",
    "        tickvals=list(range(model.cfg.n_layers - start_layer)),\n",
    "        ticktext=list(range(start_layer, model.cfg.n_layers))\n",
    "    )\n",
    ")\n",
    "fig.update_traces(showscale=True)\n",
    "\n",
    "# Round the annotations to 3 decimal places\n",
    "for i in range(model.cfg.n_heads):\n",
    "    for j in range(model.cfg.n_layers - start_layer):\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=12),\n",
    "                                x=i,\n",
    "                                y=j,\n",
    "                                text=str(round(data[j, i], 3)),\n",
    "                                showarrow=False,\n",
    "                                align='center',\n",
    "                                opacity=0.6))\n",
    "\n",
    "fig.write_html('lambdas.html')\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 9\n",
    "head_id = 7\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, cache = model.run_with_cache(model.to_tokens(train_prompt))\n",
    "\n",
    "data = cache[f'blocks.{layer_id}.attn.hook_pattern'][0, head_id].cpu()\n",
    "\n",
    "labels = [f\"{tok} ({i})\" for i, tok in enumerate(model.to_str_tokens(train_prompt))]\n",
    "\n",
    "# Create the plot using Plotly Express\n",
    "fig = px.imshow(\n",
    "    data,\n",
    "    labels=dict(x=\"Keys\", y=\"Queries\", color=\"Attention Score\"),\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    title=f'Attention patter at head {head_id} of layer {layer_id}',\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    aspect='auto'\n",
    ")\n",
    "\n",
    "# Adjust the layout for better readability\n",
    "fig.update_xaxes(tickangle=35)\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Score\"), height=800)\n",
    "fig.write_html('pattern.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa565f9cd3942c5bd1975592513ea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Davide, I'm 20 years old and I live in Rome.\n",
      "{\n",
      "'name': 'Davide',\n",
      "'age': '20',\n",
      "'city': 'Rome'\n",
      "}\n",
      "\n",
      "Hello, I'm Paul and I'm 36. I am an engineer and I've just moved to Taipei.\n",
      "\n",
      "\n",
      "I'm a member of the Taipei-based team, and I'm a member of the Taipei-based team, and I'm a member of the Taipei-based team, and I'm a member of the Taipei-based team, and I'm a member of the Taipei-based team,\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(test_prompt, stop_at_eos=False, temperature=0, max_new_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 26.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I'm Davide, I'm 20 years old and I live in Rome.\n",
      "{\n",
      "'name': 'Davide',\n",
      "'age': '20',\n",
      "'city': 'Rome'\n",
      "}\n",
      "\n",
      "Hello, I'm Paul and I'm 36. I am an engineer and I've just moved to Taipei.\n",
      "\n",
      "\n",
      "{\n",
      "'name': 'Paul',\n",
      "'age': '36',\n",
      "'city': 'Taipei'\n",
      "\n",
      "}\n",
      "\n",
      "{\n",
      "'name': 'Paul',\n",
      "'age': '36',\n",
      "'city': 'Taipei'\n",
      "\n",
      "}\n",
      "\n",
      "{\n",
      "\n",
      "'name': 'Paul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = model.to_tokens(test_prompt)\n",
    "max_new_tokens = 64\n",
    "\n",
    "for i in tqdm(range(max_new_tokens)): \n",
    "    with torch.no_grad():\n",
    "        new_tok = model.run_with_hooks(\n",
    "            tokens,\n",
    "            fwd_hooks=[(\n",
    "                        f\"blocks.{l}.attn.hook_result\",\n",
    "                        partial(head_modifier_hook, lam=lambdas[l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)] + \n",
    "                    [(\n",
    "                        f\"blocks.{l}.hook_mlp_out\",\n",
    "                        partial(mlp_modifier_hook, gam=gammas[None, l - start_layer]),\n",
    "                    ) for l in range(start_layer, model.cfg.n_layers)]\n",
    "        ).argmax(-1)[:, -1, None]\n",
    "\n",
    "    tokens = torch.cat([tokens, new_tok], dim=-1)\n",
    "\n",
    "print(model.to_string(tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-interp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
